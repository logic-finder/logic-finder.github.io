<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Web Audio API Spec Korean Translation</title>
    <link rel="stylesheet" href="index.css">
  </head>
  <body>
    <header>
      <p><a href="../index.html">return to home</a></p>
      <br>
      <p id="title">W3C Web Audio API Specification Korean Translation</p>
      <p class="description">It would be appreciated if you note that this translation is <strong>unofficial</strong>; it is just for the pure sake of my studying Web Audio API.</p>
      <p class="description">이 번역이 <strong>비공식</strong>임에 주목해주신다면 감사하겠습니다. 이 번역은 단지 순수하게 제가 Web Audio API를 공부하기 위한 것입니다.</p>
      <a href="https://www.w3.org/TR/webaudio/">link to the original document</a>
      <p>notice: currently on work: 21.11.18 ~ </p>
    </header>
    <hr>
    <main>
      <div id="frame">
        <h1>개요</h1>
        <p>이 명세는 웹 애플리케이션에서 오디오를 프로세싱하고 합성하기 위한 고수준의 WEB API를 기술합니다. 이 API의 주요한 패러다임은 오디오 라우팅 그래프에 관한 것인데, 오디오 라우팅 그래프란 다수의 AudioNode 객체들이 함께 연결되어 전반적인 오디오 렌더링을 정의하는 것입니다. 실제적인 프로세싱은 근본적인 구현 (보통 최적화된 Assembly / C / C++ 코드) 에서 주로 일어나지만, 직접적인 스크립트 프로세싱과 합성 또한 지원됩니다.</p>
        <p>아래의 '소개' 절은 이 명세의 동기를 다룹니다.</p>
        <p>이 API는 웹 플랫폼상의 다른 API 및 요소와 함께 사용되도록 설계되었습니다. 주요한 것으로는 XMLHttpRequest [XHR] 이 있습니다 (responseType과 response 어트리뷰트를 사용). 게임과 상호작용이 일어나는 어플리케이션에 대해서는, canvas 2D [2dcontext] 와 WebGL [WEBGL] 3D 그래픽 API와 함께 사용되는 것이 기대됩니다.</p>
        <h1>이 문서의 상태</h1>
        <p>생략</p>
        <h1>소개</h1>
        <p>웹에서의 오디오는 이 시점까지 상당히 원시적이었고 아주 최근까지는 Flash나 QuickTime과 같은 플러그인을 통해 전송되었습니다. HTML5에서의 audio 요소의 소개는 아주 중요했는데, 이는 기본적인 스트리밍 오디오 재생을 가능케 했습니다. 그러나, 더욱 복잡한 오디오 애플리케이션을 다루기에는 충분히 강력하지 못했습니다. 정교한 웹 기반의 게임과 상호작용이 일어나는 애플리케이션에 대해서, 또 다른 해결책이 요구되었습니다. 현대의 데스크톱 오디오 프로덕션 애플리케이션에서 발견되는 몇몇 믹싱, 프로세싱, 필터링 태스크에 더불어 현대 게임의 오디오 엔진에서 발견되는 능력들을 포함하는 것이 이 명세의 목표입니다.</p>
        <p>Web Audio API는 사용 경우 <a href="https://www.w3.org/TR/webaudio-usecases/" target="_blank">[webaudio-usecases]</a> 의 넓은 다양성을 염두에 두고 설계되었습니다. 이상적으로, 이 API는 스크립트를 통해 제어되고 브라우저에서 실행되는 최적화된 C++ 엔진으로 적절하게 구현될 수 있는 모든 사용 경우를 지원할 수 있어야 합니다. 그렇긴 하지만, 현대의 데스크톱 오디오 소프트웨어는 아주 발전된 능력을 가지고 있어서, 그것들 중 몇몇은 이 시스템으로 개발하기 어렵거나 불가능할지도 모릅니다. Apple의 Logic Audio가 그런 애플리케이션 중 하나인데, 이 애플리케이션은 외부 MIDI 컨트롤러, 임의 플러그인 오디오 이펙트, 신디사이저, 대단히 최적화된 직접 녹음형(direct-to-disk) 오디오 파일 읽기/쓰기, 단단히 통합된 time-stretching 등등을 지원합니다. 그럼에도 불구하고, 이 제안된 시스템은 적절히 복잡한 게임과 음악적인 애플리케이션을 포함하는 상호작용이 일어나는 애플리케이션의 넓은 범위를 꽤 지원할 수 있을 것입니다. 그리고 이 API는 WebGL에 의해 제공되는 아주 발전된 그래픽 기능의 아주 훌륭한 보완물이 될 수 있습니다. 이 API는 더욱 발전된 능력이 추후에 더해질 수 있도록 설계되었습니다.</p>
        <h2>기능</h2>
        <p>Web Audio API는 아래의 주요한 기능들을 지원합니다.</p>
        <ul>
          <li>간단하거나 복잡한 믹싱/이펙트 아키텍처를 위한 모듈러 라우팅</li>
          <li>높은 dynamic 범위. 내부 프로세싱에서 32비트 float을 사용.</li>
          <li>드럼 머신과 시퀸서와 같은 아주 높은 정도의 리듬 정밀도를 요구하는 음악 애플리케이션을 위한 낮은 레이턴시를 가지는 sample-accurate scheduled 사운드 재생. 이 기능은 또한 이펙트의 동적 생성의 가능성을 포함합니다.</li>
          <li>엔벨로프, 페이드인/페이드아웃, granular 이펙트, 필터 sweep, LFO 등을 위한 오디오 파라미터의 automation</li>
          <li>오디오 스트림에서의 유연한 채널 핸들링. 채널이 나눠지거나 합쳐질 수 있게 함.</li>
          <li>&lt;audio&gt; 또는 &lt;video&gt; 미디어 요소로부터의 오디오 소스 프로세싱
            <ul>
              <li>MediaStreamTrackAudioSourceNode와 [webrtc]를 사용한 원격 peer로부터 받은 오디오 프로세싱</li>
              <li>MediaStreamAudioDestinationNode와 [webrtc]를 사용해 원격 peer로 생성되거나 프로세싱된 오디오 스트림 전송</li>
            </ul>
          </li>
          <li>스크립트를 사용한 직접적 오디오 스트림 합성과 프로세싱</li>
          <li>3D 게임과 immersive한 환경의 넓은 범위를 지원하는 공간화된(spatialized) 오디오
            <ul>
              <li>패닝 모델: equalpower, HRTF, pass-through</li>
              <li>거리 감쇠(attenuation)</li>
              <li>sound cone</li>
              <li>방해/폐쇄 (obstuction/occlusion)</li>
              <li>음원(source)/청자(listener) 기반</li>
            </ul>
          </li>
          <li>선형 이펙트의 넓은 범위를, 특히 아주 높은 품질의 room 이펙트를 위한 convolution 엔진. 아래는 가능한 효과의 몇 가지 예입니다.
            <ul>
              <li>작은/큰 방(room)</li>
              <li>대성당(cathedral)</li>
              <li>콘서트 홀</li>
              <li>동굴(cave)</li>
              <li>터널(tunnel)</li>
              <li>복도(hallway)</li>
              <li>숲(forest)</li>
              <li>원형 극장(amphitheater)</li>
              <li>출입구를 거치는 멀리 있는 방으로부터의 소리</li>
              <li>extreme 필터</li>
              <li>기묘한 backward 이펙트</li>
              <li>extreme 빗(comb) 필터 이펙트</li>
            </ul>
          </li>
          <li>믹스의 전반적인 제어와 sweetening을 위한 dynamics compression</li>
          <li>효율적인 실시간 시간 영역(time-domain) 그리고 주파수 영역(frequency-domain) 분석 / 음악 시각화 지원</li>
          <li>lowpass, highpass, 그리고 다른 일반적인 필터를 위한 효율적인 biquad 필터</li>
          <li>distortion을 위한 waveshaping 효과와 다른 비선형적 효과</li>
          <li>오실레이터(oscilator)</li>
        </ul>
        <h3>모듈러 라우팅</h3>
        <p>모듈러 라우팅은 다른 AudioNode 객체 사이의 임의적인 연결을 가능케 합니다. 각 노드는 입력과 출력 양 쪽 모두 또는 어느 한 쪽을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력만을 가지고 있습니다. 목적지(destination) 노드는 하나의 입력을 가지고 출력이 없는 노드입니다. 필터와 같은 다른 노드들은 소스 노드와 목적지 노드 사이에 놓일 수 있습니다. 개발자는 두 객체가 함께 연결될 때의 저수준의 스트림 포맷 디테일에 대해 걱정할 필요가 없습니다. 옳은 일이 단지 일어나니까요. 예를 들자면, 만약 모노 오디오 스트림이 스테레오 입력에 연결되었다면 API는 단지 좌측 채널과 우측 채널을 적절히 믹스할 것입니다.</p>
        <p>가장 단순한 경우, 하나의 소스가 바로 출력으로 route될 수 있습니다. 모든 라우팅은 하나의 AudioDestinationNode를 포함하고 있는 AudioContext내에서 발생합니다.</p>
        <figure>
          <img src="img/modular-routing1.png" width="305" height="128">
          <figcaption><strong>도식 1</strong> 모듈러 라우팅의 단순한 예제</figcaption>
        </figure>
        <p>위 그림은 이 단순한 라우팅을 보여줍니다. 아래는 하나의 사운드를 재생하는 간단한 예제입니다.</p>
        <pre>예제 1
const context = new AudioContext();

function playSound() {
  const source = context.createBufferSource();
  source.buffer = dogBarkingBuffer;
  source.connect(context.destination);
  source.start(0);
}
        </pre>
        <p>아래는 세 개의 소스와 최종 출력 스테이지에서 dynamics compressor로 보내지는 convolution reverb를 보여주는 더욱 복잡한 예시입니다.</p>
        <figure>
          <img src="img/modular-routing2.png" width="561" height="411">
          <figcaption><strong>도식 2</strong> 모듈러 라우팅의 더욱 복잡한 예제</figcaption>
        </figure>
        <pre>예제 2
let context;
let compressor;
let reverb;
let source1, source2, source3;
let lowpassFilter;
let waveShaper;
let panner;
let dry1, dry2, dry3;
let wet1, wet2, wet3;
let mainDry;
let mainWet;
function setupRoutingGraph () {
  context = new AudioContext();
  // 이펙트 노드를 생성합니다.
  lowpassFilter = context.createBiquadFilter();
  waveShaper = context.createWaveShaper();
  panner = context.createPanner();
  compressor = context.createDynamicsCompressor();
  reverb = context.createConvolver();
  // main wet과 dry를 생성합니다.
  mainDry = context.createGain();
  mainWet = context.createGain();
  // 마지막 compressor를 마지막 목적지에 연결합니다.
  compressor.connect(context.destination);
  // main dry와 wet을 compressor에 연결합니다.
  mainDry.connect(compressor);
  mainWet.connect(compressor);
  // reverb를 main wet에 연결합니다.
  reverb.connect(mainWet);
  // 몇 가지 소스를 생성합니다.
  source1 = context.createBufferSource();
  source2 = context.createBufferSource();
  source3 = context.createOscillator();
  source1.buffer = manTalkingBuffer;
  source2.buffer = footstepsBuffer;
  source3.frequency.value = 440;
  // source1을 연결합니다.
  dry1 = context.createGain();
  wet1 = context.createGain();
  source1.connect(lowpassFilter);
  lowpassFilter.connect(dry1);
  lowpassFilter.connect(wet1);
  dry1.connect(mainDry);
  wet1.connect(reverb);
  // source2를 연결합니다.
  dry2 = context.createGain();
  wet2 = context.createGain();
  source2.connect(waveShaper);
  waveShaper.connect(dry2);
  waveShaper.connect(wet2);
  dry2.connect(mainDry);
  wet2.connect(reverb);
  // source3을 연결합니다.
  dry3 = context.createGain();
  wet3 = context.createGain();
  source3.connect(panner);
  panner.connect(dry3);
  panner.connect(wet3);
  dry3.connect(mainDry);
  wet3.connect(reverb);
  // 소스들을 지금 시작시킵니다.
  source1.start(0);
  source2.start(0);
  source3.start(0);
}
        </pre>
        <p>모듈러 라우팅은 또한 AudioNode들의 출력이 다른 AudioNode의 행동을 제어하는 AudioParam 파라미터에 route될 수 있게 합니다. 아래의 시나리오에서, 노드의 출력은 입력 시그널보다는 조절 시그널로서 행동합니다.</p>
        <figure>
          <img src="img/modular-routing3.png" width="346" height="337">
          <figcaption><strong>도식 3</strong> 하나의 오실레이터가 다른 오실레이터의 주파수를 조절하는 모습을 보여주는 모듈러 라우팅 시나리오</figcaption>
        </figure>
        <pre>예제 3
function setupRoutingGraph() {
  const context = new AudioContext();
  // 조절 시그널을 공급하는 낮은 주파수의 오실레이터를 생성합니다
  const lfo = context.createOscillator();
  lfo.frequency.value = 1.0;
  // 조절될 높은 주파수의 오실레이터를 생성합니다
  const hfo = context.createOscillator();
  hfo.frequency.value = 440.0;
  // gain 노드를 생성하는데 이 노드의 gain은 조절 시그널의 진폭을 결정합니다
  const modulationGain = context.createGain();
  modulationGain.gain.value = 50;
  // 그래프를 설정하고 오실레이터들을 시작시킵니다
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
        </pre>
        <h2>API 개요</h2>
        <p>정의되는 인터페이스들은 아래와 같습니다.</p>
        <ul>
          <li>AudioContext 인터페이스. 이 인터페이스는 AudioNode간의 연결을 표현하는 오디오 시그널 그래프를 포함합니다.</li>
          <li>AudioNode 인터페이스. 이 인터페이스는 오디오 소스, 오디오 출력, 중간 프로세싱 모듈을 표현합니다. AudioNode는 모듈러 방식으로 동적으로 연결될 수 있습니다. AudioNode는 AudioContext의 컨텍스트 내에서 존재합니다.</li>
          <li>AnalyserNode 인터페이스. 이는 음악 시각화나 다른 시각화 애플리케이션에서의 사용을 위한 AudioNode입니다.</li>
          <li>AudioBuffer 인터페이스. 이는 메모리에 상주하는(memory-resident) 오디오 에셋으로 작업하기 위한 것입니다. 이 오디오 에셋은 한 번의 소리, 혹은 더 긴 오디오 클립을 표현할 수 있습니다.</li>
          <li>AudioBufferSourceNode 인터페이스. 이는 AudioBuffer에서 오디오를 생성하는 AudioNode입니다.</li>
          <li>AudioDestinationNode 인터페이스. 모든 렌더링된 오디오에 대한 마지막 목적지를 나타내는 AudioNode의 서브클래스입니다.</li>
          <li>AudioParam 인터페이스. 볼륨과 같은 AudioNode 기능의 개별적 측면을 제어합니다.</li>
          <li>AudioListener 인터페이스. 공간화를 위해 PannerNode와 함께 사용됩니다.</li>
          <li>AudioWorklet 인터페이스. 스크립트를 사용해 직접 오디오를 프로세싱할 수 있는 사용자 정의 노드를 생성하기 위한 팩토리를 나타냅니다.</li>
          <li>AudioWorkletGlobalScope 인터페이스. AudioWorkletProcessor의 프로세싱 스크립트가 실행되는 컨텍스트입니다.</li>
          <li>AudioWorkletNode 인터페이스. AudioWorkletProcessor 내에서 프로세싱되는 노드를 나타내는 AudioNode입니다.</li>
          <li>AudioWorkletProcessor 인터페이스. 오디오 worker 내부의 단일 노드 인스턴스를 나타냅니다.</li>
          <li>BiquadFilterNode 인터페이스. 다음과 같은 일반적인 low-order 필터들에 대한 AudioNode입니다.
            <ul>
              <li>Low Pass</li>
              <li>High Pass</li>
              <li>Band Pass</li>
              <li>Low Shelf</li>
              <li>High Shelf</li>
              <li>Peaking</li>
              <li>Notch</li>
              <li>Allpass</li>
            </ul>
          </li>
          <li>ChannelMergerNode 인터페이스. 다수의 오디오 스트림으로부터의 채널을 하나의 오디오 스트림으로 결합하기 위한 AudioNode.</li>
          <li>ChannelSplitterNode 인터페이스. 라우팅 그래프에서 오디오 스트림의 개개의 채널에 접근하기 위한 AudioNode.</li>
          <li>ConstantSourceNode 인터페이스. AudioParam이 명목상의 constant 출력값의 automation을 허용하게 하며, 저 값을 생성하기 위한 AudioNode.</li>
          <li>ConvolverNode 인터페이스. (콘서트 홀의 소리와 같이) 실시간 선형 이펙트를 적용하기 위한 AudioNode.</li>
          <li>DelayNode 인터페이스. 동적으로 조정 가능한 가변적인 딜레이를 적용하는 AudioNode.</li>
          <li>DynamicsCompressorNode 인터페이스. dynamics compression을 위한 AudioNode.</li>
          <li>GainNode 인터페이스. 명시적인 gain 제어를 위한 AudioNode.</li>
          <li>IIRFilterNode 인터페이스. 일반적인 IIR 필터를 위한 AudioNode.</li>
          <li>MediaElementAudioSourceNode 인터페이스. &lt;audio&gt;, &lt;video&gt;, 혹은 그 밖의 미디어 요소로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamTrackAudioSourceNode 인터페이스. MediaStreamTrack으로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamAudioDestinationNode 인터페이스. 원격 peer에게 전송되는 MediaStream으로의 오디오 목적지인 AudioNode.</li>
          <li>PannerNode 인터페이스. 3D 공간에서의 공간화/포지셔닝을 위한 AudioNode.</li>
          <li>PeriodicWave 인터페이스. OscillatorNode에 의한 사용을 위한 사용자 정의 주기 파형을 명시하기 위한 인터페이스입니다.</li>
          <li>OscillatorNode 인터페이스. 주기 파형을 생성하기 위한 AudioNode.</li>
          <li>StereoPannerNode 인터페이스. 스테레오 스트림에서 오디오 입력의 equal-power 포지셔닝을 위한 AudioNode.</li>
          <li>WaveShaperNode 인터페이스. distortion과 그 밖의 다른 subtle warming 효과를 위한 비선형적 waveshaping 효과를 적용하는 AudioNode.</li>
        </ul>
        <p>Web Audio API에서 폐기(deprecated)되었지만 이것들의 대체물의 구현 경험이 있을 때 까지는 아직 제거되지 않을 몇몇 기능들이 있습니다.</p>
        <ul>
          <li>ScriptProcessorNode 인터페이스. 스크립트를 사용해 직접적으로 오디오를 생성하거나 프로세싱하기 위한 AudioNode.</li>
          <li>AudioProcessingEvent 인터페이스. ScriptProcessorNode 객체와 함께 사용되는 이벤트 유형입니다.</li>
        </ul>
        <h1>Audio API</h1>
        <h2>BaseAudioContext 인터페이스</h2>
        <p>BaseAudioContext 인터페이스는 AudioNode 객체들의 집합과 이 객체들의 연결을 나타냅니다. 이 인터페이스는 AudioDestinationNode로 향하는 임의적인 신호 라우팅을 고려합니다. 노드들은 이 컨텍스트에서 생성되고 그리고 나서 함께 연결됩니다.</p>
        <p>BaseAudioContext는 직접적으로 인스턴스화되지 않지만, 대신 실체가 있는 인터페이스인 AudioContext (실시간 렌더링용) 와 OfflineAudioContext (오프라인 렌더링용) 에 의해 상속됩니다.</p>
        <p>BaseAudioContext는 내부 슬롯 [[pending promises]] 를 가지고 생성되는데, 이 내부 슬롯은 초기적으로는 빈 promise의 정렬된 리스트입니다.</p>
        <p>각 BaseAudioContext는 고유의 미디어 요소 이벤트 태스크 소스를 가지고 있습니다. 추가적으로, BaseAudioContext는 두 개의 private 슬롯 [[rendering thread state]] 와 [[control thread state]]를 가지고 있는데, 이 슬롯들은 AudioContextState로부터 값을 취하며, 둘 다 초기적으로 "suspended"로 설정되어 있습니다.</p>
        <pre>
enum AudioContextState {
  "suspended",
  "running",
  "closed"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>suspended</i>"</td>
              <td>이 컨텍스트는 현재 연기되었습니다 (context time은 진행되지 않으며, audio 하드웨어는 전원이 차단되었거나/released 되었을 수 있습니다).</td>
            </tr>
            <tr>
              <td>"<i>running</i>"</td>
              <td>오디오가 처리되고 있습니다.</td>
            </tr>
            <tr>
              <td>"<i>closed</i>"</td>
              <td>이 컨텍스트는 released되었고, 더 이상 오디오를 처리하기 위해 사용될 수 없습니다. 모든 시스템 오디오 자원이 released되었습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
callback DecodeErrorCallback = undefined (DOMException error);

callback DecodeSuccessCallback = undefined (AudioBuffer decodedData);

[Exposed=Window]
interface BaseAudioContext : EventTarget {
  readonly attribute AudioDestinationNode destination;
  readonly attribute float sampleRate;
  readonly attribute double currentTime;
  readonly attribute AudioListener listener;
  readonly attribute AudioContextState state;
  [SameObject, SecureContext]
  readonly attribute AudioWorklet audioWorklet;
  attribute EventHandler onstatechange;

  AnalyserNode createAnalyser ();
  BiquadFilterNode createBiquadFilter ();
  AudioBuffer createBuffer (unsigned long numberOfChannels,
                            unsigned long length,
                            float sampleRate);
  AudioBufferSourceNode createBufferSource ();
  ChannelMergerNode createChannelMerger (optional unsigned long numberOfInputs = 6);
  ChannelSplitterNode createChannelSplitter (
    optional unsigned long numberOfOutputs = 6);
  ConstantSourceNode createConstantSource ();
  ConvolverNode createConvolver ();
  DelayNode createDelay (optional double maxDelayTime = 1.0);
  DynamicsCompressorNode createDynamicsCompressor ();
  GainNode createGain ();
  IIRFilterNode createIIRFilter (sequence<double> feedforward,
                                 sequence<double> feedback);
  OscillatorNode createOscillator ();
  PannerNode createPanner ();
  PeriodicWave createPeriodicWave (sequence<float> real,
                                   sequence<float> imag,
                                   optional PeriodicWaveConstraints constraints = {});
  ScriptProcessorNode createScriptProcessor(
    optional unsigned long bufferSize = 0,
    optional unsigned long numberOfInputChannels = 2,
    optional unsigned long numberOfOutputChannels = 2);
  StereoPannerNode createStereoPanner ();
  WaveShaperNode createWaveShaper ();

  Promise<AudioBuffer> decodeAudioData (
    ArrayBuffer audioData,
    optional DecodeSuccessCallback? successCallback,
    optional DecodeErrorCallback? errorCallback);
};          
        </pre>
        <h3>어트리뷰트</h3>
        <dl>
          <dt><i>audioWorklet</i> / 자료형: AudioWorklet / 읽기 전용</dt>
          <dd>Worklet 객체로의 접근을 가능케 하는데, Worklet 객체는 [HTML] 과 AudioWorklet에 의해 정의된 알고리즘을 통해 AudioWorkletProcessor 클래스 정의를 포함하고 있는 스크립트를 import할 수 있습니다.</dd>
          <br>
          <dt></i>currentTime</i> / 자료형: double / 읽기 전용</dt>
          <dd>currentTime은 컨텍스트의 렌더링 그래프에 의해 가장 최근에 처리된 오디오 블럭의 마지막 sample frame 바로 다음의 샘플 프레임의 시간 (초) 입니다. 만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, currentTime은 0의 값을 갖습니다.</dd>
          <br>
          <dd>currentTime의 시간 좌표계에서, 0의 값은 그래프에 의해 처리되는 첫번째 블럭의 첫번째 샘플 프레임과 일치합니다. 이 계(system)에서 경과된 시간은 BaseAudioContext에 의해 생성된 오디오 스트림에서의 경과된 시간과 일치하는데, 이 시간은 이 계의 다른 시계와 동기화되지 않았을 수 있습니다. (OfflineAudioContext의 경우, 스트림이 어떠한 장치에서도 현재 재생되고 있지 않으므로, 실제 시간에 대한 근사치조차도 존재하지 않습니다.</dd>
          <br>
          <dd>Web Audio API에서의 모든 예정된 시간들은 currentTime의 값에 관련이 있습니다.</dd>
          <br>
          <dd>BaseAudioContext가 "running" 상태일 때, 이 어트리뷰트의 값은, 하나의 render quantum에 일치하며, 단조적으로 증가하고 획일적인 증가 속에서 렌더링 스레드에 의해 갱신됩니다. 따라서, 실행 중인 컨텍스트의 경우, 이 계가 오디오 블럭을 처리해 가는 동안 currentTime은 꾸준히 증가하고, 항상 처리될 다음 오디오 블럭의 시작 시간을 나타냅니다. 현재 상태에서 예정된 어떠한 변화가 일어날지도 모를 때 이것은 또한 가능한 가장 빠른 시간입니다.</dd>
          <dd>currentTime은 반환되기 전에 반드시 control thread에서 atomically하게 read되어야만 합니다.</dd>
          <br>
          <dt><i>destination</i> / 자료형: AudioDestinationNode / 읽기 전용</dt>
          <dd>모든 오디오의 최종 목적지를 나타내는 하나의 입력을 가진 AudioDestinationNode. 보통 이것은 실제 오디오 하드웨어를 나타냅니다. 오디오를 현재 렌더링하고 있는 모든 AudioNode는 직접적으로 혹은 간접적으로 목적지에 연결됩니다.</dd>
          <br>
          <dt><i>listener</i> / 자료형: AudioListener / 읽기 전용</dt>
          <dd>3D 공간화에 사용되는 AudioListener</dd>
          <br>
          <dt><i>onstatechange</i> / 자료형: EventHandler</dt>
          <dd>AudioContext의 상태가 변경되었을 때 BaseAudioContext에 전파되는 이벤트에 대한 EventHandler를 설정하기 위해 사용되는 프로퍼티 (예: 해당하는 promise가 resolve되었을 때). Event 유형의 이벤트가 이벤트 핸들러에 전파될 것인데, 이는 AudioContext의 상태를 직접적으로 질의할 수 있습니다. 새롭게 생성된 AudioContext는 항상 suspended 상태에서 시작하고, 언제든지 다른 상태로 상태가 변경될 때 상태 변화 이벤트가 발생될 것입니다. 이 이벤트는 oncomplete 이벤트가 발생되기 이전에 발생됩니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>BaseAudioContext가 오디오를 다루는 (초당 샘플 프레임에서의) 샘플 레이트. 이것은 컨텍스트의 모든 AudioNode가 이 rate에서 실행될 것으로 추정합니다. 이 추정을 내림에 있어, 샘플 레이트 전환기 혹은 "varispeed" 프로세서는 실시간 프로세싱이 지원되지 않습니다. <strong><i>나이퀴스트 주파수</i></strong>는 이 샘플 레이트 값의 절반입니다.</dd>
          <br>
          <dt><i>state</i> / 자료형: AudioContextState / 읽기 전용</dt>
          <dd>BaseAudioContext의 현재 상태를 기술합니다. 이 어트리뷰트를 get하는 것은 [[control thread state]] 슬롯의 내용을 반환합니다.</dd>
        </dl>
        <br>
        <h3>메서드</h3>
        <dl>
          <dt><i>createAnalyser()</i></dt>
          <dd>AnalyserNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AnalyserNode</dd>
          <br>
          <dt><i>createBiquadFilter()</i></dt>
          <dd>몇몇 일반적인 필터 유형 중 하나로써 설정될 수 있는 second order 필터를 나타내는 BiquadFilterNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: BiquadFilterNode</dd>
          <br>
          <dt><i>createBuffer(numOfChannels, length, sampleRate)</i></dt>
          <dd>주어진 크기의 AudioBuffer를 생성합니다. 버퍼 내의 오디오 데이터는 0으로 초기화될 것입니다 (silent). 만약 인자가 어느 하나라도 음수거나, 0이거나, 명목상의 범위 바깥에 있다면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createBuffer() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 구현은 반드시 적어도 32개의 채널을 지원해야 합니다.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼 크기를 결정합니다. 이것은 반드시 적어도 1이어야 합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 구현은 반드시 적어도 범위 8000에서 96000까지의 샘플 레이트를 지원해야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: AudioBuffer</dd>
          <br>
          <dt><i>createBufferSource()</i></dt>
          <dd>AudioBufferSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioBufferSourceNode</dd>
          <br>
          <dt><i>createChannelMerger(numberOfInputs)</i></dt>
          <dd>channel merger를 나타내는 ChannelMergerNode의 팩토리 메서드. 만약 numberOfInputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelMerger(numberOfInputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfInputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>입력의 수를 결정합니다. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelMergerNode</dd>
          <br>
          <dt><i>channelSplitterNode(numberOfOutputs)</i></dt>
          <dd>channel splitter를 나타내는 ChannelSplitterNode의 팩토리 메서드. 만약 numberOfOutputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelSplitter(numberOfOutputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfOutputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>출력의 수. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelSplitterNode</dd>
          <br>
          <dt><i>createConstantSource()</i></dt>
          <dd>ConstantSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConstantSourceNode</dd>
          <br>
          <dt><i>createConvolver()</i></dt>
          <dd>ConvolverNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConvolverNode</dd>
          <br>
        </dl>
    </div>
    </main>
  </body>
</html>
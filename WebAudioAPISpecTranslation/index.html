<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Web Audio API Spec Korean Translation</title>
    <link rel="stylesheet" href="index.css">
  </head>
  <body>
    <header>
      <p><a href="../index.html">return to home</a></p>
      <br>
      <p id="title">W3C Web Audio API Specification Korean Translation</p>
      <p class="description">It would be appreciated if you note that this translation is <strong>unofficial</strong>; it is just for the pure sake of my studying Web Audio API.</p>
      <p class="description">이 번역이 <strong>비공식</strong>임에 주목해주신다면 감사하겠습니다. 이 번역은 단지 순수하게 제가 Web Audio API를 공부하기 위한 것입니다.</p>
      <a href="https://www.w3.org/TR/webaudio/">link to the original document</a>
      <p>notice: currently on work: 21.11.18 ~ </p>
    </header>
    <hr>
    <main>
      <div id="frame">
        <h1>개요</h1>
        <p>이 명세는 웹 애플리케이션에서 오디오를 프로세싱하고 합성하기 위한 고수준의 WEB API를 기술합니다. 이 API의 주요한 패러다임은 오디오 라우팅 그래프에 관한 것인데, 오디오 라우팅 그래프란 다수의 AudioNode 객체들이 함께 연결되어 전반적인 오디오 렌더링을 정의하는 것입니다. 실제적인 프로세싱은 근본적인 구현 (보통 최적화된 Assembly / C / C++ 코드) 에서 주로 일어나지만, 직접적인 스크립트 프로세싱과 합성 또한 지원됩니다.</p>
        <p>아래의 '소개' 절은 이 명세의 동기를 다룹니다.</p>
        <p>이 API는 웹 플랫폼상의 다른 API 및 요소와 함께 사용되도록 설계되었습니다. 주요한 것으로는 XMLHttpRequest [XHR] 이 있습니다 (responseType과 response 어트리뷰트를 사용). 게임과 상호작용이 일어나는 어플리케이션에 대해서는, canvas 2D [2dcontext] 와 WebGL [WEBGL] 3D 그래픽 API와 함께 사용되는 것이 기대됩니다.</p>
        <h1>이 문서의 상태</h1>
        <p>생략</p>
        <h1>소개</h1>
        <p>웹에서의 오디오는 이 시점까지 상당히 원시적이었고 아주 최근까지는 Flash나 QuickTime과 같은 플러그인을 통해 전송되었습니다. HTML5에서의 audio 요소의 소개는 아주 중요했는데, 이는 기본적인 스트리밍 오디오 재생을 가능케 했습니다. 그러나, 더욱 복잡한 오디오 애플리케이션을 다루기에는 충분히 강력하지 못했습니다. 정교한 웹 기반의 게임과 상호작용이 일어나는 애플리케이션에 대해서, 또 다른 해결책이 요구되었습니다. 현대의 데스크톱 오디오 프로덕션 애플리케이션에서 발견되는 몇몇 믹싱, 프로세싱, 필터링 태스크에 더불어 현대 게임의 오디오 엔진에서 발견되는 능력들을 포함하는 것이 이 명세의 목표입니다.</p>
        <p>Web Audio API는 사용 경우 <a href="https://www.w3.org/TR/webaudio-usecases/" target="_blank">[webaudio-usecases]</a> 의 넓은 다양성을 염두에 두고 설계되었습니다. 이상적으로, 이 API는 스크립트를 통해 제어되고 브라우저에서 실행되는 최적화된 C++ 엔진으로 적절하게 구현될 수 있는 모든 사용 경우를 지원할 수 있어야 합니다. 그렇긴 하지만, 현대의 데스크톱 오디오 소프트웨어는 아주 발전된 능력을 가지고 있어서, 그것들 중 몇몇은 이 시스템으로 개발하기 어렵거나 불가능할지도 모릅니다. Apple의 Logic Audio가 그런 애플리케이션 중 하나인데, 이 애플리케이션은 외부 MIDI 컨트롤러, 임의 플러그인 오디오 이펙트, 신디사이저, 대단히 최적화된 직접 녹음형(direct-to-disk) 오디오 파일 읽기/쓰기, 단단히 통합된 time-stretching 등등을 지원합니다. 그럼에도 불구하고, 이 제안된 시스템은 적절히 복잡한 게임과 음악적인 애플리케이션을 포함하는 상호작용이 일어나는 애플리케이션의 넓은 범위를 꽤 지원할 수 있을 것입니다. 그리고 이 API는 WebGL에 의해 제공되는 아주 발전된 그래픽 기능의 아주 훌륭한 보완물이 될 수 있습니다. 이 API는 더욱 발전된 능력이 추후에 더해질 수 있도록 설계되었습니다.</p>
        <h2>기능</h2>
        <p>Web Audio API는 아래의 주요한 기능들을 지원합니다.</p>
        <ul>
          <li>간단하거나 복잡한 믹싱/이펙트 아키텍처를 위한 모듈러 라우팅</li>
          <li>높은 dynamic 범위. 내부 프로세싱에서 32비트 float을 사용.</li>
          <li>드럼 머신과 시퀸서와 같은 아주 높은 정도의 리듬 정밀도를 요구하는 음악 애플리케이션을 위한 낮은 레이턴시를 가지는 sample-accurate scheduled 사운드 재생. 이 기능은 또한 이펙트의 동적 생성의 가능성을 포함합니다.</li>
          <li>엔벨로프, 페이드인/페이드아웃, granular 이펙트, 필터 sweep, LFO 등을 위한 오디오 파라미터의 automation</li>
          <li>오디오 스트림에서의 유연한 채널 핸들링. 채널이 나눠지거나 합쳐질 수 있게 함.</li>
          <li>&lt;audio&gt; 또는 &lt;video&gt; 미디어 요소로부터의 오디오 소스 프로세싱
            <ul>
              <li>MediaStreamTrackAudioSourceNode와 [webrtc]를 사용한 원격 peer로부터 받은 오디오 프로세싱</li>
              <li>MediaStreamAudioDestinationNode와 [webrtc]를 사용해 원격 peer로 생성되거나 프로세싱된 오디오 스트림 전송</li>
            </ul>
          </li>
          <li>스크립트를 사용한 직접적 오디오 스트림 합성과 프로세싱</li>
          <li>3D 게임과 immersive한 환경의 넓은 범위를 지원하는 공간화된(spatialized) 오디오
            <ul>
              <li>패닝 모델: equalpower, HRTF, pass-through</li>
              <li>거리 감쇠(attenuation)</li>
              <li>sound cone</li>
              <li>방해/폐쇄 (obstuction/occlusion)</li>
              <li>음원(source)/청자(listener) 기반</li>
            </ul>
          </li>
          <li>선형 이펙트의 넓은 범위를, 특히 아주 높은 품질의 room 이펙트를 위한 convolution 엔진. 아래는 가능한 효과의 몇 가지 예입니다.
            <ul>
              <li>작은/큰 방(room)</li>
              <li>대성당(cathedral)</li>
              <li>콘서트 홀</li>
              <li>동굴(cave)</li>
              <li>터널(tunnel)</li>
              <li>복도(hallway)</li>
              <li>숲(forest)</li>
              <li>원형 극장(amphitheater)</li>
              <li>출입구를 거치는 멀리 있는 방으로부터의 소리</li>
              <li>extreme 필터</li>
              <li>기묘한 backward 이펙트</li>
              <li>extreme 빗(comb) 필터 이펙트</li>
            </ul>
          </li>
          <li>믹스의 전반적인 제어와 sweetening을 위한 dynamics compression</li>
          <li>효율적인 실시간 시간 영역(time-domain) 그리고 주파수 영역(frequency-domain) 분석 / 음악 시각화 지원</li>
          <li>lowpass, highpass, 그리고 다른 일반적인 필터를 위한 효율적인 biquad 필터</li>
          <li>distortion을 위한 waveshaping 효과와 다른 비선형적 효과</li>
          <li>오실레이터(oscilator)</li>
        </ul>
        <h3>모듈러 라우팅</h3>
        <p>모듈러 라우팅은 다른 AudioNode 객체 사이의 임의적인 연결을 가능케 합니다. 각 노드는 입력과 출력 양 쪽 모두 또는 어느 한 쪽을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력만을 가지고 있습니다. 목적지(destination) 노드는 하나의 입력을 가지고 출력이 없는 노드입니다. 필터와 같은 다른 노드들은 소스 노드와 목적지 노드 사이에 놓일 수 있습니다. 개발자는 두 객체가 함께 연결될 때의 저수준의 스트림 포맷 디테일에 대해 걱정할 필요가 없습니다. 옳은 일이 단지 일어나니까요. 예를 들자면, 만약 모노 오디오 스트림이 스테레오 입력에 연결되었다면 API는 단지 좌측 채널과 우측 채널을 적절히 믹스할 것입니다.</p>
        <p>가장 단순한 경우, 하나의 소스가 바로 출력으로 route될 수 있습니다. 모든 라우팅은 하나의 AudioDestinationNode를 포함하고 있는 AudioContext내에서 발생합니다.</p>
        <figure>
          <img src="img/modular-routing1.png" width="305" height="128">
          <figcaption><strong>도식 1</strong> 모듈러 라우팅의 단순한 예제</figcaption>
        </figure>
        <p>위 그림은 이 단순한 라우팅을 보여줍니다. 아래는 하나의 사운드를 재생하는 간단한 예제입니다.</p>
        <pre>예제 1
const context = new AudioContext();

function playSound() {
  const source = context.createBufferSource();
  source.buffer = dogBarkingBuffer;
  source.connect(context.destination);
  source.start(0);
}
        </pre>
        <p>아래는 세 개의 소스와 최종 출력 스테이지에서 dynamics compressor로 보내지는 convolution reverb를 보여주는 더욱 복잡한 예시입니다.</p>
        <figure>
          <img src="img/modular-routing2.png" width="561" height="411">
          <figcaption><strong>도식 2</strong> 모듈러 라우팅의 더욱 복잡한 예제</figcaption>
        </figure>
        <pre>예제 2
let context;
let compressor;
let reverb;
let source1, source2, source3;
let lowpassFilter;
let waveShaper;
let panner;
let dry1, dry2, dry3;
let wet1, wet2, wet3;
let mainDry;
let mainWet;
function setupRoutingGraph () {
  context = new AudioContext();
  // 이펙트 노드를 생성합니다.
  lowpassFilter = context.createBiquadFilter();
  waveShaper = context.createWaveShaper();
  panner = context.createPanner();
  compressor = context.createDynamicsCompressor();
  reverb = context.createConvolver();
  // main wet과 dry를 생성합니다.
  mainDry = context.createGain();
  mainWet = context.createGain();
  // 마지막 compressor를 마지막 목적지에 연결합니다.
  compressor.connect(context.destination);
  // main dry와 wet을 compressor에 연결합니다.
  mainDry.connect(compressor);
  mainWet.connect(compressor);
  // reverb를 main wet에 연결합니다.
  reverb.connect(mainWet);
  // 몇 가지 소스를 생성합니다.
  source1 = context.createBufferSource();
  source2 = context.createBufferSource();
  source3 = context.createOscillator();
  source1.buffer = manTalkingBuffer;
  source2.buffer = footstepsBuffer;
  source3.frequency.value = 440;
  // source1을 연결합니다.
  dry1 = context.createGain();
  wet1 = context.createGain();
  source1.connect(lowpassFilter);
  lowpassFilter.connect(dry1);
  lowpassFilter.connect(wet1);
  dry1.connect(mainDry);
  wet1.connect(reverb);
  // source2를 연결합니다.
  dry2 = context.createGain();
  wet2 = context.createGain();
  source2.connect(waveShaper);
  waveShaper.connect(dry2);
  waveShaper.connect(wet2);
  dry2.connect(mainDry);
  wet2.connect(reverb);
  // source3을 연결합니다.
  dry3 = context.createGain();
  wet3 = context.createGain();
  source3.connect(panner);
  panner.connect(dry3);
  panner.connect(wet3);
  dry3.connect(mainDry);
  wet3.connect(reverb);
  // 소스들을 지금 시작시킵니다.
  source1.start(0);
  source2.start(0);
  source3.start(0);
}
        </pre>
        <p>모듈러 라우팅은 또한 AudioNode들의 출력이 다른 AudioNode의 행동을 제어하는 AudioParam 파라미터에 route될 수 있게 합니다. 아래의 시나리오에서, 노드의 출력은 입력 시그널보다는 조절 시그널로서 행동합니다.</p>
        <figure>
          <img src="img/modular-routing3.png" width="346" height="337">
          <figcaption><strong>도식 3</strong> 하나의 오실레이터가 다른 오실레이터의 주파수를 조절하는 모습을 보여주는 모듈러 라우팅 시나리오</figcaption>
        </figure>
        <pre>예제 3
function setupRoutingGraph() {
  const context = new AudioContext();
  // 조절 시그널을 공급하는 낮은 주파수의 오실레이터를 생성합니다
  const lfo = context.createOscillator();
  lfo.frequency.value = 1.0;
  // 조절될 높은 주파수의 오실레이터를 생성합니다
  const hfo = context.createOscillator();
  hfo.frequency.value = 440.0;
  // gain 노드를 생성하는데 이 노드의 gain은 조절 시그널의 진폭을 결정합니다
  const modulationGain = context.createGain();
  modulationGain.gain.value = 50;
  // 그래프를 설정하고 오실레이터들을 시작시킵니다
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
        </pre>
        <h2>API 개요</h2>
        <p>정의되는 인터페이스들은 아래와 같습니다.</p>
        <ul>
          <li>AudioContext 인터페이스. 이 인터페이스는 AudioNode간의 연결을 표현하는 오디오 시그널 그래프를 포함합니다.</li>
          <li>AudioNode 인터페이스. 이 인터페이스는 오디오 소스, 오디오 출력, 중간 프로세싱 모듈을 표현합니다. AudioNode는 모듈러 방식으로 동적으로 연결될 수 있습니다. AudioNode는 AudioContext의 컨텍스트 내에서 존재합니다.</li>
          <li>AnalyserNode 인터페이스. 이는 음악 시각화나 다른 시각화 애플리케이션에서의 사용을 위한 AudioNode입니다.</li>
          <li>AudioBuffer 인터페이스. 이는 메모리에 상주하는(memory-resident) 오디오 에셋으로 작업하기 위한 것입니다. 이 오디오 에셋은 한 번의 소리, 혹은 더 긴 오디오 클립을 표현할 수 있습니다.</li>
          <li>AudioBufferSourceNode 인터페이스. 이는 AudioBuffer에서 오디오를 생성하는 AudioNode입니다.</li>
          <li>AudioDestinationNode 인터페이스. 모든 렌더링된 오디오에 대한 마지막 목적지를 나타내는 AudioNode의 서브클래스입니다.</li>
          <li>AudioParam 인터페이스. 볼륨과 같은 AudioNode 기능의 개별적 측면을 제어합니다.</li>
          <li>AudioListener 인터페이스. 공간화를 위해 PannerNode와 함께 사용됩니다.</li>
          <li>AudioWorklet 인터페이스. 스크립트를 사용해 직접 오디오를 프로세싱할 수 있는 사용자 정의 노드를 생성하기 위한 팩토리를 나타냅니다.</li>
          <li>AudioWorkletGlobalScope 인터페이스. AudioWorkletProcessor의 프로세싱 스크립트가 실행되는 컨텍스트입니다.</li>
          <li>AudioWorkletNode 인터페이스. AudioWorkletProcessor 내에서 프로세싱되는 노드를 나타내는 AudioNode입니다.</li>
          <li>AudioWorkletProcessor 인터페이스. 오디오 worker 내부의 단일 노드 인스턴스를 나타냅니다.</li>
          <li>BiquadFilterNode 인터페이스. 다음과 같은 일반적인 low-order 필터들에 대한 AudioNode입니다.
            <ul>
              <li>Low Pass</li>
              <li>High Pass</li>
              <li>Band Pass</li>
              <li>Low Shelf</li>
              <li>High Shelf</li>
              <li>Peaking</li>
              <li>Notch</li>
              <li>Allpass</li>
            </ul>
          </li>
          <li>ChannelMergerNode 인터페이스. 다수의 오디오 스트림으로부터의 채널을 하나의 오디오 스트림으로 결합하기 위한 AudioNode.</li>
          <li>ChannelSplitterNode 인터페이스. 라우팅 그래프에서 오디오 스트림의 개개의 채널에 접근하기 위한 AudioNode.</li>
          <li>ConstantSourceNode 인터페이스. AudioParam이 명목상의 constant 출력값의 automation을 허용하게 하며, 저 값을 생성하기 위한 AudioNode.</li>
          <li>ConvolverNode 인터페이스. (콘서트 홀의 소리와 같이) 실시간 선형 이펙트를 적용하기 위한 AudioNode.</li>
          <li>DelayNode 인터페이스. 동적으로 조정 가능한 가변적인 딜레이를 적용하는 AudioNode.</li>
          <li>DynamicsCompressorNode 인터페이스. dynamics compression을 위한 AudioNode.</li>
          <li>GainNode 인터페이스. 명시적인 gain 제어를 위한 AudioNode.</li>
          <li>IIRFilterNode 인터페이스. 일반적인 IIR 필터를 위한 AudioNode.</li>
          <li>MediaElementAudioSourceNode 인터페이스. &lt;audio&gt;, &lt;video&gt;, 혹은 그 밖의 미디어 요소로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamTrackAudioSourceNode 인터페이스. MediaStreamTrack으로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamAudioDestinationNode 인터페이스. 원격 peer에게 전송되는 MediaStream으로의 오디오 목적지인 AudioNode.</li>
          <li>PannerNode 인터페이스. 3D 공간에서의 공간화/포지셔닝을 위한 AudioNode.</li>
          <li>PeriodicWave 인터페이스. OscillatorNode에 의한 사용을 위한 사용자 정의 주기 파형을 명시하기 위한 인터페이스입니다.</li>
          <li>OscillatorNode 인터페이스. 주기 파형을 생성하기 위한 AudioNode.</li>
          <li>StereoPannerNode 인터페이스. 스테레오 스트림에서 오디오 입력의 equal-power 포지셔닝을 위한 AudioNode.</li>
          <li>WaveShaperNode 인터페이스. distortion과 그 밖의 다른 subtle warming 효과를 위한 비선형적 waveshaping 효과를 적용하는 AudioNode.</li>
        </ul>
        <p>Web Audio API에서 폐기(deprecated)되었지만 이것들의 대체물의 구현 경험이 있을 때 까지는 아직 제거되지 않을 몇몇 기능들이 있습니다.</p>
        <ul>
          <li>ScriptProcessorNode 인터페이스. 스크립트를 사용해 직접적으로 오디오를 생성하거나 프로세싱하기 위한 AudioNode.</li>
          <li>AudioProcessingEvent 인터페이스. ScriptProcessorNode 객체와 함께 사용되는 이벤트 유형입니다.</li>
        </ul>
        <h1>1. Audio API</h1>
        <h2>1.1. BaseAudioContext 인터페이스</h2>
        <p>BaseAudioContext 인터페이스는 AudioNode 객체들의 집합과 이 객체들의 연결을 나타냅니다. 이 인터페이스는 AudioDestinationNode로 향하는 임의적인 신호 라우팅을 고려합니다. 노드들은 이 컨텍스트에서 생성되고 그리고 나서 함께 연결됩니다.</p>
        <p>BaseAudioContext는 직접적으로 인스턴스화되지 않지만, 대신 실체가 있는 인터페이스인 AudioContext (실시간 렌더링용) 와 OfflineAudioContext (오프라인 렌더링용) 에 의해 상속됩니다.</p>
        <p>BaseAudioContext는 내부 슬롯 [[pending promises]] 를 가지고 생성되는데, 이 내부 슬롯은 초기적으로는 빈 promise의 정렬된 리스트입니다.</p>
        <p>각 BaseAudioContext는 고유의 미디어 요소 이벤트 태스크 소스를 가지고 있습니다. 추가적으로, BaseAudioContext는 두 개의 private 슬롯 [[rendering thread state]] 와 [[control thread state]]를 가지고 있는데, 이 슬롯들은 AudioContextState로부터 값을 취하며, 둘 다 초기적으로 "suspended"로 설정되어 있습니다.</p>
        <pre>
enum AudioContextState {
  "suspended",
  "running",
  "closed"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>suspended</i>"</td>
              <td>이 컨텍스트는 현재 연기되었습니다 (context time은 진행되지 않으며, audio 하드웨어는 전원이 차단되었거나/released 되었을 수 있습니다).</td>
            </tr>
            <tr>
              <td>"<i>running</i>"</td>
              <td>오디오가 처리되고 있습니다.</td>
            </tr>
            <tr>
              <td>"<i>closed</i>"</td>
              <td>이 컨텍스트는 released되었고, 더 이상 오디오를 처리하기 위해 사용될 수 없습니다. 모든 시스템 오디오 자원이 released되었습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
callback DecodeErrorCallback = undefined (DOMException error);

callback DecodeSuccessCallback = undefined (AudioBuffer decodedData);

[Exposed=Window]
interface BaseAudioContext : EventTarget {
  readonly attribute AudioDestinationNode destination;
  readonly attribute float sampleRate;
  readonly attribute double currentTime;
  readonly attribute AudioListener listener;
  readonly attribute AudioContextState state;
  [SameObject, SecureContext]
  readonly attribute AudioWorklet audioWorklet;
  attribute EventHandler onstatechange;

  AnalyserNode createAnalyser ();
  BiquadFilterNode createBiquadFilter ();
  AudioBuffer createBuffer (unsigned long numberOfChannels,
                            unsigned long length,
                            float sampleRate);
  AudioBufferSourceNode createBufferSource ();
  ChannelMergerNode createChannelMerger (optional unsigned long numberOfInputs = 6);
  ChannelSplitterNode createChannelSplitter (
    optional unsigned long numberOfOutputs = 6);
  ConstantSourceNode createConstantSource ();
  ConvolverNode createConvolver ();
  DelayNode createDelay (optional double maxDelayTime = 1.0);
  DynamicsCompressorNode createDynamicsCompressor ();
  GainNode createGain ();
  IIRFilterNode createIIRFilter (sequence<double> feedforward,
                                 sequence<double> feedback);
  OscillatorNode createOscillator ();
  PannerNode createPanner ();
  PeriodicWave createPeriodicWave (sequence<float> real,
                                   sequence<float> imag,
                                   optional PeriodicWaveConstraints constraints = {});
  ScriptProcessorNode createScriptProcessor(
    optional unsigned long bufferSize = 0,
    optional unsigned long numberOfInputChannels = 2,
    optional unsigned long numberOfOutputChannels = 2);
  StereoPannerNode createStereoPanner ();
  WaveShaperNode createWaveShaper ();

  Promise<AudioBuffer> decodeAudioData (
    ArrayBuffer audioData,
    optional DecodeSuccessCallback? successCallback,
    optional DecodeErrorCallback? errorCallback);
};          
        </pre>
        <h3>1.1.1. 어트리뷰트</h3>
        <dl>
          <dt><i>audioWorklet</i> / 자료형: AudioWorklet / 읽기 전용</dt>
          <dd>Worklet 객체로의 접근을 가능케 하는데, Worklet 객체는 [HTML] 과 AudioWorklet에 의해 정의된 알고리즘을 통해 AudioWorkletProcessor 클래스 정의를 포함하고 있는 스크립트를 import할 수 있습니다.</dd>
          <br>
          <dt></i>currentTime</i> / 자료형: double / 읽기 전용</dt>
          <dd>currentTime은 컨텍스트의 렌더링 그래프에 의해 가장 최근에 처리된 오디오 블럭의 마지막 sample frame 바로 다음의 샘플 프레임의 시간 (초) 입니다. 만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, currentTime은 0의 값을 갖습니다.</dd>
          <br>
          <dd>currentTime의 시간 좌표계에서, 0의 값은 그래프에 의해 처리되는 첫번째 블럭의 첫번째 샘플 프레임과 일치합니다. 이 계(system)에서 경과된 시간은 BaseAudioContext에 의해 생성된 오디오 스트림에서의 경과된 시간과 일치하는데, 이 시간은 이 계의 다른 시계와 동기화되지 않았을 수 있습니다. (OfflineAudioContext의 경우, 스트림이 어떠한 장치에서도 현재 재생되고 있지 않으므로, 실제 시간에 대한 근사치조차도 존재하지 않습니다.)</dd>
          <br>
          <dd>Web Audio API에서의 모든 예정된 시간들은 currentTime의 값에 관련이 있습니다.</dd>
          <br>
          <dd>BaseAudioContext가 "running" 상태일 때, 이 어트리뷰트의 값은, 하나의 render quantum에 일치하며, 단조적으로 증가하고 획일적인 증가 속에서 렌더링 스레드에 의해 갱신됩니다. 따라서, 실행 중인 컨텍스트의 경우, 이 계가 오디오 블럭을 처리해 가는 동안 currentTime은 꾸준히 증가하고, 항상 처리될 다음 오디오 블럭의 시작 시간을 나타냅니다. 현재 상태에서 예정된 어떠한 변화가 일어날지도 모를 때 이것은 또한 가능한 가장 빠른 시간입니다.</dd>
          <dd>currentTime은 반환되기 전에 반드시 control thread에서 atomically하게 read되어야만 합니다.</dd>
          <br>
          <dt><i>destination</i> / 자료형: AudioDestinationNode / 읽기 전용</dt>
          <dd>모든 오디오의 최종 목적지를 나타내는 하나의 입력을 가진 AudioDestinationNode. 보통 이것은 실제 오디오 하드웨어를 나타냅니다. 오디오를 현재 렌더링하고 있는 모든 AudioNode는 직접적으로 혹은 간접적으로 목적지에 연결됩니다.</dd>
          <br>
          <dt><i>listener</i> / 자료형: AudioListener / 읽기 전용</dt>
          <dd>3D 공간화에 사용되는 AudioListener</dd>
          <br>
          <dt><i>onstatechange</i> / 자료형: EventHandler</dt>
          <dd>AudioContext의 상태가 변경되었을 때 BaseAudioContext에 전파되는 이벤트에 대한 EventHandler를 설정하기 위해 사용되는 프로퍼티 (예: 해당하는 promise가 resolve되었을 때). Event 유형의 이벤트가 이벤트 핸들러에 전파될 것인데, 이는 AudioContext의 상태를 직접적으로 질의할 수 있습니다. 새롭게 생성된 AudioContext는 항상 suspended 상태에서 시작하고, 언제든지 다른 상태로 상태가 변경될 때 상태 변화 이벤트가 발생될 것입니다. 이 이벤트는 oncomplete 이벤트가 발생되기 이전에 발생됩니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>BaseAudioContext가 오디오를 다루는 (초당 샘플 프레임에서의) 샘플 레이트. 이것은 컨텍스트의 모든 AudioNode가 이 rate에서 실행될 것으로 추정합니다. 이 추정을 내림에 있어, 샘플 레이트 전환기 혹은 "varispeed" 프로세서는 실시간 프로세싱이 지원되지 않습니다. <strong><i>나이퀴스트 주파수</i></strong>는 이 샘플 레이트 값의 절반입니다.</dd>
          <br>
          <dt><i>state</i> / 자료형: AudioContextState / 읽기 전용</dt>
          <dd>BaseAudioContext의 현재 상태를 기술합니다. 이 어트리뷰트를 get하는 것은 [[control thread state]] 슬롯의 내용을 반환합니다.</dd>
        </dl>
        <br>
        <h3>1.1.2. 메서드</h3>
        <dl>
          <dt><i>createAnalyser()</i></dt>
          <dd>AnalyserNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AnalyserNode</dd>
          <br>
          <dt><i>createBiquadFilter()</i></dt>
          <dd>몇몇 일반적인 필터 유형 중 하나로써 설정될 수 있는 second order 필터를 나타내는 BiquadFilterNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: BiquadFilterNode</dd>
          <br>
          <dt><i>createBuffer(numOfChannels, length, sampleRate)</i></dt>
          <dd>주어진 크기의 AudioBuffer를 생성합니다. 버퍼 내의 오디오 데이터는 0으로 초기화될 것입니다 (silent). ⌛ 만약 인자가 어느 하나라도 음수거나, 0이거나, 명목상의 범위 바깥에 있다면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createBuffer() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 구현은 반드시 적어도 32개의 채널을 지원해야 합니다.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼 크기를 결정합니다. 이것은 반드시 적어도 1이어야 합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 구현은 반드시 적어도 범위 8000에서 96000까지의 샘플 레이트를 지원해야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: AudioBuffer</dd>
          <br>
          <dt><i>createBufferSource()</i></dt>
          <dd>AudioBufferSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioBufferSourceNode</dd>
          <br>
          <dt><i>createChannelMerger(numberOfInputs)</i></dt>
          <dd>channel merger를 나타내는 ChannelMergerNode의 팩토리 메서드. ⌛ 만약 numberOfInputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelMerger(numberOfInputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfInputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>입력의 수를 결정합니다. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelMergerNode</dd>
          <br>
          <dt><i>channelSplitterNode(numberOfOutputs)</i></dt>
          <dd>channel splitter를 나타내는 ChannelSplitterNode의 팩토리 메서드. ⌛ 만약 numberOfOutputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelSplitter(numberOfOutputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfOutputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>출력의 수. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelSplitterNode</dd>
          <br>
          <dt><i>createConstantSource()</i></dt>
          <dd>ConstantSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConstantSourceNode</dd>
          <br>
          <dt><i>createConvolver()</i></dt>
          <dd>ConvolverNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConvolverNode</dd>
          <br>
          <dt><i>createDelay(maxDelayTime)</i></dt>
          <dd>DelayNode의 팩토리 메서드. 초기 기본 delay time은 0초입니다.</dd>
          <table>
            <caption>BaseAudioContext.createDelay(maxDelayTime) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>maxDelayTime</td>
              <td>double</td>
              <td>x</td>
              <td>v</td>
              <td>지연선(delay line)을 고려하는 최대 delay time (초) 를 명시합니다. ⌛ 만약 명시되었다면, 이 값은 반드시 0보다 크고 3분보다 작아야 하며 그렇지 않다면 NotSupportedError 예외가 반드시 발생되어야 합니다. 만약 명시되지 않았다면, 1이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: DelayNode</dd>
          <br>
          <dt><i>createDynamicsCompressor()</i></dt>
          <dd>DynamicsCompressorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: DynamicsCompressorNode</dd>
          <br>
          <dt><i>createGain()</i></dt>
          <dd>GainNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: GainNode</dd>
          <br>
          <dt><i>createIIRFilter(feedforward, feedback)</i></dt>
          <table>
            <caption>BaseAudioContext.createIIRFilter(feedforward, feedback) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>feedforward</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedforward (분자) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 모든 값들이 0이라면, ⌛ InvalidStateError가 반드시 발생되어야 합니다. ⌛ 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
            <tr>
              <td>feedback</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedback (분모) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 이 배열의 첫번째 요소가 0이라면, ⌛ InvalidStateError가 반드시 발생되어야 합니다. ⌛ 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: IIRFilterNode</dd>
          <br>
          <dt><i>createOscillator()</i></dt>
          <dd>OscillatorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: OscillatorNode</dd>
          <br>
          <dt><i>createPanner()</i></dt>
          <dd>PannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: PannerNode</dd>
          <br>
          <dt><i>createPeriodicWave(real, imag, constraints)</i></dt>
          <dd>PeriodicWave를 생성하기 위한 팩토리 메서드</dd>
          <dd>이 메서드를 호출할 때, 다음의 단계를 실행합니다:</dd>
          <ol>
            <li>⌛ 만약 real과 imag가 같은 길이가 아니면, IndexSizeError가 반드시 발생되어야 합니다.</li>
            <li>o가 PeriodicWaveOptions 유형의 새로운 객체이게 합니다.</li>
            <li>이 팩토리 메서드에 전달된 real과 imag 파라미터를 각각 o에 같은 이름의 어트리뷰트로 설정합니다.</li>
            <li>o에 disableNormalization 어트리뷰트를 이 팩토리 메서드에 전달된 constraints 어트리뷰트의 disableNormalization 어트리뷰트의 값으로 설정합니다.</li>
            <li>BaseAudioContext에 이 팩토리 메서드가 첫번째 인자로서 호출되었음을 전달하며, 새로운 PeriodicWave p와, o를 생성합니다.</li>
            <li>p를 반환합니다.</li>
          </ol>
          <table>
            <caption>BaseAudioContext.createPeriodicWave() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>real</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>코사인 파라미터의 sequence. 더욱 자세한 설명은 real 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>imag</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>사인 파라미터의 sequence. 더욱 자세한 설명은 imag 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>constraints</td>
              <td>PeriodicWaveConstraints</td>
              <td>x</td>
              <td>v</td>
              <td>만약 주어지지 않았다면, 파형은 정규화(normalize)됩니다. 그렇지 않으면, 파형은 constraints에 의해 주어진 값에 따라 정규화됩니다.</td>
            </tr>
          </table>
          <dd>반환 유형: PeriodicWave</dd>
          <br>
          <dt><i>createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels)</i></dt>
          <dd>ScriptProcessorNode의 팩토리 메서드. 이 메서드는 폐기되었습니다. 왜냐하면 이 메서드는 AudioWorkletNode에 의해 대체되는 것이 의도되었기 때문입니다. 스크립트를 사용하는 직접적인 오디오 프로세싱을 위해 ScriptProcessorNode를 생성합니다. ⌛ 만약 bufferSize 또는 numberOfInputChannels 또는 numberOfOutputChannels가 유효한 범위 바깥이면 IndexSizeError가 반드시 발생되어야만 합니다.</dd>
          <dd>numberOfInputChannels와 numberOfOutputChannels 모두에 대해 값이 0인 것은 유효하지 않습니다. ⌛ 이 경우 IndexSizeError가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>bufferSize</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>bufferSize 파라미터는 샘플 프레임의 단위에서의 버퍼 크기를 결정합니다. 만약 이것이 전달되지 않았거나, 그 값이 0이라면, 구현은 주어진 환경에 가장 최선인 버퍼 크기를 선택할 것이고, 그 크기는 노드의 생명 주기 내내 2의 상수 제곱일 것입니다. 그렇지 않고 만약 코드 작성자가 명시적으로 bufferSize를 명시했다면, 이것은 반드시 다음의 값들 중 하나여야만 합니다: 256, 512, 1024, 2048, 4096, 8192, 16384. 이 값은 얼마나 자주 onaudioprocess 이벤트가 전파되고 얼마나 많은 샘플 프레임이 각 호출당 처리될 필요가 있는지를 제어합니다. 낮은 bufferSize 값은 낮은 (더 좋은) 레이턴시를 결과로 낳을 것입니다. 높은 값은 오디오 깨짐(breakup)과 glitch를 방지하기 위해 필수적일 것입니다. 코드 작성자가 이 버퍼 크기를 명시하지 않고 구현이 레이턴시와 오디오 품질을 균형잡는 좋은 버퍼 크기를 선택하도록 하는 것이 권장됩니다. ⌛ 만약 이 파라미터의 값이 위에서 나열된 2의 제곱 값들 중 하나가 아니라면, IndexSizeError가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>numberOfInputChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>이 파라미터는 이 노드의 입력에 대한 채널의 수를 결정합니다. 기본값은 2입니다. 32까지의 값이 반드시 지원되어야만 합니다. ⌛ 만약 채널의 수가 지원되지 않는다면 NotSupportedError가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>numberOfOutputChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>이 파라미터는 이 노드의 출력에 대한 채널의 수를 결정합니다. 기본값은 2입니다. 32까지의 값이 반드시 지원되어야만 합니다. ⌛ 만약 채널의 수가 지원되지 않는다면 NotSupportedError가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ScriptProcessorNode</dd>
          <br>
          <dt><i>createStereoPanner()</i></dt>
          <dd>StereoPannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: StereoPannerNode</dd>
          <br>
          <dt><i>createWaveShaper()</i></dt>
          <dd>비선형 왜곡을 나타내는 WaveShaperNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: WaveShaperNode</dd>
          <br>
          <dt><i>decodeAudioData(audioData, successCallback, errorCallback)</i></dt>
          <dd>ArrayBuffer에 포함된 오디오 파일 데이터를 비동기적으로 디코드합니다. 예를 들어 ArrayBuffer는 XMLHttpRequest의 responseType을 "arraybuffer"로 설정한 후에 response 어트리뷰트로부터 로딩될 수 있습니다. 오디오 파일 데이터는 &lt;audio&gt; 요소에 의해 지원되는 포맷입니다. decodeAudioData()에 전달된 버퍼는 sniffing에 의해 결정된 content-type을 가지며, 이는 [mimesniff]에 기술되어 있습니다. </dd>
          <dd>비록 이 함수의 interfacing의 주된 방법은 이것의 promise 반환 값을 통한 것이지만, callback 파라미터가 레거시 이유에 의해 제공됩니다.</dd>
          <dd>⌛ decodeAudioData가 호출되었을 때, 다음의 과정이 반드시 컨트롤 스레드에서 수행되어야만 합니다.</dd>
          <ol>
            <li>만약 이것의 관련된 global object의 연관된 Document가 완전히 active하지 않다면 "InvalidStateError" DOMException과 함께 rejected된 promise를 반환합니다.</li>
            <li>promise를 새로운 Promise라고 합시다.</li>
            <li>audioData에서의 IsDatachedBuffer 동작이 false라면 ([ECMASCRIPT]에 기술되어 있음), 다음의 과정을 수행합니다.
              <ol>
                <li>promise에 [[pending promises]]를 append합니다.</li>
                <li>audioData ArrayBuffer를 detach합니다. 이 동작은 [ECMASCRIPT]에 기술되어 있습니다. 만약 이 동작이 발생한다면, 단계 3으로 건너뜁니다.</li>
                <li>다른 스레드에서 수행될 decoding 동작을 queue합니다.</li>
              </ol>
            </li>
            <li>그렇지 않다면, 다음의 error 단계를 수행합니다.
              <ol>
                <li>error를 DataCloneError라고 합시다.</li>
                <li>promise를 error와 함께 reject하고, 이것을 [[pending promises]]로부터 제거합니다.</li>
                <li>errorCallback을 error와 함께 호출하는 media element task를 queue합니다.</li>
              </ol>
            </li>
            <li>promise를 반환합니다.</li>
          </ol>
          <dd>다른 스레드에서 수행될 decoding 동작을 queue할 때, 다음의 단계가 반드시 컨트롤 스레드도 아니고 렌더링 스레드도 아닌 스레드에서 발생되어야만 하고, 이것을 <strong><i>decoding 스레드</i></strong>라고 합니다.</dd>
          <div class="note"><p>알림: 다수의 decodeAudioData 호출을 제공하기 위해, 다수의 decoding 스레드는 병렬적으로 실행될 수 있습니다.</p></div>
          <ol>
            <li>can decode를 boolean 플래그라고 하고, 이는 초기적으로 true로 설정되어 있습니다.</li>
            <li>MIME Sniffing §6.2 오디오 또는 비디오 타입 패턴 매칭하기 를 사용하여, audioData의 MIME 유형 결정을 시도합니다. 만약 오디오 혹은 video 타입 패턴 매칭 알고리즘이 undefined를 반환한다면, can decode를 false로 설정합니다.</li>
            <li>만약 can decode가 true라면, 인코딩된 audioData를 linear PCM로 디코드 시도합니다. 실패의 경우에, can decode를 false로 설정합니다.</li>
            <li>만약 can decode가 false라면, 다음의 단계를 수행하는 media element task를 queue합니다.
              <ol>
                <li>error를 이름이 EncodingError인 DOMException이라고 합시다.
                  <ol>
                    <li>promise를 error와 함께 reject하고, 이것을 [[pending promises]]로부터 remove합니다.</li>
                  </ol>
                </li>
                <li>만약 errorCallback이 missing이 아니라면, errorCallback을 error와 함께 호출합니다.</li>
              </ol>
              <li>그렇지 않다면,
                <ol>
                  <li>디코드된 linear PCM 오디오 데이터를 나타내는 결과를 취하고, 이것을 BaseAudioContext의 샘플 레이트로 리샘플링합니다 (만약 이것이 audioData의 샘플 레이트와 다르다면).</li>
                  <li>다음의 단계를 수행하는 media element task를 queue합니다.
                    <ol>
                      <li>buffer를 최종 결과를 담고 있는 AudioBuffer라고 합시다 (아마 샘플 레이트 변환을 수행한 이후에).</li>
                      <li>promise를 buffer와 함께 resolve합니다.</li>
                      <li>만약 successCallback이 missing이 아니라면, successCallback을 buffer와 함께 호출합니다.</li>
                    </ol>
                  </li>
                </ol>
              </li>
            </li>
          </ol>
          <table>
            <caption>BaseAudioContext.decodeAudioData() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>audioData</td>
              <td>ArrayBuffer</td>
              <td>x</td>
              <td>x</td>
              <td>압축된 audio 데이터를 포함하고 있는 ArrayBuffer</td>
            </tr>
            <tr>
              <td>successCallback</td>
              <td>DecodeSuccessCallback?</td>
              <td>v</td>
              <td>v</td>
              <td>디코딩이 완료되었을 때 호출될 콜백 함수. 이 콜백에 대한 하나의 인자는 디코딩된 PCM 오디오 데이터를 나타내는 AudioBuffer입니다.</td>
            </tr>
            <tr>
              <td>errorCallback</td>
              <td>DecodeErrorCallback?</td>
              <td>v</td>
              <td>v</td>
              <td>만약 오디오 파일을 디코딩하는 중 오류가 있으면 호출될 콜백 함수</td>
            </tr>
          </table>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <br>
          <h3>1.1.3. 콜백 DecodeSuccessCallback() 파라미터</h3>
          <dl>
            <dt>decodedData / 유형: AudioBuffer</dt>
            <dd>디코드된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
          </dl>
          <br>
          <h3>1.1.4. 콜백 DecodeErrorCallback() 파라미터</h3>
          <dl>
            <dt>error / 유형: DOMException</dt>
            <dd>디코드 도중에 발생한 에러</dd>
          </dl>
        </dl>
        <br>
        <h3>1.1.5. 생애 주기</h3>
        <p>한 번 생성되고 나면, AudioContext는 자신이 재생할 소리가 더 이상 남아있지 않거나, 페이지가 꺼질 때까지 계속 소리를 재생합니다.</p>
        <br>
        <h3>1.1.6. 자기 성찰(introspection) 또는 직렬화 primitives(기본 요소?)의 부족</h3>
        <p>Web Audio API는 오디오 소스 스케쥴링에 사용하고 나서 잊는(fire-and-forget) 접근 방식을 취합니다. 즉, 소스 노드는 AudioContext의 생애 주기 동안 각 노트에 대해 생성되고, 절대로 명시적으로 그래프에서 제거되지 않습니다. 이것은 직렬화 API와 호환되지 않습니다. 왜냐하면 직렬화될 수 있는 노드들의 안정된 집합이 존재하지 않기 때문입니다.</p>
        <p>게다가, 자기 성찰 API를 가지는 것은 content 스크립트가 가비지 컬렉션을 관찰할 수 있게 허용할 것입니다.</p>
        <h3>1.1.7. BaseAudioContext 서브클래스와 연관된 시스템 자원</h3>
        <p>서브클래스 AudioContext와 OfflineAudioContext는 비용이 많이 드는(expensive) 객체로 여겨져야 합니다. 이 객체들은 생성하는 것은 높은 우선도의 스레드를 생성하는 것이나, 낮은 레이턴시의 시스템 오디오 스트림을 사용하는 것을 포함할 수도 있는데, 양측 다 에너지 소모에 영향을 가집니다. 문서에서 하나 이상의 AudioContext를 생성하는 것은 보통 필요하지 않습니다.</p>
        <p>BaseAudioContext 서브클래스를 구성하거나 재개하는 것은 그 컨텍스트에 대해 시스템 자원을 취득하는 것을 포함합니다. AudioContext에 대해, 이것은 또한 시스템 오디오 스트림의 생성을 요구합니다. 이 연산들은 컨텍스트가 컨텍스트의 연관된 오디오 그래프로부터 출력을 생성하는 것을 시작할 때 반환됩니다.</p>
        <p>추가적으로, 유저 에이전트는 구현이 정의된(implementation-defined) AudioContext의 최대 수를 가질 수 있는데, 이는 새로운 AudioContext를 생성하려는 시도가 실패한 이후에, NotSupportedError를 발생시킵니다.</p>
        <p>suspend와 close는 작성자로 하여금 시스템 자원을 해방하게 할 수 있는데, 이는 스레드, 프로세스, 오디오 스트림을 포함합니다. BaseAudioContext를 연기(suspend)하는 것은 BaseAudioContext의 일부 자원을 해방하는 구현을 허용하고, BaseAudioContext가 이후에 resume을 호출함으로써 작동을 계속하도록 허락합니다. AudioContext를 닫는(close)것은 AudioContext의 모든 자원을 해방하는 구현을 허락하는데, 이는 AudioContext가 사용될 수 없거나 다시 재개될 수 없는 이후입니다.</p>
        <div class="note">
          <p>노트: 예를 들어, 이것은 audio callback이 정기적으로 발생되는 것을 기다리거나, 하드웨어가 프로세싱을 위해 준비되기를 기다리는 것을 포함할 수 있습니다.</p>
        </div>
        <br>
        <h2>1.2. AudioContext 인터페이스</h2>
        <p>이 인터페이스는 유저에게 향하는 신호를 생산하는 실시간 출력 장치로 라우팅되는 AudioDestinationNode를 가지고 있는 오디오 그래프를 나타냅니다. 대부분의 사용 경우에, 오직 하나의 AudioContext가 문서당 사용됩니다.</p>
        <pre>
enum AudioContextLatencyCategory {
  "balanced",
  "interactive",
  "playback"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>balanced</i>"</td>
              <td>오디오 출력 레이턴시와 파워 소모의 균형을 유지합니다.</td>
            </tr>
            <tr>
              <td>"<i>interactive</i>"</td>
              <td>glitching이 없는 가능한 가장 낮은 오디오 출력 레이턴시를 제공합니다. 이것이 기본값입니다.</td>
            </tr>
            <tr>
              <td>"<i>playback</i>"</td>
              <td>오디오 출력 레이턴시에 대한 중단 없이 일관된 재생을 우선적으로 처리합니다. 파워 소모가 가장 적습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface AudioContext : BaseAudioContext {
  constructor (optional AudioContextOptions contextOptions = {});
  readonly attribute double baseLatency;
  readonly attribute double outputLatency;
  AudioTimestamp getOutputTimestamp ();
  Promise<undefined> resume ();
  Promise<undefined> suspend ();
  Promise<undefined> close ();
  MediaElementAudioSourceNode createMediaElementSource (HTMLMediaElement mediaElement);
  MediaStreamAudioSourceNode createMediaStreamSource (MediaStream mediaStream);
  MediaStreamTrackAudioSourceNode createMediaStreamTrackSource (
    MediaStreamTrack mediaStreamTrack);
  MediaStreamAudioDestinationNode createMediaStreamDestination ();
};
        </pre>
        <p>AudioContext는 user agent가 컨텍스트 상태가 "suspended"에서 "running"으로 전이되는 것을 허용한 경우 <strong>시작되도록 허용</strong>됩니다. user agent는 이 초기 전이를 비허용할 수 있으며 이것을 허용하기 위해서는 AudioContext'의 관련된 전역 객체가 sticky activation을 가지고 있어야만 합니다.</p>
        <p>AudioContext는 내부 슬롯을 가지고 있습니다:</p>
        <dl>
          <dt><i>[[suspended by user]]</i></dt>
          <dd>context가 사용자 코드에 의해 suspended되었는지를 나타내는 boolean flag. 초기값은 false.</dd>
        </dl>
        <br>
        <h3>1.2.1. 생성자</h3>
        <dl>
          <dt><i>AudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단시킵니다.</dd>
          <br>
          <dd>AudioContext를 생성할 때, 아래의 과정들을 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <div class="note">
          <p>노트: 프로그래밍적으로 작성자에게 AudioContext의 생성이 실패했다는 것을 알리는 것이 불행하게도 가능하지 않습니다. 유저 에이전트는 만약 작성자가 로깅 메커니즘에 접근권한을 가지고 있다면 정보 메시지를 로그할 것이 장려됩니다 (예: 개발자 도구 콘솔).</p>
        </div>
        <br>
        <table>
          <caption>AudioContext.constructor(contextOptions) 메서드의 인자</caption>
          <tr>
            <td>매개변수</td>
            <td>자료형</td>
            <td>nullable</td>
            <td>optional</td>
            <td>설명</td>
          </tr>
          <tr>
            <td>contextOptions</td>
            <td>AudioContextOptions</td>
            <td>x</td>
            <td>v</td>
            <td>AudioContext가 어떻게 생성되어야 하는지를 제어하는 유저가 명시한 옵션</td>
          </tr>
        </table>
        <br>
        <h3>1.2.2. 어트리뷰트</h3>
        <dl>
          <dt><i>baseLatency</i> / 자료형: double / 읽기 전용</dt>
          <dd>baseLatency는 AudioDestinationNode에서 audio subsystem으로 오디오를 전달하는 AudioContext에 의해 발생되는 프로세싱 레이턴시의 초를 나타냅니다. 이것은 AudioDestinationNode의 출력과 오디오 하드웨어 사이의 다른 프로세싱에 의해 유발될 수 있는 다른 추가적인 레이턴시를 포함하지 않으며 분명히 오디오 그래프 그 자체에 의해 발생되는 모든 레이턴시를 포함하지 않습니다.</dd>
          <br>
          <dd>예를 들어, 만약 오디오 컨텍스트가 44.1 kHz로 실행되고 AudioDestinationNode가 내부적으로 double buffering을 구현하고 각 render quantum마다 오디오를 처리하고 출력할 수 있다면, 이 경우 프로세싱 레이턴시는 (2 * 128) / 44100 = 5.805ms (근사치) 입니다.</dd>
          <br>
          <dt><i>outputLatency</i> / 자료형: double / 읽기 전용 / 현재 파이어폭스 엔진에만 있음</dt>
          <dd>오디오 출력 레이턴시의 추정 (단위: 초). 예: UA가 호스트 시스템에게 버퍼를 재생하는 것과 버퍼 내의 첫번째 샘플이 실제로 오디오 출력 장치에 의해 처리되는 시간을 요구하는 것. 음향 시그널을 생산하는 스피커와 헤드폰과 같은 장치에 대해서, 이 후자의 시간은 샘플의 소리가 생산될 때의 시간을 나타냅니다.</dd>
          <br>
          <dd>outputLatency 어트리뷰트 값은 플랫폼과 연결된 하드웨어 오디오 출력 장치에 달려 있습니다. outputLatency 어트리뷰트 값은 연결된 오디오 출력 장치가 동일한 이상 컨텍스트의 생명 주기에 대해 변하지 않습니다. 만약 오디오 출력 장치가 변경되었다면 outputLatency 어트리뷰트 값은 그에 맞추어 갱신될 것입니다.</dd>
        </dl>
        <br>
        <h3>1.2.3. 메서드</h3>
        <dl>
          <dt><i>close()</i></dt>
          <dd>사용되고 있던 시스템 자원을 해방하며 AudioContext를 닫습니다. 이것은 자동적으로 AudioContext가 생성한 모든 객체를를 해방하지 않을 것이지만, AudioContext의 currentTime의 진행을 유예할 것이고, 오디오 데이터를 프로세싱하는 것을 멈춥니다.</dd>
          <br>
          <dd>close가 호출되었을 때, 아래의 단계를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <br>
          <div class="note">
            <p>AudioContext가 closed되었을 때, 구현은 suspending되었을 때보다 더 공격적으로 자원을 해방하는 것을 선택할 수 있습니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>createMediaElementSource(mediaElement)</i></dt>
          <dd>주어진 HTMLMediaElement로 MediaElementAudioSourceNode를 생성합니다. 이 메서드를 호출한 결과로서, HTMLMediaElement로부터의 오디오 재생이 AudioContext의 프로세싱 그래프 내로 재라우팅(re-route)될 것입니다.</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaElementSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaElement</td>
              <td>HTMLMediaElement</td>
              <td>x</td>
              <td>x</td>
              <td>재라우팅될 미디어 요소</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaElementAudioSourceNode</dd>
          <br>
          <dt><i>createMediaStreamDestination</i></dt>
          <dd>MediaStreamAudioDestinationNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: MediaStreamAudiodestinationNode</dd>
          <br>
          <dt><i>createMediaStreamSource(mediaStream)</i></dt>
          <dd>MediaStreamAudioSourceNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaStreamSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaStream</td>
              <td>MediaStream</td>
              <td>x</td>
              <td>x</td>
              <td>소스(source)로서 작동할 미디어 스트림.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaStreamAudioSourceNode</dd>
          <br>
          <dt><i>getOutputTimestamp()</i></dt>
          <dd>컨텍스트에 대해 두 개의 연관된 오디오 스트림 포지션 값을 포함하고 있는 새로운 AudioTimeStamp 인스턴스를 반환합니다: contextTime 멤버는 (컨텍스트의 currentTime과 같은 단위와 기원에서) 오디오 출력 장치에 의해 현재 렌더링되고 있는 샘플 프레임의 시간 (예: 출력 오디오 스트림 포지션)  입니다; performanceTime 멤버는 (performance.now()와 같은 단위와 기원에서) 저장된 contextTime에 해당하는 샘플 프레임이 오디오 출력 장치에 의해 렌더링된 순간을 추산하는 시간을 포함합니다. (이는 [hr-time-3] 에서 기술됨).</dd>
          <br>
          <dd>만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, getOutputTimestamp 호출은 두 멤버가 모두 0을 포함하고 있는 AudioTimestamp 인스턴스를 반환합니다.</dd>
          <br>
          <dd>컨텍스트의 렌더링 그래프가 오디오 블럭의 처리를 시작한 이후에, 이것의 currentTime 어트리뷰트 값은 항상 getOutputTimestamp 메서드 호출로 인해 얻어진 contextTime 값을 넘어섭니다.</dd>
          <br>
          <pre>예제 4
getOutputTimestamp 메서드로부터 반환된 값은 컨텍스트의 시간 값보다 살짝 후의 퍼포먼스 시간 추산을 얻기 위해 사용될 수 있습니다:

function outputPerformanceTime(contextTime) {
  const timestamp = context.getOutputTimestamp();
  const elapsedTime = contextTime - timestamp.contextTime;
  return timestamp.performanceTime + elapsedTime * 1000;
}

상기의 예제에서 추산의 정확도는 인자 값이 얼마나 현재 출력 오디오 스트림 포지션에 가까운지에 달려 있습니다: 주어진 contextTime이 timestamp.contextTime에 가까울수록, 얻어지는 추산의 정확도는 좋아집니다.
          </pre>
          <br>
          <div class="note">
            <p>노트: 컨텍스트의 currentTime과 getOutputTimestamp 메서드 호출로부터 얻어진 contextTime의 차이는 신용 가능한 출력 레이턴시 추산으로써 여겨질 수 없습니다. 왜냐하면 currentTime은 비균일한 시간 구간에서 증가될 수 있으므로, outputLatency 어트리뷰트가 대신 사용되어야 합니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioTimestamp</dd>
          <br>
          <dt><i>resume()</i></dt>
          <dd>AudioContext가 연기되었을 때 AudioContext의 currentTime의 진행을 재개합니다.</dd>
          <dd>resume이 호출되었을 때, 아래의 절차를 실행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>suspend()</i></dt>
          <dd>AudioContext의 currentTime의 진행을 연기하고, 이미 처리된 모든 현재의 컨텍스트 프로세싱 블럭들이 목적지에서 재생되는 것을 허용하고, 그리고 나서 시스템이 오디오 하드웨어에 대한 주장(claim)을 해방하도록 허용합니다. 이것을 일반적으로 어플리케이션이 AudioContext가 어떤 때는 필요가 없을 것이라는 것을 알고, 일시적으로 AudioContext에 연관된 시스템 자원을 해방하기를 소원할 때 유용합니다. 이 promise는 프레임 버퍼가 비었을 때 (프레임 버퍼가 하드웨어에서 손을 뗐을 때), 혹은 일시적으로 (다른 효과 없이) 만약 컨텍스트가 이미 연기되었을 경우 resolve합니다. 이 promise는 컨텍스트가 closed되었을 때 rejected됩니다.</dd>
          <br>
          <dd>suspend가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <h3>1.2.4. AudioContextOptions</h3>
        <p>AudioContextOptions 딕셔너리는 AudioContext에 대해 사용자가 정의한 옵션들을 명시하기 위해 사용됩니다.</p>
        <br>
        <pre>
dictionary AudioContextOptions {
  (AudioContextLatencyCategory or double) latencyHint = "interactive";
  float sampleRate;
};
        </pre>
        <br>
        <h4>1.2.4.1. 딕셔너리 AudioContextOptions 멤버</h4>
        <dl>
          <dt><i>latencyHint</i> / 자료형: (AudioContextLatencyCategory 또는 double) / 기본값: "interactive"</dt>
          <dd>재생의 유형을 명시하는데, 이는 오디오 출력 레이턴시와 파워 소모 사이의 균형(tradeoff)에 영향을 미칩니다.</dd>
          <br>
          <dd>latencyHint의 선호된 값은 AudioContextLatencyCategory로부터의 값입니다. 그러나, double 또한 레이턴시와 파워 소모를 균형잡기 위한 더 좋은 제어를 위한 레이턴시 (단위: 초) 를 위해 명시될 수 있습니다. 이 숫자를 적절히 해석하는 것은 브라우저의 재량에 있습니다. 사용되는 실제 레이턴시는 AudioContext의 baseLatenct 어트리뷰트에 의해 주어집니다.</dd>
          <br>
          <dt><i>sampleRate / 자료형: float</i></dt>
          <dd>생성될 AudioContext에 대해 이 값에 sampleRate를 설정합니다. 이 지원된 값들은 AudioBuffer의 샘플 레이트와 같습니다. 만약 명시된 샘플 레이트가 지원되지 않는다면 NotSupportedError 예외가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>만약 sampleRate가 명시되지 않았다면, 이 AudioContext의 출력 장치의 선호되는 샘플 레이트가 사용됩니다.</dd>
        </dl>
        <br>
        <h3>1.2.5. AudioTimestamp</h3>
        <pre>
dictionary AudioTimestamp {
  double contextTime;
  DOMHighResTimeStamp performanceTime;
};
        </pre>
        <br>
        <h4>1.2.5.1. 딕셔너리 AudioTimestamp 멤버</h4>
        <dl>
          <dt><i>contextTime</i> / 자료형: double</dt>
          <dd>BaseAudioContext의 currentTime의 시간 좌표계의 점(point)를 나타냅니다.</dd>
          <br>
          <dt><i>performanceTime</i> / 자료형: DOMHighResTimeStamp</dt>
          <dd>퍼포먼스 인터페이스 구현의 시간 좌표계의 점을 나타냅니다 ([hr-time-3]에서 기술됩니다).</dd>
        </dl>
        <br>
        <h2>1.3. OfflineAudioContext 인터페이스</h2>
        <p>OfflineAudioContext는 실시간보다 (잠재적으로) 렌더링/믹싱다운을 더 빠르게 하기 위한 BaseAudioContext의 특정한 유형입니다. 이것은 오디오 하드웨어로 렌더링하지 않지만, 대신 가능한 한 빨리 렌더링하는데, AudioBuffer로 렌더링된 결과와 함께 반환된 promise를 fulfill합니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioContext : BaseAudioContext {
  constructor(OfflineAudioContextOptions contextOptions);
  constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate);
  Promise<AudioBuffer> startRendering();
  Promise<undefined> resume();
  Promise<undefined> suspend(double suspendTime);
  readonly attribute unsigned long length;
  attribute EventHandler oncomplete;
};          
        </pre>
        <br>
        <h3>1.3.1. 생성자</h3>
        <dl>
          <dt><i>OfflineAudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단합니다.</dd>
          <br>
          <dd>c를 새로운 OfflineAudioContext 객체라고 합니다. c를 다음과 같이 초기화합니다:</dd>
          <ol>
            <li>c에 대해 [[control thread state]]를 "suspended"로 설정합니다.</li>
            <li>c에 대해 [[rendering thread state]]를 "suspended"로 설정합니다.</li>
            <li>AudioDestinationNode의 channelCount가 contextOptions.numberOfChannels로 설정된 채로 AudioDestinationNode를 생성합니다.</li>
          </ol>
          <br>
          <table>
            <caption>OfflineAudioContext.constructor(contextOptions) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>contextOptions</td>
              <td></td>
              <td></td>
              <td></td>
              <td>이 컨텍스트를 생성하는 데 필요한 초기 파라미터들.</td>
            </tr>
          </table>
          <br>
          <dt><i>OfflineAudioContext(numberOfChannels, length, sampleRate)</i></dt>
          <dd>OfflineAudioContext는 AudioContext.createBuffer와 같은 인자로 생성될 수 있습니다. 만약 인자 중 하나라도 음수거나, 0이거나, 명목상의 범위 밖이면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>OfflineAudioContext는 아래가 대신 호출된 것처럼 생성됩니다.</dd>
          <br>
          <pre>
new OfflineAudioContext({
  numberOfChannels: numberOfChannels,
  length: length,
  sampleRate: sampleRate
})
          </pre>
          <table>
            <caption>OfflineAudioContext.constructor(numberOfChannels, length, sampleRate) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 지원되는 채널의 수는 createBuffer()를 참고하세요.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼의 크기를 결정합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼 내의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 유효한 샘플 레이트에 대해서는 createBuffer()를 참고하세요.</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.3.2. 어트리뷰트</h3>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임 내의 버퍼의 크기. 이것은 생성자의 length 파라미터의 값과 같습니다.</dd>
          <br>
          <dt><i>oncomplete</i> / 자료형: EventHandler</dt>
          <dd>OfflineAudioCompletionEvent 유형의 EventHandler. 이것은 OfflineAudioContext에서 발생되는 마지막 이벤트입니다.</dd>
        </dl>
        <br>
        <h3>1.3.3. 메서드</h3>
        <dl>
          <dt><i>startRendering()</i></dt>
          <dd>주어진 현재 연결과 예정된 변화를 가지고, 오디오 렌더링을 시작합니다.</dd>
          <br>
          <dd>비록 렌더링된 오디오 데이터를 얻는 일차적인 방법은 이것의 promise 반환 값을 통해서이지만, 이 인스턴스는 또한 레거시의 이유로 complete라는 이름의 이벤트를 발생시킬 것입니다.</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <br>
          <dt><i>resume()</i> / 현재 크롬 엔진에만 있음</dt>
          <dd>OfflineAudioContext의 currentTime의 진행을 OfflineAudioContext가 suspended되었을 때 재개시킵니다.</dd>
          <br>
          <dd>이 메서드가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <dt><i>suspend(suspendTime)</i></dt>
          <dd>특정한 시간에 오디오 컨텍스트에서의 시간 진행의 연기를 예정하고 promise를 반환합니다. 이거슨 일반적으로 OfflineAudioContext에서 오디오 그래프를 동기적으로(synchronously) 조작할 때 유용합니다.</dd>
          <br>
          <dd>연기의 최대 정밀도는 render quantum의 크기이고 명시된 연기 시간은 가장 가까운 render quantum 경계선(boundary) 까지에서 반올림될 것입니다. 이러한 이유로, 다수의 연기를 같은 quantized 프레임에 예정하는 것은 허용되지 않습니다. 또한, 정밀한 연기를 보장하기 위해 스케쥴링은 컨텍스트가 실행 중(running)이 아닌 동안에 이루어져야 합니다.</dd>
          <br>
          <table>
            <caption>OfflineAudioContext.suspend() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>suspendTime</td>
              <td>double</td>
              <td>x</td>
              <td>x</td>
              <td>
                명시된 시간에 렌더링의 연기를 예정하는데, 이는 quantized되고 render quantum 크기까지 반올림될 것입니다. 만약 quantized 프레임 수가
                <ol>
                  <li>음수거나</li>
                  <li>current time보다 작거나 혹은 같거나</li>
                  <li>전체 render duration보다 크거나 혹은 같거나</li>
                  <li>같은 시간에 또 다른 suspend가 예정되어 있다면</li>
                </ol>
                promise는 InvalidStateError와 함께 rejected됩니다.
              </td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
        </dl>
        <br>
        <h3>1.3.4. OfflineAudioContextOptions</h3>
        <p>이것은 OfflineAudioContext를 생성하는 데 쓰이는 옵션들을 명시합니다.</p>
        <pre>
dictionary OfflineAudioContextOptions {
  unsigned long numberOfChannels = 1;
  required unsigned long length;
  required float sampleRate;
};
        </pre>
        <h4>1.3.4.1. 딕셔너리 OfflineAudioContextOptions 멤버</h4>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long</dt>
          <dd>샘플 프레임에서의 렌더링된 AudioBuffer의 길이</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 기본값: 1</dt>
          <dd>이 OfflineAudioContext의 채널의 수.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float</dt>
          <dd>이 OfflineAudioContext의 샘플 레이트</dd>
        </dl>
        <br>
        <h3>1.3.5. OfflineAudioCompletionEvent 인터페이스</h3>
        <p>이것은 레거시적 이유로 OfflineAudioContext에 전파되는 Event 객체입니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioCompletionEvent : Event {
  constructor (DOMString type, OfflineAudioCompletionEventInit eventInitDict);
  readonly attribute AudioBuffer renderedBuffer;
};
        </pre>
        <h4>1.3.5.1. 어트리뷰트</h4>
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer / 읽기 전용</i></dt>
          <dd>렌더링된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
        </dl>
        <br>
        <h4>1.3.5.2. OfflineAudioCompletionEventInit</h4>
        <br>
        <pre>
dictionary OfflineAudioCompletionEventInit : EventInit {
  required AudioBuffer renderedBuffer;
};
        </pre>
        <h5>1.3.5.2.1. 딕셔너리 OfflineAudioCompletionEventInit 멤버</h5>        
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer</i></dt>
          <dd>이 이벤트의 renderedBuffer 어트리뷰트에 할당될 값</dd>
        </dl>
        <br>
        <h2>1.4. AudioBuffer 인터페이스</h2>
        <p>이 인터페이스는 메모리에 상주하는 오디오 에셋을 나타냅니다. 이것은 하나 이상의 채널을 포함할 수 있는데 각 채널은 [-1, 1]의 명목상의 범위를 가지는 32비트 부동 소수점 선형 PCM 값을 가지고 있는 것으로 보이나 이 값들은 이 범위에 국한되지 않습니다. 통상적으로, PCM 데이터의 길이는 꽤 짧을 것이 기대됩니다 (보통 1분보다 짧은 정도). 음악 사운드트랙같은 더 긴 소리들에 대해서는, 스트리밍은 audio 요소와 MediaElementAudioSourceNode와 함께 사용되어야 합니다.</p>
        <br>
        <p>AudioBuffer는 하나 이상의 AudioContext에 의해 사용될 수 있고, OfflineAudioContext와 AudioContext 사이에서 공유될 수 있습니다.</p>
        <br>
        <p>AudioBuffer는 4개의 내부 슬롯을 갖습니다.</p>
        <dl>
          <dt><i>[[number of channels]]</i></dt>
          <dd>이 AudioBuffer의 오디오 채널의 수. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[length]]</i></dt>
          <dd>이 AudioBuffer의 각 채널의 길이. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[sample rate]]</i></dt>
          <dd>이 AudioBuffer의 샘플 레이트 (단위: Hz). / 자료형: float</dd>
          <br>
          <dt><i>[[internal data]]</i></dt>
          <dd>오디오 샘플 데이터를 가지고 있는 데이터 블럭.</dd>
        </dl>
        <br>
        <pre>
[Exposed=Window]
interface AudioBuffer {
  constructor (AudioBufferOptions options);
  readonly attribute float sampleRate;
  readonly attribute unsigned long length;
  readonly attribute double duration;
  readonly attribute unsigned long numberOfChannels;
  Float32Array getChannelData (unsigned long channel);
  undefined copyFromChannel (Float32Array destination,
                             unsigned long channelNumber,
                             optional unsigned long bufferOffset = 0);
  undefined copyToChannel (Float32Array source,
                           unsigned long channelNumber,
                           optional unsigned long bufferOffset = 0);
};
        </pre>
        <br>
        <h3>1.4.1. 생성자</h3>
        <dl>
          <dt><i>AudioBuffer(options)</i></dt>
          <dd>추후 번역</dd>
          <br>
          <table>
            <caption>AudioBuffer.constructor() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>options</td>
              <td>AudioBufferOptions</td>
              <td>x</td>
              <td>x</td>
              <td>이 AudioBuffer의 프로퍼티를 결정하는 AudioBufferOptions</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.4.2. 어트리뷰트</h3>
        <dl>
          <dt><i>duration</i> / 자료형: double / 읽기 전용</dt>
          <dd>PCM 오디오 데이터의 지속 기간 (단위: 초)</dd>
          <br>
          <dd>이것은 [[length]]와 [[sample rate]] 사이에서 나눗셈을 수행함으로써 AudioBuffer의 [[sample rate]]와 [[length]]로부터 계산됩니다.</dd>
          <br>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임의 PCM 오디오 데이터의 길이. 이것은 반드시 [[length]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>이산 오디오 채널의 수. 이것은 반드시 [[number of channels]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>초당 샘플 내의 PCM 오디오 데이터의 샘플 레이트. 이것은 반드시 [[sample rate]]의 값을 반환해야만 합니다.</dd>
        </dl>
        <br>
        <h3>1.4.3. 메서드</h3>
        <dl>
          <dt><i>copyFromChannel(destination, channelNumber, bufferOffset)</i></dt>
          <dd>copyFromChannel() 메서드는 AudioBuffer의 명시된 채널로부터 destination 배열로 샘플들을 복제합니다.</dd>
          <br>
          <dd>buffer를 N<sub>b</sub> 프레임을 가진 AudioBuffer라고 하고, N<sub>f</sub>를 destination 배열의 원소의 수라고 하고, k를 bufferOffset의 값이라고 합시다. 그럼 buffer로부터 destination으로 복제된 프레임의 수는 max(0, min(N<sub>b</sub>-k, N<sub>f</sub>))입니다. 만약 이것이 N<sub>f</sub>보다 작다면, destination의 잔여 원소는 수정되지 않습니다.</dd>
          <table>
            <caption>AudioBuffer.copyFromChannel() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destination</td>
              <td>Float32Array</td>
              <td>x</td>
              <td>x</td>
              <td>채널 데이터가 복제되어 들어갈 배열</td>
            </tr>
            <tr>
              <td>channelNumber</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>데이터를 복제해올 채널의 인덱스. 만약 channelNumber가 AudioBuffer의 채널 수보다 크거나 같다면, ⌛ IndexSizeError가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>bufferOffset</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>선택적인 offset으로, 기본값은 0입니다. 이 offset에서 시작하는 AudioBuffer로부터의 데이터가 destination에 복제됩니다.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>copyToChannel(source, channelNumber, bufferOffset)</i></dt>
          <dd>copyToChannel() 메서드는 source 배열로부터 AudioBuffer의 명시된 채널로 샘플을 복제합니다.</dd>
          <br>
          <dd>만약 source가 버퍼로 복제될 수 없다면 UnknownError가 발생될 수 있습니다.</dd>
          <br>
          <dd>buffer가 N<sub>b</sub> 프레임을 가진 AudioBuffer라고 하고, N<sub>f</sub>가 source 배열의 원소의 수라고 하고, k가 bufferOffset의 값이라고 합시다. 그럼 source로부터 buffer에 복제되는 프레임의 수는 max(0, min(N<sub>b</sub>-k, N<sub>f</sub>))입니다. 만약 이것이 N<sub>f</sub>보다 작다면, buffer의 잔여 원소는 수정되지 않습니다.</dd>
          <table>
            <caption>AudioBuffer.copyToChannel() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>source</td>
              <td>Float32Array</td>
              <td>x</td>
              <td>x</td>
              <td>채널 데이터가 복제될 배열</td>
            </tr>
            <tr>
              <td>channelNumber</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>데이터를 복제해 넣을 채널의 인덱스. 만약 channelNumber가 AudioBuffer의 채널의 수보다 크거나 같다면, ⌛ IndexSizeError가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>bufferOffset</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>선택적인 offset으로, 기본값은 0입니다. source로부터의 데이터가 이 offset에서 시작하는 AudioBuffer에 복제됩니다.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>getChannelData(channel)</i></dt>
          <dd>acquire the content에 기술된 규칙에 따라, 참조를 얻거나 [[internal data]]에 저장된 바이트의 복제본을 새로운 Float32Array에 얻습니다.</dd>
          <br>
          <dd>만약 [[internal data]] 또는 새로운 Float32Array가 생성될 수 없다면 UnknownError가 발생될 수 있습니다.</dd>
          <br>
          <table>
            <caption>AudioBuffer.getChannelData() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>channel</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>이 매개변수는 데이터를 얻을 특정한 채널을 나타내는 인덱스입니다. 0의 인덱스 값은 첫번째 채널을 나타냅니다. ⌛ 인덱스 값은 반드시 [[number of channels]]보다 작아야만 하며 그렇지 않을 경우 IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: Float32Array</dd>
          <br>
          <div class="note">
            <p>알림: copyToChannel() 메서드와 copyFromChannel() 메서드는 더 큰 배열의 view인 Float32Array를 전달함으로써 배열의 부분을 채우기 위해 사용될 수 있습니다. AudioBuffer의 채널로부터 데이터를 읽고, 데이터가 청크로 처리될 수 있을 때, copyFromChannel()이 getChannelData()를 호출하고 결과 배열에 접근하는 것에 비해 선호되어야 합니다. 왜냐하면 이것은 불필요한 메모리 할당과 복제를 방지할 수 있기 때문입니다.</p>
          </div>
          <br>
          <dd>내부 동작 acquire the contents of an AudioBuffer는 AudioBuffer의 내용이 몇몇 API 구현에 의해 필요되어질 때 호출됩니다. 이 동작은 호출자에게 불변하는 채널 데이터를 반환합니다.</dd>
          <dd><strong>acquire the content</strong> 동작이 AudioBuffer에서 발생되었을 때, 다음의 과정을 실행합니다.</dd>
          <ol>
            <li>만약 AudioBuffer의 ArrayBuffer중 어느 것에서든 동작 IsDetachedBuffer가 true를 반환한다면, 이 과정들을 중단하고, 호출자에게 길이 0의 채널 데이터 버퍼를 반환합니다.</li>
            <li>이 AudioBuffer에서 이전에 getChannelData()에 의해 반환된 배열들에 대해 모든 ArrayBuffer를 분리합니다.
              <div class="note">
                <p>알림: AudioBuffer는 오직 createBuffer() 또는 AudioBuffer 생성자를 통해서만 생성될 수 있기 때문에, 이것은 throw할 수 없습니다.</p>
              </div>
            </li>
            <li>저 ArrayBuffer들로부터의 근본적인 [[internal data]]를 유지하고 그것들에 대한 참조를 호출자에게 반환합니다.</li>
            <li>데이터의 복제본을 가지고 있는 ArrayBuffer들을 AudioBuffer에 부착하고, 이는 다음 getChannelData() 호출에 의해 반환될 것입니다.</li>
          </ol>
          <dd>acquire the contents of an AudioBuffer 동작은 다음의 경우에 호출됩니다.</dd>
          <ul>
            <li>AudioBufferSourceNode.start가 호출되었을 때, 이것은 노드의 버퍼의 내용을 획득합니다. 만약 이 동작이 실패한다면, 아무것도 재생되지 않습니다.</li>
            <li>AudioBufferSourceNode의 버퍼가 설정되었고 AudioBufferSourceNode.start가 이전에 호출되었을 때, setter는 AudioBuffer의 내용을 획득합니다. 만약 이 동작이 실패한다면, 아무것도 재생되지 않습니다.</li>
            <li>ConvolverNode의 buffer가 AudioBuffer로 설정되었을 때 이것은 AudioBuffer의 내용을 획득합니다.</li>
            <li>AudioProcessingEvent의 전파가 완료되었을 때, 이것은 이것의 outputBuffer의 내용을 획득합니다.
            </li>
          </ul>
          <div class="note">
            <p>알림: 이는 copyToChannel()이 AudioBuffer의 내용을 획득한 AudioNode에 의해 현재 사용되고 있는 AudioBuffer의 내용을 변경하기 위해 사용될 수 없음을 의미합니다. 왜냐하면 AudioNode는 이전에 획득된 데이터를 계속 사용할 것이기 때문입니다.</p>
          </div>
        </dl>
        <br>
        <h3>1.4.4. AudioBufferOptions</h3>
        <p>이는 AudioBuffer를 생성할 때 사용할 옵션을 명시합니다. length와 sampleRate 멤버가 요구됩니다.</p>
        <pre>
dictionary AudioBufferOptions {
  unsigned long numberOfChannels = 1;
  required unsigned long length;
  required float sampleRate;
};
        </pre>
        <br>
        <h4>1.4.4.1. 딕셔너리 AudioBufferOptions 멤버</h4>
        <p>이 딕셔너리의 멤버들의 허용되는 값은 제한되어 있습니다. createBuffer()를 참고하십시오.</p>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long</dt>
          <dd>버퍼의 샘플 프레임의 길이. 제약 사항에 대해서는 length를 참고하십시오.</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 기본값: 1</dt>
          <dd>버퍼의 채널의 수. 제약 사항에 대해서는 numberOfChannels를 참고하십시오.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float</dt>
          <dd>버퍼의 샘플 레이트 (단위: Hz). 제약 사항에 대해서는 sampleRate를 참고하십시오.</dd>
        </dl>
        <br>
        <h2>1.5. AudioNode 인터페이스</h2>
        <p>AudioNode는 AudioContext의 구성 요소입니다. 이 인터페이스는 오디오 소스, 오디오 목적지, 중간 프로세싱 모듈을 나타냅니다. 이 모듈들은 오디오를 렌더링하여 오디오 하드웨어로 보내는 프로세싱 그래프를 형성하기 위해 함께 연결될 수 있습니다. 각 노드들은 입력 그리고/또는 출력을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력이 있습니다. 필터와 같은 대부분의 프로세싱 노드들은 하나의 입력과 하나의 출력을 갖습니다. 각 유형의 AudioNode는 어떻게 이것이 오디오를 처리하거나 합성하는지에 대한 세부 사항이 다릅니다. 하지만, 일반적으로, AudioNode는 (입력이 있다면) 입력을 처리하고, (출력이 있다면) 출력으로 오디오를 생성합니다.</p>
        <br>
        <p>각 출력은 하나 이상의 채널을 가집니다. 채널의 정확한 수는 각 AudioNode의 세부 사항에 달려 있습니다.</p>
        <br>
        <p>출력은 하나 이상의 AudioNode 입력에 연결될 수 있으므로, 팬 아웃이 지원됩니다. 입력은 초기적으로 연결을 가지고 있지 않지만, 하나 이상의 AudioNode 출력으로부터 연결될 수 있으므로, 핸 인이 지원됩니다. connect() 메서드가 AudioNode의 출력을 다른 AudioNode의 입력에 연결하기 위해 호출되었을 때, 우리는 이것을 입력에의 연결이라고 부릅니다.</p>
        <br>
        <p>각 AudioNode 입력은 특정한 수의 채널을 주어진 시간에 가지고 있습니다. 이 숫자는 입력에 만들어진 연결(들)에 따라 바뀔 수 있습니다. 만약 입력이 연결을 가지고 있지 않다면 이것은 소리가 없는(silent) 하나의 채널을 가지고 있습니다.</p>
        <br>
        <p>각 입력에 대해, AudioNode는 그 입력으로 가는 모든 연결의 믹싱을 수행합니다. 규범적인 필요조건과 세부 사항에 대해서는 § 4 채널 업 믹싱과 다운 믹싱을 참고해 보세요.</p>
        <br>
        <p>입력의 프로세싱과 AudioNode의 내부 연산은 AudioContext 시간에 대하여 계속하여 발생하는데, 이는 노드가 연결된 출력을 가지고 있는지의 여부에 관계없으며, 이 출력이 궁극적으로 AudioContext의 AudioDestinationNode에 도달하는지의 여부에도 관계없습니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioNode : EventTarget {
  AudioNode connect (AudioNode destinationNode,
                     optional unsigned long output = 0,
                     optional unsigned long input = 0);
  undefined connect (AudioParam destinationParam, optional unsigned long output = 0);
  undefined disconnect ();
  undefined disconnect (unsigned long output);
  undefined disconnect (AudioNode destinationNode);
  undefined disconnect (AudioNode destinationNode, unsigned long output);
  undefined disconnect (AudioNode destinationNode,
                        unsigned long output,
                        unsigned long input);
  undefined disconnect (AudioParam destinationParam);
  undefined disconnect (AudioParam destinationParam, unsigned long output);
  readonly attribute BaseAudioContext context;
  readonly attribute unsigned long numberOfInputs;
  readonly attribute unsigned long numberOfOutputs;
  attribute unsigned long channelCount;
  attribute ChannelCountMode channelCountMode;
  attribute ChannelInterpretation channelInterpretation;
};
        </pre>
        <br>
        <h3>1.5.1. AudioNode 생성</h3>
        <p>AudioNode는 두 가지 방법으로 생성될 수 있는데 그 방법이란 특정한 인터페이스에 대한 생성자를 사용하는 것 혹은 BaseAudioContext 또는 AudioContext의 <strong>팩토리 메서드</strong>를 사용하는 것입니다.</p>
        <br>
        <p>AudioNode의 생성자의 첫번째 인자로써 전달되는 BaseAudioContext는 생성될 AudioNode의 <strong>연관된 BaseAudioContext</strong>라고 불려집니다. 유사하게, 팩토리 메서드를 사용할 때, AudioNode의 연관된 BaseAudioContext는 이 팩토리 메서드가 호출된 BaseAudioContext입니다.</p>
        <br>
        <p>팩토리 메서드를 사용하여, BaseAudioContext c에서 호출된, 특정한 유형 n의 새로운 AudioNode를 생성하기 위해서는, 다음의 절차를 수행합니다.</p>
        <br>
        <ol>
          <li>node를 유형 n의 새로운 객체라고 합시다.</li>
          <li>option을 이 팩토리 메서드에 연관된 인터페이스에 대한 연관된 유형의 딕셔너리라고 합시다.</li>
          <li>이 팩토리 메서드에 전달된 각 매개변수에 대해, option에 있는 같은 이름의 딕셔너리 멤버를 이 매개변수의 값으로 설정합니다.</li>
          <li>인자로서 c와 option과 함께 node상에서 n에 대한 생성자를 호출합니다.</li>
          <li>node를 반환합니다</li>
        </ol>
        <br>
        <p>AudioNode를 상속받는 객체 o를 <strong>초기화</strong>한다는 것은, 이 인터페이스의 생성자에 전달된 context와 dict 인자가 주어졌을 때 다음의 절차를 수행한다는 것을 의미합니다.</p>
        <br>
        <ol>
          <li>o의 연관된 BaseAudioContext를 context로 설정합니다.</li>
          <li>numberOfInputs, numberOfOutputs, channelCount, channelCountMode, channelInterpretation의 값을 기본값으로 설정하는데, 이 기본값은 각 AudioNode의 섹션에 기술된 이 특정한 인터페이스의 기본값입니다.</li>
          <li>전달된 dict의 각 멤버에 대해 다음의 절차를 수행하는데, k가 멤버의 키고, v가 멤버의 값입니다. 만약 이 절차를 수행하는 중에 어떠한 예외가 발생된다면, 반복을 중단하고 예외를 알고리즘의 호출자(생성자 또는 팩토리 메서드)에게 전파합니다.
            <ol>
              <li>만약 k가 이 인터페이스의 AudioParam의 이름이라면, 이 AudioParam의 value 어트리뷰트를 v로 설정합니다.</li>
              <li>위가 아니고, 만약 k가 이 인터페이스의 어트리뷰트의 이름이라면, 이 어트리뷰트에 연관된 객체를 v로 설정합니다.</li>
            </ol>
          </li>
        </ol>
        <br>
        <p>팩토리 메서드의 <strong>연관된 인터페이스</strong>란 이 메서드로부터 반환된 객체의 인터페이스입니다. 인터페이스의 <strong>연관된 option 객체</strong>란 이 인터페이스의 생성자에 전달될 수 있는 option 객체입니다.</p>
        <br>
        <p>AudioNode는 [DOM]에 기술된 대로, EventTarget입니다. 이것이 의미하는 바는 다른 EventTarget들이 이벤트를 받아들이는 것과 같은 방식으로 AudioNode에 이벤트를 전파할 수 있다는 것입니다.</p>
        <br>
        <pre>
enum ChannelCountMode {
  "max",
  "clamped-max",
  "explicit"
};
        </pre>
        <br>
        <p>노드의 channelCount와 channelInterpretation 값과 함께, ChannelCountMode는, 어떻게 노드에의 입력이 믹스될 지를 제어하는 <strong><i>computedNumberOfChannels</i></strong>를 결정하기 위해 사용됩니다. computedNumberOfChannels는 아래에서 보여지는 것과 같이 결정됩니다. 어떻게 믹싱이 이루어지는지에 대해서는 § 4 채널 업믹싱과 다운믹싱을 참고하십시오.</p>
        <br>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>speakers</i>"</td>
              <td>업믹스 방정식 또는 다운믹스 방정식을 사용합니다. 채널의 수가 기본 스피커 레이아웃의 어느 것과도 일치하지 않는 경우에는, "discrete"로 되돌아갑니다.</td>
            </tr>
            <tr>
              <td>"<i>discrete</i>"</td>
              <td>채널이 다 떨어질 때까지 채널을 채움으로써 업믹스하고, 그리고 나서 남은 채널들을 0으로 맞춥니다. 가능한 한 많은 채널을 채움으로써 다운믹스하고, 그리고 나서 남은 채널들을 탈락시킵니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
enum ChannelInterpretation {
  "speakers",
  "discrete"
};
        </pre>
        <br>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>max</i>"</td>
              <td>computedNumberOfChannels는 입력의 모든 연결의 채널의 수의 최대입니다. 이 모드에서 channelCount는 무시됩니다.</td>
            </tr>
            <tr>
              <td>"<i>clamped-max</i>"</td>
              <td>computedNumberOfChannels는 "max"에서처럼 결정되고 주어진 channelCount의 최대 값으로 clamp됩니다.</td>
            </tr>
            <tr>
              <td>"<i>explicit</i>"</td>
              <td>computedNumberOfChannels는 channelCount에 의해 명시된 값입니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <h3>1.5.2. AudioNode 꼬리 시간(tail-time)</h3>
        <p>AudioNode는 꼬리 시간을 가질 수 있습니다. 이는 AudioNode에 아무 소리도 입력되지 않았을 지라도(fed silence), 출력은 소리가 있을 수 있다(non-silent)는 것입니다.</p>
        <br>
        <p>만약 과거의 입력이 미래의 출력에 영향을 주는 것과 같은 내부 프로세싱 상태를 AudioNode가 가지고 있다면 AudioNode는 0이 아닌 꼬리 시간을 가집니다. AudioNode는 심지어 입력이 소리 있음에서 소리 없음으로 전이된 후일지라도 계산된 꼬리 시간동안 소리가 있는 출력을 생성하는 것을 계속할 수도 있습니다.</p>
        <br>
        <h3>1.5.3. AudioNode 생명 주기</h3>
        <p>만약 다음의 조건 중 어느 하나라도 부합한다면, AudioNode는 render quantum 도중에 활발히 처리 중(actively processing)일 수 있습니다.</p>
        <ul>
          <li>AudioScheduledSourceNode는 적어도 현재 렌더링되는 quantum의 부분을 재생하고 있을 때에만 actively processing입니다.</li>
          <li>MediaElementAudioSourceNode는 이것의 mediaElement가 적어도 현재 렌더링되는 quantum의 부분을 재생하고 있을 때에만 actively processing입니다.</li>
          <li>MediaStreamAudioSourceNode 또는 MediaStreamTrackAudioSourceNode는 연관된 MediaStreamTrack 객체가 "live"인 readyState 어트리뷰트, false인 muted 어트리뷰트, 그리고 true인 enabled 어트리뷰트를 가지고 있을 때 actively processing입니다.</li>
          <li>사이클 안에 있는 DelayNode는 오직 현재 render quantum의 출력 샘플의 절댓값이 2^-126보다 같거나 클 때에만 actively processing입니다.</li>
          <li>ScriptProcessorNode는 입력 또는 출력이 연결되어 있을 때 actively processing입니다.</li>
          <li>AudioWorkletNode는 AudioWorkletProcessor의 [[callable process]]가 true를 반환하고 이것의 active source flag가 true이거나 이것의 입력 중 하나에 연결된 AudioNode가 actively processing일 때 actively processing입니다.</li>
          <li>모든 다른 AudioNode는 이것의 입력 중 하나에 연결된 AudioNode가 actively processing일 때 actively processing을 시작하고, 다른 actively processing인 AudioNode로부터 전달받은 입력이 더 이상 출력에 영향을 미치지 않을 때 actively processing을 멈춥니다.</li>
        </ul>
        <br>
        <div class="note">
          <p>알림: 이것은 꼬리 시간을 가진 AudioNode를 고려합니다.</p>
        </div>
        <br>
        <p>활발히 처리 중이 아닌 AudioNode는 무음을 내는 채널 하나를 출력합니다.</p>
        <br>
        <h3>1.5.4. 어트리뷰트</h3>
        <dl>
          <dt><i>channelCount</i> / 자료형: unsigned long</dt>
          <dd>channelCount는 노드의 입력에 대한 연결을 업믹싱하거나 다운믹싱할 때 사용되는 채널의 수입니다. 특정 노드를 제외하고 기본값은 2이며, 특정 노드의 경우 이 값은 특별히 결정되어 있습니다. 이 어트리뷰트는 입력이 없는 노드에 대해서는 효과를 가지지 않습니다. ⌛ 만약 이 값이 0이나 구현의 최대 채널 수보다 큰 값으로 설정되었다면 구현은 반드시 NotSupportedError 예외를 발생시켜야만 합니다.</dd>
          <br>
          <dd>추가적으로, 몇몇 노드는 채널 카운트의 가능한 값에 대해 추가적인 <strong>channelCount 제약 사항</strong>를 가지고 있습니다.
            <dl>
              <dt>AudioDestinationNode</dt>
              <dd>목적지 노드가 AudioContext의 목적지인지 OfflineAudioContext의 목적지인지에 따라 동작이 다릅니다.
                <dl>
                  <dt>AudioContext</dt>
                  <dd>채널 카운트는 반드시 1과 maxChannelCount 사이여야만 합니다. ⌛ 이 범위 바깥의 값을 설정하려는 시도에 대해서는 IndexSizeError가 반드시 발생되어야만 합니다.</dd>
                  <dt>OfflineAudioContext</dt>
                  <dd>채널 카운트는 변경될 수 없습니다. ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
                </dl>
              </dd>
              <dt>AudioWorkletNode</dt>
              <dd>§ 1.32.3.3.2 AudioWorkletNodeOptions로 채널 설정하기를 참고하십시오.</dd>
              <dt>ChannelMergerNode</dt>
              <dd>채널 카운트는 변경될 수 없고, ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>ChannelSplitterNode</dt>
              <dd>채널 카운트는 변경될 수 없고, ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>ConvolverNode</dt>
              <dd>채널 카운트는 2보다 클 수 없으며, ⌛ 2보다 큰 값으로 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>DynamicsCompressorNode</dt>
              <dd>채널 카운트는 2보다 클 수 없으며, ⌛ 2보다 큰 값으로 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>PannerNode</dt>
              <dd>채널 카운트는 2보다 클 수 없으며, ⌛ 2보다 큰 값으로 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>ScriptProcessorNode</dt>
              <dd>채널 카운트는 변경될 수 없고, ⌛ 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>StereoPannerNode</dt>
              <dd>채널 카운트는 2보다 클 수 없으며, ⌛ 2보다 큰 값으로 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
            </dl>
            <p>이 어트리뷰트에 대한 더 많은 정보에 대해서는 § 4 채널 업믹싱과 다운믹싱을 참고하십시오.</p>
          </dd>
          <dt><i>channelCountMode</i> / 자료형: ChannelCountMode</dt>
          <dd>channelCountMode는 노드의 입력에 대한 연결을 업믹싱하거나 다운믹싱할 때 채널들이 카운트되는 방법을 결정합니다. 기본값은 "max"입니다. 이 어트리뷰트는 입력이 없는 노드에 대해서는 효과를 가지지 않습니다.</dd>
          <br>
          <dd>추가적으로, 몇몇 노드는 채널 카운트 모드의 가능한 값에 대한 추가적인 <strong>channelCountMode 제약 사항</strong>을 가지고 있습니다.
            <dl>
              <dt>AudioDestinationNode</dt>
              <dd>만약 AudioDestinationNode가 OfflineAudioContext의 목적지 노드라면, 채널 카운트 모드는 변경될 수 없습니다. ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>ChannelMergerNode</dt>
              <dd>채널 카운트 모드는 "explicit"에서 변경될 수 없으며 ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>ChannelSplitterNode</dt>
              <dd>채널 카운트 모드는 "explicit"에서 변경될 수 없으며 ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>ConvolverNode</dt>
              <dd>채널 카운트 모드는 "max"로 설정될 수 없으며, ⌛ 이 값을 "max"로 설정하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>DynamicsCompressorNode</dt>
              <dd>채널 카운트 모드는 "max"로 설정될 수 없으며, ⌛ 이 값을 "max"로 설정하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>PannerNode</dt>
              <dd>채널 카운트 모드는 "max"로 설정될 수 없으며, ⌛ 이 값을 "max"로 설정하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>ScriptProcessorNode</dt>
              <dd>채널 카운트 모드는 "explicit"에서 변경될 수 없으며 ⌛ 이 값을 변경하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
              <dt>StereoPannerNode</dt>
              <dd>채널 카운트 모드는 "max"로 설정될 수 없으며, ⌛ 이 값을 "max"로 설정하려는 시도에 대해서는 NotSupportedError가 반드시 발생되어야만 합니다.</dd>
            </dl>
            <p>이 어트리뷰트에 대한 더 많은 정보에 대해서는 § 4 채널 업믹싱과 다운믹싱을 참고하십시오.</p>
          </dd>
          <dt><i>channelInterpretation</i> / 자료형: ChannelInterpretation</dt>
          <dd>channelInterpretation은 노드의 입력에 연결을 업믹싱하거나 다운믹싱할 때 개개의 채널들이 어떻게 다루어질 것인지를 결정합니다. 기본값은 "speakers"입니다. 이 어트리뷰트는 입력이 없는 노드에 대해서는 효과를 가지지 않습니다.</dd>
          <br>
          <dd>추가적으로, 몇몇 노드는 채널 해석의 가능한 값들에 대한 추가적인 <strong>channelInterpretation 제약 사항</strong>을 가지고 있습니다.
            <dl>
              <dt>ChannelSplitterNode</dt>
              <dd>채널 해석은 "discrete"에서 변경될 수 없고 ⌛ 이 값을 변경하려는 시도에 대해서는 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
            </dl>
            <p>이 어트리뷰트에 대한 더 많은 정보에 대해서는 § 4 채널 업믹싱과 다운믹싱을 참고하십시오.</p>
          </dd>
          <dt><i>context</i> / 자료형: BaseAudioContext / 읽기 전용</dt>
          <dd>이 AudioNode를 소유하고 있는 BaseAudioContext.</dd>
          <dt><i>numberOfInputs</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>이 AudioNode에 입력되는 입력의 수. <strong>소스 노드</strong>에 대해서, 이 값은 0입니다. 이 어트리뷰트는 많은 AudioNode 유형에 대해 미리 결정되어 있지만, ChannelMergerNode와 AudioWorkletNode같은 몇몇 AudioNode는 가변적인 입력의 수를 가집니다.</dd>
          <dt><i>numberOfOutputs</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>이 AudioNode에서 나오는 출력의 수. 이 어트리뷰트는 몇몇 AudioNode 유형에 대해 미리 결정되어 있지만, ChannelSplitterNode와 AudioWorkletNode 같은 노드에 대해서 가변적일 수 있습니다.</dd>
        </dl>
        <br>
        <h3>1.5.5. 메서드</h3>
        <dl>
          <dt><i>connect(destinationNode, output, input)</i></dt>
          <dd>하나의 특정한 노드의 주어진 출력과 다른 특정한 노드의 주어진 입력 사이에 오직 하나의 연결만이 있을 수 있습니다. 같은 종점에 대한 다수의 연결은 무시됩니다.</dd>
          <pre>
예제 5
예를 들어,
nodeA.connect(nodeB);
nodeA.connect(nodeB);
위 코드는 아래와 같은 효과를 가집니다.
nodeA.connect(nodeB);
          </pre>
          <dd>이 메서드는 destination AudioNode 객체를 반환합니다.</dd>
          <table>
            <caption>AudioNode.connect(destinationNode, output, input) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationNode</td>
              <td></td>
              <td></td>
              <td></td>
              <td>destination 매개변수는 연결할 AudioNode입니다. 만약 destination 매개변수가 다른 AudioContext를 사용하여 생성된 AudioNode라면, ⌛ InvalidAccessError가 반드시 발생되어야만 합니다. 즉, AudioNode는 AudioContext간에 공유될 수 없습니다. 다수의 AudioNode는 같은 AudioNode에 연결될 수 있으며, 이는 채널 업믹싱과 다운믹싱 섹션에 기술되어 있습니다.</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>output 매개변수는 어떤 AudioNode 출력이 연결될지를 기술하는 인덱스입니다. 만약 이 매개변수가 out-of-bounds라면, ⌛ IndexSizeError 예외가 반드시 발생되어야만 합니다. 하나의 AudioNode 출력을 다수의 connect() 호출과 함께 하나 이상의 입력에 연결하는 것은 가능합니다. 따라서, "fan-out"이 지원됩니다.</td>
            </tr>
            <tr>
              <td>input</td>
              <td></td>
              <td></td>
              <td></td>
              <td>input 매개변수는 destination AudioNode의 어떤 입력에 연결될지를 기술하는 인덱스입니다. 만약 이 매개변수가 out-of-bounds라면, ⌛ IndexSizeError 예외가 반드시 발생되어야만 합니다. cycle을 생성하는 다른 AudioNode에 AudioNode를 연결하는 것은 가능합니다. 즉, AudioNode는 다른 AudioNode에 연결될 수 있는데, 이는 차례로 연결되어 결국 첫번째 AudioNode의 입력 혹은 AudioParam에 연결됩니다.</td>
            </tr>
          </table>
          <dd>반환 유형: AudioNode</dd>
          <br>
          <dt><i>connect(destinationParam, output)</i></dt>
          <dd>AudioNode를 AudioParam에 연결하고, a-rate 시그널로 파라미터 값을 제어합니다.</dd>
          <dd>다수의 connect() 호출과 함께 하나 이상의 AudioParam에 AudioNode 출력을 연결하는 것은 가능합니다. 따라서, "fan-out"이 지원됩니다.</dd>
          <dd>다수의 connect() 호출과 함께 하나의 AudioParam에 하나 이상의 AudioNode 출력을 연결하는 것은 가능합니다. 따라서, "fan-in"이 지원됩니다.</dd>
          <dd>AudioParam은 이것에 연결된 AudioNode 출력으로부터의 렌더링된 오디오 데이터를 취해 이것이 이미 모노가 아니라면 다운믹싱함으로써 이것을 모노로 변환하고, 그러한 다른 출력과 함께 믹스하고 최종적으로 내재적인 파라미터 값으로 믹스할 것이며 (AudioParam이 통상적으로 오디오 연결 없이 가지는 값), 이는 이 파라미터에 예정된 타임라인 변화를 포함합니다.</dd>
          <dd>모노로 다운믹싱하는 것은 channelCount = 1, channelCountMode = "explicit", channelInterpretation = "speakers"를 가진 AudioNode을 위해 다운믹싱하는 것과 같습니다.</dd>
          <dd>하나의 특정한 노드의 주어진 출력과 특정한 AudioParam 사이에는 오직 하나의 연결만이 있을 수 있습니다. 같은 종점을 가진 다수의 연결은 무시됩니다.</dd>
          <pre>
예제 6
예를 들어,
nodeA.connect(param);
nodeA.connect(param);
위 코드는 아래와 같은 효과를 가집니다.
nodeA.connect(param);
          </pre>
          <table>
            <caption>AudioNode.connect(destinationParam, output) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationParam</td>
              <td>AudioParam</td>
              <td>x</td>
              <td>x</td>
              <td>destination 매개변수는 연결할 AudioParam입니다. 이 메서드는 destination AudioParam 객체를 반환하지 않습니다. ⌛ 만약 destinationParam이 이 메서드가 호출된 AudioNode를 생성한 BaseAudioContext와 다른 BaseAudioContext에 속한 AudioNode에 속한다면, InvalidAccessError가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>output 매개변수는 AudioNode의 어떤 출력이 연결될지를 기술하는 인덱스입니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect()</i></dt>
          <dd>AudioNode로부터 나가는 모든 연결을 해제합니다.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect(output)</i></dt>
          <dd>AudioNode의 하나의 출력을 이것이 연결된 다른 AudioNode나 AudioParam으로부터 해제합니다.</dd>
          <table>
            <caption>AudioNode.disconnect(output) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>이 매개변수는 AudioNode의 어떤 출력이 해제될지를 기술하는 인덱스입니다. 이것은 주어진 출력으로부터 나가는 모든 연결을 해제합니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect(destinationNode)</i></dt>
          <dd>특정한 목적지 AudioNode로 가는 AudioNode의 모든 출력을 해제합니다.</dd>
          <table>
            <caption>AudioNode.disconnect(destinationNode) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationNode</td>
              <td></td>
              <td></td>
              <td></td>
              <td>destinationNode 매개변수는 해제할 AudioNode입니다. 이것은 주어진 destinationNode로 나가는 모든 연결을 해제합니다. ⌛ 만약 destinationNode와의 연결이 없다면, InvalidAccessError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect(destinationNode, output)</i></dt>
          <dd>목적지 AudioNode의 모든 입력으로부터 AudioNode의 특정한 출력을 해제합니다.</dd>
          <table>
            <caption>AudioNode.disconnect(destinationNode, output) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationNode</td>
              <td></td>
              <td></td>
              <td></td>
              <td>destinationNode 매개변수는 해제할 AudioNode입니다. ⌛ 만약 주어진 output으로부터 destinationNode에 연결이 없다면, InvalidAccessError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>output 매개변수는 AudioNode의 어떤 출력을 해제할지를 기술하는 인덱스입니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect(destinationNode, output, input)</i></dt>
          <dd>목적지 AudioNode의 특정한 입력으로부터 AudioNode의 특정한 출력을 해제합니다.</dd>
          <table>
            <caption>AudioNode.disconnect(destinationNode, output, input) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationNode</td>
              <td></td>
              <td></td>
              <td></td>
              <td>destinationNode 매개변수는 해제할 AudioNode입니다. ⌛ 만약 주어진 입력으로부터 주어진 출력에 대해 destinationNode에 연결이 없다면, InvalidAccessError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>output 매개변수는 AudioNode의 어떤 출력을 해제할지를 기술하는 인덱스입니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>input</td>
              <td></td>
              <td></td>
              <td></td>
              <td>input 매개변수는 목적지 AudioNode의 어떤 입력을 해제할지를 기술하는 인덱스입니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <dt><i>disconnect(destinationParam)</i></dt>
          <dd>특정한 목적지 AudioParm으로 가는 AudioNode의 모든 출력을 해제합니다. 이 동작이 취해지면 계산된(computed) 파라미터 값에 대한 이 AudioNode의 기여는 0이 됩니다. 내재적인 파라미터 값은 이 동작에 의해 영향받지 않습니다.</dd>
          <table>
            <caption>AudioNode.disconnect(destinationParam) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationParam</td>
              <td>AudioParam</td>
              <td>x</td>
              <td>x</td>
              <td>destinationParam 매개변수는 해제할 AudioParam입니다. ⌛ 만약 destinationParam에 연결이 없다면, InvalidAccessError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>disconnect(destinationParam, output)</i></dt>
          <dd>특정한 목적지 AudioParam으로부터 AudioNode의 특정한 출력을 해제합니다. 이 동작이 취해지면 계산된(computed) 파라미터 값에 대한 이 AudioNode의 기여는 0이 됩니다. 내재적인 파라미터 값은 이 동작에 의해 영향받지 않습니다.</dd>
          <table>
            <caption>AudioNode.disconnect(destinationParam, output) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>destinationParam</td>
              <td>AudioParam</td>
              <td>x</td>
              <td>x</td>
              <td>destinationParam 매개변수는 해제할 AudioParam입니다. ⌛ 만약 destinationParam에 연결이 없다면, InvalidAccessError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
            <tr>
              <td>output</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>output 매개변수는 AudioNode의 어떤 출력을 해제할지를 기술하는 인덱스입니다. ⌛ 만약 이 매개변수가 out-of-bounds라면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: undefined</dd>
        </dl>
        <br>
        <h3>1.5.6. AudioNodeOptions</h3>
        <p>이는 모든 AudioNode를 생성할 때 사용될 수 있는 옵션을 명시합니다. 모든 멤버는 선택적입니다. 그러나, 각 노드에 사용되는 특정한 값은 실제 노드에 따라 다릅니다.</p>
        <pre>
dictionary AudioNodeOptions {
  unsigned long channelCount;
  ChannelCountMode channelCountMode;
  ChannelInterpretation channelInterpretation;
};
        </pre>
        <br>
        <h4>1.5.6.1. 딕셔너리 AudioNodeOptions 멤버</h4>
        <dl>
          <dt><i>channelCount</i> / 자료형: unsigned long</dt>
          <dd>channelCount 어트리뷰트에 대해 원하는 채널의 수</dd>
          <br>
          <dt><i>channelCountMode</i> / 자료형: ChannelCountMode</dt>
          <dd>channelContMode 어트리뷰트에 대해 원하는 모드</dd>
          <br>
          <dt><i>channelInterpretation</i> / 자료형: ChannelInterpretation</dt>
          <dd>channelInterpretation 어트리뷰트에 대해 원하는 모드</dd>
        </dl>
        <br>
        <h2>1.6. AudioParam 인터페이스</h2>
        <p>AudioParam은 AudioNode의 기능의 개별적 측면을 제어하는데, 예를 들자면 음량이 있습니다. 이 파라미터는 value 어트리뷰트를 사용하여 특정한 값으로 즉시 설정될 수 있습니다. 또는, 값은 아주 정밀한 시간에 (AudioContext의 currentTime 어트리뷰트의 좌표계에서) 발생하도록 예정될 수도 있는데, 엔벨로프, 볼륨 페이드, LFO, filter sweep, grain window 등이 그 예입니다. 이 방법으로, 임의적인 타임라인 기반의 자동화 곡선이 모든 AudioParam에 대해서 설정될 수 있습니다. 추가적으로, AudioNode의 출력으로부터의 오디오 신호는 AudioParam에 연결될 수 있고 고유 파라미터 값으로 평가됩니다.</p>
        <br>
        <p>몇몇 합성과 AudioNode 처리는 어트리뷰트의 값이 반드시 '오디오 샘플 당' 기반으로 고려되어져야만 하는 어트리뷰트로써 AudioParam을 가집니다. 다른 AudioParam에 대해서는, 샘플 정확성은 중요하지 않으며 값 변화는 더욱 조악하게 샘플될 수 있습니다. 개개의 AudioParam은 이것이 a-rate 파라미터인지 k-rate 파라미터인지를 명시할 것입니다. a-rate 파라미터란 이것의 값이 반드시 '오디오 샘플 당' 기반으로 고려되어져야 함을 의미합니다.</p>
        <br>
        <p>구현은 반드시 블럭 프로세싱을 사용해야 하며, 각 AudioNode는 하나의 render quantum을 처리합니다.</p>
        <br>
        <p>각 render quantum에 대해, <strong><i>k-rate</i></strong> 파라미터의 값은 반드시 첫 번째 샘플 프레임의 시간에 샘플되어야만 하고, 그 값은 반드시 전체 블럭에 대해 사용되어야만 합니다. <strong><i>a-rate</i></strong> 파라미터의 값은 반드시 블럭의 각 샘플 프레임에 대해 샘플되어야만 합니다. AudioParam에 따라, 이것의 rate는 automationRate 어트리뷰트를 "a-rate"나 "k-rate"로 설정함으로써 제어될 수 있습니다. 추가적인 세부 사항은 각 AudioParam의 설명을 참고해 보세요.</p>
        <br>
        <p>각 AudioParam은 이 파라미터의 <strong>단순한 명목상의 범위</strong>를 함께 형성하는 minValue와 maxValue 어트리뷰트를 포함하고 있습니다. 실제로는, 파라미터의 값은 [minValue, maxValue] 범위로 강제됩니다. 전체 사항에 대해서는 § 1.6.3. 값의 계산을 참고해 보세요.</p>
        <br>
        <p>많은 AudioParam의 경우 minValue와 maxValue는 최대로 가능한 범위로 설정되는 것이 의도됩니다. 이 경우, maxValue는 <strong><i>most-positive-single-float</i></strong> 값으로 설정되어야 하는데, 이 값은 3.4028235e38입니다. (그러나, 오직 IEEE-754 double precision float value만을 지원하는 JavaScript에서는, 이것은 반드시 3.4028234663852886e38로 작성되어야만 합니다.) 유사하게, minValue는 <strong><i>most-negative-single-float</i></strong>로 설정되어야 하고, 이 값은 -3.4028235e38입니다. (유사하게, 이것은 반드시 JavaScript에서 -3.4028234663852886e38로 작성되어야만 합니다.)</p>
        <br>
        <p>AudioParam은 0개 이상의 <strong>자동화 이벤트</strong>를 유지합니다. 각 자동화 이벤트는, AudioContext의 currentTime 어트리뷰트의 시간 좌표계의 <strong>자동화 이벤트 시간</strong>에 관련하여, 특정한 시간 범위에 대한 파라미터의 값 변화를 명시합니다. 자동화 이벤트의 리스트는 자동화 이벤트 시간의 오름차순으로 유지됩니다.</p>
        <br>
        <p>주어진 자동화 이벤트의 행동은 이 이벤트의 자동화 이벤트 시간과 리스트의 인접한 이벤트의 자동화 이벤트 뿐만이 아니라, AudioContext의 현재 시간의 함수입니다. 다음의 <strong>자동화 메서드</strong>들은 이벤트 리스트에 메서드에 따른 새로운 이벤트를 추가함으로써 이벤트 리스트를 변경시킵니다.</p>
        <ul>
          <li>setValueAtTime() - setValue</li>
          <li>linearRampToValueAtTime() - LinearRampToValue</li>
          <li>exponentialRampToValueAtTime() - exponentialRampToValue</li>
          <li>setTargetAtTime() - setTarget</li>
          <li>setValueCurveAtTime() - setValueCurve</li>
        </ul>
        <br>
        <p>다음의 규칙이 이 메서드들을 호출할 때 적용될 것입니다:</p>
        <ul>
          <li>자동화 이벤트 시간은 일반적인 샘플 레이트에 관하여 양자화되지 않습니다. 곡선과 경사를 결정하는 공식은 이벤트를 예정할 때 주어진 정확한 수치적 시간에 적용됩니다.</li>
          <li>만약 이 이벤트들 중 하나가 이미 하나 이상의 이벤트가 있는 시간에 추가되었다면, 이 이벤트는 그 이벤트들 이후에 리스트에 배치될 것이지만, 시간이 그 이벤트 이후에 있는 이벤트 전입니다.</li>
          <li>⌛ 만약 setValueCurveAtTime()이 시간 T와 기간 D로 호출되고 T보다 엄격하게 큰 시간을 가지지만, T+D보다는 엄격히 작은 어떠한 이벤트가 있다면, NotSupportedError 예외가 반드시 발생되어야 합니다. 요컨대, 다른 이벤트를 포함하고 있는 시간 기간 동안에 value curve를 예정하는 것은 괜찮지 않지만, 다른 이벤트의 시간에 정확히 value curve를 예정하는 것은 괜찮습니다.</li>
          <li>⌛ 유사하게 만약 [T, T+D)에 포함된 시간에 자동화 메서드가 호출되었다면 NotSupportedError가 반드시 발생되어야만 합니다 (T는 curve의 시간이고 D는 기간임).</li>
        </ul>
        <br>
        <div class="note">
          <p>노트: AudioParam 어트리뷰트는 읽기 전용입니다. 단, value 어트리뷰트는 예외입니다.</p>
        </div>
        <br>
        <p>AudioParam의 automation rate는 다음의 값 중 하나와 함께 automationRate 어트리뷰트를 설정함으로써 선택될 수 있습니다. 그러나, 몇몇 AudioParam은 automation rate가 변경될 수 있는지에 대한 제약을 가지고 있습니다.</p>
        <br>
        <pre>
enum AutomationRate {
  "a-rate",
  "k-rate"
};
        </pre>
        <br>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>a-rate</i>"</td>
              <td>이 AudioParam이 a-rate 프로세싱으로 설정됩니다.</td>
            </tr>
            <tr>
              <td>"<i>k-rate</i>"</td>
              <td>이 AudioParam이 k-rate 프로세싱으로 설정됩니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <p>각 AudioParam은 내부 슬롯 [[current value]]를 가지고 있으며, 초기적으로 AudioParam의 defaultValue로 설정되어 있습니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioParam {
  attribute float value;
  attribute AutomationRate automationRate;
  readonly attribute float defaultValue;
  readonly attribute float minValue;
  readonly attribute float maxValue;
  AudioParam setValueAtTime (float value, double startTime);
  AudioParam linearRampToValueAtTime (float value, double endTime);
  AudioParam exponentialRampToValueAtTime (float value, double endTime);
  AudioParam setTargetAtTime (float target, double startTime, float timeConstant);
  AudioParam setValueCurveAtTime (sequence<float> values,
                                  double startTime,
                                  double duration);
  AudioParam cancelScheduledValues (double cancelTime);
  AudioParam cancelAndHoldAtTime (double cancelTime);
};
        </pre>
        <h3>1.6.1. 어트리뷰트</h3>
        <dl>
          <dt><i>automationRate</i> / 자료형: AutomationRate</dt>
          <dd>AudioParam의 자동화율. 기본값은 실제 AudioParam에 따라 다릅니다. 기본값에 대해선 각 AudioParam의 설명을 참고하십시오.</dd>
          <dd>몇몇 노드는 다음과 같이 추가적인 <strong><i>자동화율 제약조건</i></strong>이 있습니다.
            <dl>
              <dt>AudioBufferSourceNode</dt>
              <dd>playbackRate와 detune AudioParam은 반드시 "k-rate"여야만 합니다. ⌛ 만약 이 비율이 "a-rate"로 변경된다면 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>DynamicsCompressorNode</dt>
              <dd>threshold, knee, ratio, attack, release AudioParam은 반드시 "k-rate"여야만 합니다. ⌛ 만약 이 비율이 "a-rate"로 변경된다면 InvalidStateError가 반드시 발생되어야만 합니다.</dd>
              <dt>PannerNode</dt>
              <dd>만약 panningModel이 "HRTF"라면, PannerNode의 모든 AudioParam의 automationRate를 설정하는 것은 무시됩니다. 이와 같이, AudioListener의 모든 AudioParam의 automationRate를 설정하는 것은 무시됩니다. 이 경우, AudioParam은 automationRate가 "k-rate"로 설정된 것처럼 행동합니다.</dd>
            </dl>
          </dd>
          <br>
          <dt><i>defaultValue</i> / 자료형: float / 읽기 전용</dt>
          <dd>value 어트리뷰트의 초기값</dd>
          <br>
          <dt><i>maxValue</i> / 자료형: float / 읽기 전용</dt>
          <dd>파라미터가 취할 수 있는 명목상의 최대값. minValue와 함께, 이것은 이 파라미터의 명목상의 범위를 형성합니다.</dd>
          <br>
          <dt><i>minValue</i> / 자료형: float / 읽기 전용</dt>
          <dd>파라미터가 취할 수 있는 명목상의 최소값. maxValue와 함께, 이것은 이 파라미터의 명목상의 범위를 형성합니다.</dd>
          <br>
          <dt><i>value</i> / 자료형: float</dt>
          <dd>파라미터의 부동소수점 값. 이 어트리뷰트는 defaultValue로 초기화됩니다.</dd>
          <dd>이 어트리뷰트를 get하는 것은 [[current value]] 슬롯의 내용을 반환합니다. 반환되는 값의 알고리즘에 대해서는 § 1.6.3. 값의 계산을 참고하십시오.</dd>
          <dd>이 어트리뷰트를 set하는 것은 요청된 값을 [[current value]] 슬롯에 할당하고, 현재 AudioContext의 currentTime과 [[current value]] 와 함께 setValueAtTime() 메서드를 호출하는 효과를 가지고 있습니다. setValueAtTime()에 의해 발생되는 예외들이 또한 이 어트리뷰트를 set함에 의해 발생됩니다.</dd>
        </dl>
        <br>
        <h3>1.6.2. 메서드</h3>
        <dl>
          <dt><i>cancelAndHoldAtTime(cancelTime)</i></dt>
          <dd>이 메서드는 cancelTime과 같거나 큰 시간에서의 모든 예정된 파라미터 변화를 취소한다는 점에서 cancelScheduledValues()와 유사합니다. 그러나, 추가적으로, cancelTime에 일어날 자동화 값이 다른 자동화 이벤트가 시작될 때까지 모든 미래 시간에 대해 전파됩니다.</dd>
          <br>
          <dd>자동화가 작동 중이고 cancelAndHoldAtTime()을 호출한 후에, cancelTime이 도달하기 전의 시간에서 자동화가 시작될 수 있을 때 cancelAndHoldAtTime()의 앞에서 타임라인의 동작은 꽤 복잡합니다. 그러므로 cancelAndHoldAtTime()의 동작은 다음의 알고리즘으로 명시됩니다.</dd>
          <br>
          <dd>t<sub>c</sub>를 cancelTime의 값이라고 하자.</dd>
          <ol>
            <li>E<sub>1</sub>을 시간 t<sub>1</sub>에서의 이벤트라고 하자. 이 때, t<sub>1</sub>은 t<sub>1</sub>&lt;=t<sub>c</sub>을 만족하는 가장 큰 수이다.</li>
            <li>E<sub>2</sub>을 시간 t<sub>2</sub>에서의 이벤트라고 하자. 이 때, t<sub>2</sub>는 t<sub>c</sub>&lt;=t<sub>2</sub>을 만족하는 가장 작은 수이다.</li>
            <li>만약 E<sub>2</sub>가 존재한다면
              <ol>
                <li>만약 E<sub>2</sub>이 선형 혹은 지수 ramp라면,
                  <ol>
                    <li>시간 t<sub>c</sub>에서의 원래 ramp의 값이었을 end value를 가지는, 시간 t<sub>c</sub>에서 끝나는 동일한 종류의 ramp로 E<sub>2</sub>를 효과적으로 rewrite한다.
                    </li>
                    <img src="img/cancel-linear.svg" width=500>
                    <li>단계 5로 간다.</li>
                  </ol>
                </li>
                <li>그렇지 않으면, 단계 4로 간다.</li>
              </ol>
            </li>
            <li>만약 E<sub>1</sub>이 존재한다면
              <ol>
                <li>만약 E<sub>1</sub>이 setTarget 이벤트라면
                  <ol>
                    <li>묵시적으로, setTarget이 시간 t<sub>c</sub>에서 가졌을 값과 함께, 시간 t<sub>c</sub>에 setValueAtTime 이벤트를 삽입한다.</li>
                    <img src="img/cancel-setTarget.svg" width=500>
                    <li>단계 5로 간다.</li>
                  </ol>
                </li>
                <li>만약 E<sub>1</sub>이 start time t<sub>3</sub>과 duration d를 가지는 setValueCurve라면
                  <ol>
                    <li>만약 t<sub>c</sub>&gt;t<sub>3</sub>+d라면, 단계 5로 간다.</li>
                    <li>그렇지 않으면,
                      <ol>
                        <li>효과적으로 이 이벤트를 start time t3과 새로운 duration tc-t3을 가지는 setValueCurve 이벤트로 대체한다. 그러나, 이것은 진정한 대체가 아니다. 이 자동화는 반드시 원본과 같은 출력을 생성하는 것에 신경을 써야만 하고, 다른 duration을 사용해 계산된 것이 아니다. (이것은 value curve의 샘플링을 조금 다른 방식으로 유발할 것이며, 다른 결과를 생성할 것이다.)</li>
                        <img src="img/cancel-setValueCurve.svg" width="500">
                        <li>단계 5로 간다.</li>
                      </ol>
                    </li>
                  </ol>
                </li>
              </ol>
            </li>
            <li>t<sub>c</sub>보다 큰 시간을 가지는 모든 이벤트를 제거한다.</li>
          </ol>
          <br>
          <dd>만약 어떤 이벤트도 추가되지 않았다면, cancelAndHoldAtTime() 이후의 자동화 값은 원래 타임라인이 시간 t<sub>c</sub>에서 가졌을 상수 값입니다.
            <table>
              <caption>AudioParam.cancelAndHoldAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>cancelTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>사전에 예정된 파라미터 변화가 취소되어 나갈 시간. 이것은 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있는 시간입니다. ⌛ 만약 cancelTime이 음수거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 cancelTime이 currentTime보다 작다면, 이것은 currentTime으로 clamped됩니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>cancelScheduledValues(cancelTime)</i></dt>
          <dd>cancelTime보다 크거나 같은 시간에서의 모든 예정된 파라미터 변화를 취소합니다. 예정된 파라미터 변화를 취소한다는 것은 이벤트 리스트에서 예정된 이벤트를 제거한다는 것을 의미합니다. 자동화 이벤트 시간이 cancelTime보다 작은 active automation 또한 취소되고, 그러한 취소는 불연속을 유발할 수도 있는데 왜냐하면 (그러한 자동화 전으로부터의) 원래 값이 즉시 복구되기 때문입니다. cancelAndHoldAtTime()에 의해 예정된 hold value 또한 만약 hold time이 cancelTime 이후에 발생한다면 제거됩니다.</dd>
          <br>
          <dd>setValueCurveAtTime()에 대해서, T<sub>0</sub>과 T<sub>D</sub>를 각각 이 이벤트에 대해서, 해당하는 startTime과 duration이라고 합시다. 그렇다면 만약 cancelTime이 [T<sub>0</sub>, T<sub>0</sub>+T<sub>D</sub>]의 범위 내에 있다면, 이 이벤트는 타임라인에서 제거됩니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.cancelScheduledValues() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>cancelTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>사전에 예정된 파라미터 변화가 취소되어 나갈 시간. 이것은 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있는 시간입니다. ⌛ 만약 cancelTime이 음수거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 cancelTime이 currentTime보다 작다면, 이것은 currentTime으로 clamped됩니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>exponentialRampToValueAtTime(value, endTime)</i></dt>
          <dd>이전에 예정된 파라미터 값으로부터 주어진 값으로 파라미터 값에 지수적으로 연속적인 변화를 예정합니다. 필터 주파수와 재생 속도를 나타내는 파라미터는 지수적으로 변화할 때 가장 좋은데, 왜냐하면 인간이 소리를 인지하는 방식 때문입니다.</dd>
          <br>
          <dd>시간 구간 T<sub>0</sub>&lt;=t&lt;T<sub>1</sub> 동안의 값은 다음과 같이 계산됩니다 (T<sub>0</sub>은 이전 이벤트의 시간, T<sub>1</sub>은 이 메서드에 전달된 endTime 매개변수).</dd>
          <br>
          <dd>v(t) = V<sub>0</sub>(V<sub>1</sub>/V<sub>0</sub>)^((t - T<sub>0</sub>) / (T<sub>1</sub> - T<sub>0</sub>))</dd>
          <br>
          <dd>여기서 V<sub>0</sub>은 시간 T<sub>0</sub>에서의 값이고 V<sub>1</sub>은 이 메서드에 전달된 value 매개변수입니다. 만약 V<sub>0</sub>과 V<sub>1</sub>이 반대되는 부호를 가지고 있거나 V<sub>0</sub>이 0이라면, T<sub>0</sub>&lt;=t&lt;T<sub>1</sub>에 대해 v(t) = V<sub>0</sub>입니다.</dd>
          <br>
          <dd>이것은 또한 0으로의 지수적 ramp는 가능하지 않음을 암시합니다. 훌륭한 근사가 적절하게 선택된 시간 상수와 함께 setTargetAtTime()을 사용해 달성될 수 있습니다.</dd>
          <br>
          <dd>만약 이 <i>ExponentialRampToValue</i> 이벤트 이후로 더 이상의 이벤트가 없다면, t&gt=T<sub>1</sub>에 대해, v(t) = V<sub>1</sub>입니다.</dd>
          <br>
          <dd>만약 이 이벤트에 앞서는 이벤트가 없다면, 지수적 ramp는 마치 setValueAtTime(value, currentTime)이 호출된 것처럼 동작하는데 여기서 value는 어트리뷰트의 현재 값이고 currentTime은 exponentialRampToValueAtTime()이 호출된 컨텍스트의 currentTime입니다.</dd>
          <br>
          <dd>만약 앞서는 이벤트가 SetTarget 이벤트라면, T<sub>0</sub>과 V<sub>0</sub>은 현재 시간과 SetTarget 자동화의 값으로부터 선택됩니다. 즉, 만약 SetTarget 이벤트가 시작되지 않았다면, T<sub>0</sub>은 이벤트의 시작 시간이고, V<sub>0</sub>은 SetTarget 이벤트가 시작하기 방금 전의 값입니다. 이 경우, ExponentialRampToValue 이벤트는 효과적으로 SetTarget 이벤트를 대체합니다. 만약 SetTarget 이벤트가 이미 시작되었다면, T<sub>0</sub>은 현재 컨텍스트 시간이고, V<sub>0</sub>은 T<sub>0</sub>에서의 현재 SetTarget 자동화 값입니다. 두 경우 모두에서, 자동화 curve는 연속적입니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.exponentialRampToValueAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>value</td>
                <td>float</td>
                <td>x</td>
                <td>x</td>
                <td>주어진 시간에 파라미터가 지수적으로 ramp할 값. ⌛ 만약 이 값이 0과 같다면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
              </tr>
              <tr>
                <td>endTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>지수적 ramp가 끝나는 시간이며 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계. ⌛ 만약 endTime이 음수거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 endTime이 currentTime보다 작다면, currentTime으로 clamped됩니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>linearRampToValueAtTime(value, endTime)</i></dt>
          <dd>사전에 예정된 파라미터 값에서 주어진 값으로 파라미터 값에 선형으로 연속적인 변화를 예정합니다.</dd>
          <br>
          <dd>시간 구간 T<sub>0</sub>&lt;=t&lt;T<sub>1</sub> 동안의 값은 다음과 같이 계산됩니다 (여기서 T<sub>0</sub>은 이전 이벤트의 시간이고 T<sub>1</sub>은 이 메서드에 전달된 endTime 매개변수임).</dd>
          <br>
          <dd>v(t)=V<sub>0</sub>+[(V<sub>1</sub>-V<sub>0</sub>)(t-T<sub>0</sub>)]/(T<sub>1</sub>-T<sub>0</sub>)</dd>
          <br>
          <dd>여기서 V<sub>0</sub>은 시간 T<sub>0</sub>에서의 값이고 V<sub>1</sub>은 이 메서드에 전달된 value 매개변수의 값입니다.</dd>
          <br>
          <dd>만약 이 <i>LinearRampToValue</i> 이벤트 이후로 더 이상의 이벤트가 없다면 t&gt;=T<sub>1</sub>에 대해서, v(t)=V<sub>1</sub>입니다.</dd>
          <br>
          <dd>만약 이 이벤트에 선행하는 이벤트가 없다면, linear ramp는 마치 setValueAtTime(value, currentTime)이 호출된 것처럼 행동하는데, 여기서 value는 어트리뷰트의 현재 값이고 currentTime은 linearRampToValueAtTime()이 호출된 시간에서의 컨텍스트 currentTime입니다.</dd>
          <br>
          <dd>만약 선행하는 이벤트가 SetTarget 이벤트라면, T<sub>0</sub>과 V<sub>0</sub>은 SetTarget 자동화의 값과 현재 시간으로부터 선택됩니다. 즉, 만약 SetTarget 이벤트가 시작되지 않았다면, T<sub>0</sub>은 이벤트의 시작 시간이고, V<sub>0</sub>은 SetTarget 이벤트가 시작하기 바로 전의 값입니다. 이 경우, LinearRampToValue 이벤트는 효과적으로 SetTarget 이벤트를 대체합니다. 만약 SetTarget 이벤트가 이미 시작됐다면, T<sub>0</sub>은 현재 컨텍스트 시간이고, V<sub>0</sub>은 시간 T<sub>0</sub>에서의 현재 SetTarget 자동화 값입니다. 두 경우 모두에서, 자동화 curve는 연속적입니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.linearRampToValueAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>value</td>
                <td>float</td>
                <td>x</td>
                <td>x</td>
                <td>주어진 시간에 파라미터가 선형적으로 ramp 할 값.</td>
              </tr>
              <tr>
                <td>endTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>자동화가 끝나는 시간으로 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있음. ⌛ 만약 endTime이 음수거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 endTime이 currentTime보다 작다면, currentTime으로 clamp됩니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>setTargetAtTime(target, startTime, timeConstant)</i></dt>
          <dd>주어진 시간 상수를 가지는 비율로 주어진 시간에 목표 값으로의 지수적 접근을 시작합니다. 다른 사용 가운데서, 이것은 ADSR 엔벨로프의 "decay"와 "release" 부분을 구현하는 데 유용합니다. 파라미터 값이 주어진 시간에 목표 값으로 즉시 변화하지 않고, 대신 목표 값으로 서서히 변화한다는 점에 주의하십시오.</dd>
          <br>
          <dd>시간 구간 T<sub>0</sub>= t 동안 (T<sub>0</sub>은 startTime 매개변수)</dd>
          <br>
          <dd>v(t)=V<sub>1</sub>+(V<sub>0</sub>-V<sub>1</sub>)e^(-(t-T<sub>0</sub>)/τ)</dd>
          <br>
          <dd>여기서 V<sub>0</sub>은 T<sub>0</sub>(startTime 매개변수)에서의 초기값([[current value]] 어트리뷰트)이고, V<sub>1</sub>은 target 매개변수와 같으며, τ는 timeConstant 매개변수입니다.</dd>
          <br>
          <dd>만약 LinearRampToValue나 ExponentialRampToValue 이벤트가 이 이벤트의 뒤라면, 동작은 linearRampToValueAtTime() 또는 ExponentialRampToVAlueAtTime()에 각각 기술되어 있습니다. 다른 모든 이벤트에 대해서, SetTarget 이벤트는 다음 이벤트의 시간에 끝납니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.SetTargetAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>target</td>
                <td>float</td>
                <td>x</td>
                <td>x</td>
                <td>주어진 시간에 파라미터가 변화를 <i>시작할</i> 값</td>
              </tr>
              <tr>
                <td>startTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>지수적 접근이 시작될 시간이며, AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있음. ⌛ 만약 start가 음수거나 유한한 수가 아니라면, RangeError 예외가 반드시 발생되어야만 합니다. 만약 startTime이 currentTime보다 작다면, currentTime으로 clamp됩니다.</td>
              </tr>
              <tr>
                <td>timeConstant</td>
                <td>float</td>
                <td>x</td>
                <td>x</td>
                <td>목표 값으로의 first-order 필터 (지수적) 접근의 시간-상수 값. 이 값이 클수록, 전이는 더 느려질 것입니다. ⌛ 이 값은 반드시 음수가 아니여야만 하고 그렇지 않다면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 timeConstant가 0이라면, 출력값은 즉시 마지막 값으로 뜁니다. 더욱 정밀하게는, <i>timeConstant</i>는 계단 입력 응답 (0에서 1 값으로의 전이) 이 주어졌을 때 first-order 선형 연속 시불변 시스템이 값 1-1/e (약 63.2%) 에 도달하는 데 걸리는 시간입니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>setValueAtTime(value, startTime)</i></dt>
          <dd>주어진 시간에서의 파라미터 값 변화를 예정합니다.</dd>
          <br>
          <dd>만약 이 SetValue 이벤트 뒤에 더 이상의 이벤트가 없다면, t&gt;=T<sub>0</sub>에 대해 v(t)=V이며, 여기서 T<sub>0</sub>은 startTime 매개변수이고 V는 value 매개변수입니다. 다른 말로 하자면, 값은 일정하게 유지됩니다.</dd>
          <br>
          <dd>만약 이 SetValue 이벤트 뒤의 (시간 T<sub>1</sub>을 가지는) 다음 이벤트가 LinearRampToValue나 ExponentialRampToValue 유형의 이벤트가 아니라면, T<sub>0</sub>&lt;=t&lt;T<sub>1</sub>에 대해서</dd>
          <br>
          <dd>v(t)=V</dd>
          <br>
          <dd>다른 말로 하자면, 이 시간 구간 동안 값은 일정하게 유지되며, 이는 "계단" 함수의 생성을 가능케 합니다.</dd>
          <br>
          <dd>만약 이 SetValue 이벤트 뒤의 다음 이벤트가 LinearRampToValue나 ExponentialRampToValue 유형의 이벤트라면 각각 linearRampToValueAtTime() 혹은 exponentialRampToValueAtTime()을 참고하십시오.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.setValueAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>value</td>
                <td>float</td>
                <td>x</td>
                <td>x</td>
                <td>주어진 시간에 파라미터가 변화를 시작해나갈 값</td>
              </tr>
              <tr>
                <td>startTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>파라미터가 주어진 값으로 변화하는 시간이며 BaseAudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있음. ⌛ 만약 startTime이 음수거나 유한한 수가 아니라면, RangeError 예외가 반드시 발생되어야만 합니다. 만약 startTime이 currentTime보다 작다면, currentTime으로 clamp됩니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
          <br>
          <dt><i>setValueCurveAtTime(values, startTime, duration)</i></dt>
          <dd>주어진 기간 동안 주어진 시간에서 시작하는 임의의 파라미터 값들의 배열을 설정합니다. 값들의 개수는 희망되는 기간에 맞도록 스케일링될 것입니다.</dd>
          <br>
          <dd>T<sub>0</sub>을 startTime, T<sub>D</sub>을 duration, V를 values 배열, N을 values 배열의 길이라고 합시다. 그렇다면, 시간 구간 T&lt;=0t&lt;T<sub>0</sub>+T<sub>D</sub>동안,</dd>
          <br>
          <dd>k=floor((N-1)(t-T<sub>0</sub>)/T<sub>D</sub>)</dd>
          <br>
          <dd>그리고서 v(t)는 V[k]와 V[k+1] 사이를 선형적으로 보간함으로써 계산됩니다.</dd>
          <br>
          <dd>curve 시간 구간 (t&gt;=T<sub>0</sub>+T<sub>D</sub>)의 끝 이후에, (만약 있다면) 또 다른 자동화 이벤트가 있을 때까지, 값은 마지막 curve 값에서 변하지 않고 유지될 것입니다.</dd>
          <br>
          <dd>setValueAtTime()의 암시적 호출은 값 V[N-1]을 가지고서 시간 T<sub>0</sub>+T<sub>D</sub>에서 만들어져 이어지는 자동화가 setValueCurveAtTime() 이벤트의 끝에서 시작되도록 합니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioParam.setValueCurveAtTime() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>values</td>
                <td>sequence&lt;float&gt;</td>
                <td>x</td>
                <td>x</td>
                <td>파라미터 값 curve를 나타내는 float 값들의 sequence. 이 값들은 주어진 시간에 시작해 주어진 기간 동안 지속되도록 적용될 것입니다. 이 메서드가 호출되었을 때, 이 curve의 내부적 복제본이 자동화 목적을 위해 생성됩니다. 그러므로 전달된 배열의 내용의 차후의 수정은 AudioParam에 영향을 미치지 않습니다. ⌛ 만약 이 어트리뷰트가 2보다 작은 길이를 가지는 sequence&lt;float&gt; 객체라면 InvalidStateError가 반드시 발생되어야만 합니다.</td>
              </tr>
              <tr>
                <td>startTime</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>값 curve가 적용될 시작 시간이며 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있음. ⌛ 만약 startTime이 음수거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 startTime이 currentTime보다 작다면, currentTime으로 clamp됩니다.</td>
              </tr>
              <tr>
                <td>duration</td>
                <td>double</td>
                <td>x</td>
                <td>x</td>
                <td>값들이 values 매개변수에 따라 계산될 (startTime 매개변수 이후의) 시간의 양이며 단위는 초. ⌛ 만약 duration이 엄격하게 양수가 아니거나 유한한 수가 아니라면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: AudioParam</dd>
        </dl>
        <br>
        <h3>1.6.3. 값의 계산</h3>
        <p>두 가지 다른 종류의 AudioParam이 있는데, 각각 simple parameter와 compound parameter입니다. <strong>simple parameter</strong>(기본)는 AudioNode의 최종 오디오 출력을 계산하기 위해서 자기 자신이 사용됩니다. <strong>compound parameter</strong>는 다른 AudioParam과 함께 사용되어 AudioNode의 출력을 계산하는 입력으로 사용되는 값을 계산합니다.</p>
        <br>
        <p><strong>compoundValue</strong>는 오디오 DSP를 제어하는 최종 값이고 오디오 렌더링 스레드에 의해 각 rendering time quantum 동안 계산됩니다.</p>
        <br>
        <p>AudioParam의 값의 계산은 두 부분으로 이루어져 있습니다.</p>
        <br>
        <ul>
          <li><i>paramIntrinsicValue</i> 값은 value 어트리뷰트와 자동화 이벤트로부터 계산됩니다.</li>
          <li><i>paramComputedValue</i> 값은 오디오 DSP를 제어하는 최종 값이고 오디오 렌더링 스레드에 의해 각 render quantum 동안 계산됩니다.</li>
        </ul>
        <br>
        <p>이 값들은 반드시 다음과 같이 계산되어져야만 합니다.</p>
        <br>
        <ol>
          <li>paramIntrinsicValue는 각 time마다 계산될 것인데, 이는 value 어트리뷰트에 직접적으로 설정되는 값이거나, 만약 이 time에 혹은 time들 전의 자동화 이벤트가 이벤트가 있다면, 이 이벤트들로부터 계산되는 값입니다. 만약 자동화 이벤트가 주어진 시간 범위에서 제거된다면, paramIntrinsicValue 값은 변경되지 않은 채로 유지될 것이고 value 어트리뷰트가 직접적으로 설정되거나, 자동화 이벤트가 이 시간 범위에 추가되기 전까지 이것의 이전 값에 머물 것입니다.</li>
          <li>[[current value]]를 이 render quantum의 시작에 paramIntrinsicValue의 값으로 설정합니다.</li>
          <li>paramComputedValue는 paramIntrinsicVAlue 값과 입력 AudioParam 버퍼의 값의 합입니다. 만약 이 합이 NaN이라면, 이 합을 defaultValue로 대체합니다.</li>
          <li>만약 이 AudioParam이 compound parameter라면, 이것의 최종 값을 다른 AudioParam들과 함께 계산합니다.</li>
          <li>computedValue를 paramComputedValue로 설정합니다.</li>
        </ol>
        <br>
        <p>computedValue의 <strong>명목상의 범위</strong>는 이 파라미터가 효과적으로 가질 수 있는 낮은 값과 높은 값입니다. simple parameter에 대해서, computedValue는 이 파라미터의 simple nominal range로 clamp됩니다. compound parameter는 이루어져 있는 각기 다른 AudioParam 값들로부터 계산된 이후에 명목상의 범위로 clamped된 최종 값을 가집니다.</p>
        <br>
        <p>자동화 메서드가 사용되었을 때, clamping은 여전히 적용됩니다. 그러나, 자동화는 마치 clamping이 전혀 없는 것처럼 작동됩니다. 오직 자동화 값이 출력에 적용될 때만이 clamping이 위에 명시된 것처럼 이루어집니다.</p>
        <br>
        <pre>
예제 7
예를 들어, 노드 N이 명목상의 범위 [0, 1]을 가지고 있는 AudioParam P와, 다음의 자동화 시퀸스를 가지고 있다고 고려해 봅시다.

N.p.setValueAtTime(0, 0);
N.p.linearRampToValueAtTime(4, 1);
N.p.linearRampToValueAtTime(0, 2);

curve의 초기 기울기는 curve가 최대값 1에 도달하기 전까지 4이고, 도달하고 나면 출력은 일정하게 유지됩니다.
마지막으로, 시간 2 근처에서, curve의 기울기는 -4가 됩니다.
이것은 아래의 그래프에 그려져 있으며 점선은 clipping이 없었다면 발생했을 것을 나타내며,
그냥 선은 명목상의 범위에 대한 clipping에 기인한 audioparam의 실제 기대되는 동작을 나타냅니다.
        </pre>
        <br>
        <figure>
          <img src="img/audioparam-automation-clipping.png" width="915" height="546">
          <figcaption><strong>도식 4</strong> 명목상의 범위로부터 AudioParam 자동화의 clipping의 예시</figcaption>
        </figure>
        <br>
        <h3>1.6.4. AudioParam 자동화 예시</h3>
        <br>
        <figure>
          <img src="img/audioparam-automation1.png" width="917" height="541">
          <figcaption><strong>도식 5</strong> 파라미터 자동화의 예시</figcaption>
        </figure>
        <br>
        <pre>
예제 8
const curveLength = 44100;
const curve = new Float32Array(curveLength);
for (const i = 0; i &lt; curveLength; ++i)
  curve[i] = Math.sin(Math.PI * i / curveLength);
const t0 = 0;
const t1 = 0.1;
const t2 = 0.2;
const t3 = 0.3;
const t4 = 0.325;
const t5 = 0.5;
const t6 = 0.6;
const t7 = 0.7;
const t8 = 1.0;
const timeConstant = 0.1;
param.setValueAtTime(0.2, t0);
param.setValueAtTime(0.3, t1);
param.setValueAtTime(0.4, t2);
param.linearRampToValueAtTime(1, t3);
param.linearRampToValueAtTime(0.8, t4);
param.setTargetAtTime(.5, t4, timeConstant);
// 다음의 exponential이 올바른 곳에서 시작하게 하기 위해
// setTargetAtTime이 시간 t5에 있을 곳을 계산함
// 그래서 jump discontinuity가 없음.
// 명세에 따르면, v(t) = 0.5 + (0.8 - 0.5)*exp(-(t-t4)/timeConstant)
// 따라서 v(t5) = 0.5 + (0.8 - 0.5)*exp(-(t5-t4)/timeConstant)
param.setValueAtTime(0.5 + (0.8 - 0.5)*Math.exp(-(t5 - t4)/timeConstant), t5);
param.exponentialRampToValueAtTime(0.75, t6);
param.exponentialRampToValueAtTime(0.05, t7);
param.setValueCurveAtTime(curve, t7, t8 - t7);
        </pre>
        <br>
        <h2>1.7. AudioScheduledSourceNode 인터페이스</h2>
        <p>이 인터페이스는 AudioBufferSourceNode, ConstantSourceNode, OscillatorNode와 같은 소스 노드의 공통적인 기능을 나타냅니다.</p>
        <br>
        <p>소스가 시작되기 전에 (start()를 호출함으로써) 소스 노드는 반드시 무음 (0) 을 출력해야 합니다. 소스가 정지된 후 (stop()을 호출함으로써), 소스는 반드시 무음을 출력해야 합니다 (0).</p>
        <br>
        <p>AudioScheduledSourceNode는 직접적으로 인스턴스화될 수 없지만, 대신 소스 노드에 대한 구체적인 인터페이스에 의해 확장됩니다.</p>
        <br>
        <p>AudioScheduledSourceNode는 이것의 연관된 BaseAudioContext의 currentTime이 AudioScheduledSourceNode가 시작되기로 설정된 시간보다 크거나 같고, 이것이 정지되기로 설정된 시간보다 작을 때 <strong>재생</strong>된다고 말해집니다.</p>
        <br>
        <p>AudioScheduledSourceNode는 내부 boolean 슬롯 [[source started]]와 함께 생성되는데, 이는 초기적으로 false로 설정되어 있습니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioScheduledSourceNode : AudioNode {
  attribute EventHandler onended;
  undefined start(optional double when = 0);
  undefined stop(optional double when = 0);
};
        </pre>
        <br>
        <h3>1.7.1. 어트리뷰트</h3>
        <dl>
          <dt><i>onended</i> / 자료형: EventHandler</dt>
          <dd>AudioScheduledSourceNode 노드 유형에 전파되는 ended 이벤트의 EventHandler (HTML[HTML]에 기술되어 있음) 를 설정하기 위해 사용되는 프로퍼티. 소스 노드가 재생을 멈췄을 때 (concrete 노드에 의해 결정됨), Event (HTML[HTML]에 기술되어 있음) 유형의 이벤트가 이벤트 핸들러에 전파될 것입니다.</dd>
          <br>
          <dd>모든 AudioScheduledSourceNode에 대해서, stop()에 의해 결정된 stop time에 도달했을 때 onended 이벤트가 전파됩니다. AudioBufferSourceNode에 대해서, duration에 도달했거나 전체 buffer가 재생되었다면 이 이벤트가 또한 전파됩니다.</dd>
        </dl>
        <br>
        <h3>1.7.2. 메서드</h3>
        <br>
        <dl>
          <dt><i>start(when)</i></dt>
          <dd>정확한 시간에 소리에 재생을 예정합니다.</dd>
          <br>
          <dd>⌛ 이 메서드가 호출되었을 때, 아래의 절차를 수행합니다.
            <ol>
              <li>⌛ 만약 이 AudioScheduledSourceNode 내부 슬롯 [[source started]]가 true라면, InvalidStateError 예외가 반드시 발생되어야만 합니다.</li>
              <li>아래에서 설명되는 parameter constraint에 기인해 반드시 발생되어야만 하는 error들에 대해서 검사합니다. 만약 이 단계 동안 예외가 발생되었다면, 이 과정들을 중단합니다.</li>
              <li>이 AudioScheduledSourceNode의 내부 슬롯 [[source started]]를 true로 설정합니다.</li>
              <li>AudioScheduledSourceNode를 시작하기 위해 control message를 queue하며, 메시지 속에 parameter 값들을 포함합니다.</li>
              <li>연관된 AudioContext에 control message를 보내 다음의 모든 조건들이 만족되었을 때만 렌더링 스레드의 실행을 시작합니다.
                <ol>
                  <li>컨텍스트의 [[control thread state]]가 "suspended"입니다.</li>
                  <li>컨텍스트가 시작되도록 허용되었습니다.</li>
                  <li>[[suspended by user]] 플래그가 false입니다.</li>
                </ol>
                <div class="note">
                  <p>알림: 이것은 start()가 시작되도록 허용되지 않았을 AudioContext를 시작되게 만듭니다.</p>
                </div>
              </li>
            </ol>
          </dd>
          <br>
          <dd>
            <table>
              <caption>AudioScheduledSourceNode.start(when) 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>when</td>
                <td>double</td>
                <td>x</td>
                <td>v</td>
                <td>when 매개변수는 어떤 시간에 (단위: 초) 소리가 재생을 시작해야 하는지를 기술합니다. 이것은 AudioContext의 currentTime 어트리뷰트와 동일한 시간 좌표계에 있습니다. AudioScheduledSourceNode에 의해 생성된 신호가 소리의 시작 시간에 좌우될 때, when의 정확한 시간은 항상 가장 가까운 샘플 프레임으로 반올림되는 일 없이 사용됩니다. 만약 0이 이 값에 대해 전달되거나 값이 currentTime보다 작다면, 소리는 즉시 시작될 것입니다. ⌛ 만약 when이 음수면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>stop(when)</i></dt>
          <dd>정확한 시간에 소리에 재생 정지를 예정합니다. 만약 stop이 이미 호출된 이후에 다시 호출되었다면, 마지막 호출이 적용되는 호출일 것입니다. 이전 호출에 의해 설정된 stop time들은 버퍼가 차후의 호출에 앞서 정지되지 않는 이상 적용되지 않을 것입니다. 만약 버퍼가 이미 정지되었다면, 추가적인 stop 호출은 효과가 없습니다. 만약 stop time이 예정된 start time에 앞서 도달되었다면, 소리는 재생되지 않을 것입니다.</dd>
          <br>
          <dd>⌛ 이 메서드가 호출되었을 때, 아래의 절차를 수행합니다.
            <ol>
              <li>⌛ 만약 이 AudioScheduledSourceNode의 내부 슬롯 [[source started]]가 true가 아니라면, InvalidStateError가 반드시 발생되어야만 합니다.</li>
              <li>아래에서 설명되는 parameter constraint에 기인해 반드시 발생되어야만 하는 오류들을 검사합니다.</li>
              <li>AudioScheduledSourceNode를 정지하기 위해 control message를 queue하고, 메시지 내에 parameter values를 포함합니다.</li>
            </ol>
          </dd>
          <br>
          <dd>만약 노드가 AudioBufferSourceNode라면, AudioBufferSourceNode를 정지하기 위해 control messgae를 실행시키는 것은 재생 알고리즘에서 handleStop() 함수를 호출하는 것을 의미합니다.</dd>
        </dl>
        <br>
        <dd>
          <table>
            <caption>AudioScheduledSourceNode.stop(when) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>when</td>
              <td>double</td>
              <td>x</td>
              <td>v</td>
              <td>when 매개변수는 언제 (단위: 초) source가 재생을 멈춰야 하는지를 기술합니다. 이것은 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있습니다. 만약 0이 이 값에 대해 전달되거나 이 값이 currentTime보다 작다면, 소리는 즉시 정지될 것입니다. ⌛ 만약 when이 음수라면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
            </tr>
          </table>
        </dd>
        <br>
        <dd>반환 유형: undefined</dd>
        <br>
        <h2>1.8. AnalyserNode 인터페이스</h2>
        <p>이 인터페이스는 실시간 주파수 그리고 시간 영역 분석 정보를 제공 가능한 노드를 나타냅니다. 오디오 스트림은 입력에서 출력까지 처리되지 않은 채로 전달될 것입니다.</p>
        <br>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td>이 output은 연결되지 않은 채로 남아있을 수도 있습니다.</td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface AnalyserNode : AudioNode {
  constructor (BaseAudioContext context, optional AnalyserOptions options = {});
  undefined getFloatFrequencyData (Float32Array array);
  undefined getByteFrequencyData (Uint8Array array);
  undefined getFloatTimeDomainData (Float32Array array);
  undefined getByteTimeDomainData (Uint8Array array);
  attribute unsigned long fftSize;
  readonly attribute unsigned long frequencyBinCount;
  attribute double minDecibels;
  attribute double maxDecibels;
  attribute double smoothingTimeConstant;
};
        </pre>
        <br>
        <h3>1.8.1. 생성자</h3>
        <br>
        <dl>
          <dt><i>AnalyserNode(context, options)</i></dt>
          <dd>이 생성자가 BaseAudioContext c와 옵션 객체 option과 함께 호출되었을 때, user agent는 반드시 이 AudioNode this를 context와 options를 인자로서 함께 초기화해야만 합니다.</dd>
          <dd>
            <table>
              <caption>AnalyserNode.constructor() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>context</td>
                <td>BaseAudioContext</td>
                <td>x</td>
                <td>x</td>
                <td>이 새로운 AnalyserNode가 연관될 BaseAudioContext.</td>
              </tr>
              <tr>
                <td>options</td>
                <td>AnalyserOptions</td>
                <td>x</td>
                <td>v</td>
                <td>이 AnalyserNode의 선택적인 초기 파라미터 값.</td>
              </tr>
            </table>
          </dd>
        </dl>
        <br>
        <h3>1.8.2. 어트리뷰트</h3>
        <br>
        <dl>
          <dt><i>fftSize</i> / 자료형: unsigned long</dt>
          <dd>주파수 도메인 분석에 사용될 FFT의 크기 (단위: 샘플 프레임). ⌛ 이 값은 반드시 2의 제곱이여야 하며 그 범위는 32에서 32768까지입니다. 그렇지 않으면 IndexSizeError 예외가 반드시 발생되어야만 합니다. 기본값은 2048입니다. 큰 FFT 크기는 계산량이 많을 수 있음에 주의하십시오.</dd>
          <br>
          <dd>만약 fftSize가 다른 값으로 변경되면, (getByteFrequencyData()와 getFloatFrequencyData()에 대해서) 주파수 데이터의 smoothing과 연관된 모든 상태가 리셋됩니다. 즉 smoothing over time에 쓰인 이전 블럭인 ^X<sub>-1</sub>[k]가 모든 k에 대해 0으로 설정됩니다.</dd>
          <br>
          <dd>증가하는 fftSize는 현재 시간 도메인 데이터가 반드시 이전에는 포함되지 않았던 지난 프레임들을 포함하기 위해 확장되어야만 한다는 것을 의미함에 주목하십시오. 이는 AnalyserNode가 효과적으로 반드시 지난 32768개의 샘플 프레임을 가지고 있어야만 하며 현재 시간 도메인 데이터는 그것으로부터의 가장 최근의 fftSize 샘플 프레임이라는 것을 의미합니다.</dd>
          <br>
          <dt><i>frequencyBinCount</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>fftSize의 절반.</dd>
          <br>
          <dt><i>maxDecibels</i> / 자료형: double</dt>
          <dd>부호없는 바이트 값으로의 전환을 위해 FFT 분석 데이터의 스케일링 범위에서의 최대 파워 값. 기본값은 -30입니다. ⌛ 만약 이 어트리뷰트의 값이 minDecibels와 같거나 작은 값으로 설정되었다면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <br>
          <dt><i>minDecibels</i> / 자료형: double</dt>
          <dd>부호없는 바이트 값으로의 전환을 위해 FFT 분석 데이터의 스케일링 범위에서의 최소 파워 값. 기본값은 -100입니다. ⌛ 만약 이 어트리뷰트의 값이 maxDecibels와 같거나 큰 값으로 설정되었다면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <br>
          <dt><i>smoothingTimeConstant</i> / 자료형: double</dt>
          <dd>0에서 1까지의 값으로 0은 마지막 분석 프레임에서의 time averaging이 없음을 나타냅니다. 기본값은 0.8입니다. ⌛ 만약 이 어트리뷰트의 값이 0보다 작거나 1보다 큰 값으로 설정된다면, IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <br>
        </dl>
        <br>
        <h3>1.8.3. 메서드</h3>
        <br>
        <dl>
          <dt><i>getByteFrequencyData(array)</i></dt>
          <dd>인자로서 전달된 Uint8Array에 의해 소유되는 reference to the bytes를 얻습니다. 현재 주파수 데이터를 이 bytes에 복제합니다. 만약 배열이 frequencyBinCount보다 적은 원소를 가지고 있다면, 초과되는 원소는 탈락됩니다. 만약 배열이 FrequencyBinCount보다 많은 원소를 가지고 있다면, 초과되는 원소는 무시됩니다. 가장 최근의 fftSize 프레임이 주파수 데이터를 계산하는 데 사용됩니다.</dd>
          <br>
          <dd>만약 또 다른 getByteFrequencyData()나 getFloatFrequencyData() 호출이 이전과 동일한 render quantum 내에서 발생한다면, 현재 주파수 데이터는 같은 데이터에 의해 갱신되지 않습니다. 대신, 이전에 계산된 데이터가 반환됩니다.</dd>
          <br>
          <dd>unsigned byte array에 저장된 값들은 다음의 방식으로 계산됩니다. Y[k]를 FFT windowing and smoothing에서 기술되는 현재 주파수 데이터라고 합시다. 그렇다면 바이트 값 b[k]는 다음과 같습니다.</dd>
          <br>
          <dd>b[k] = Math.floor(255 * (Y[k] - dB<sub>min</sub>) / (dB<sub>max</sub> - dB<sub>min</sub>))</dd>
          <br>
          <dd>여기서 dB<sub>min</sub>는 minDecibels이고 dB<sub>max</sub>는 maxDecibels입니다. 만약 b[k]가 0에서 255 범위 바깥에 있다면, b[k]는 이 범위 안에 들도록 clip됩니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AnalyserNode.getByteFrequencyData() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>array</td>
                <td>Uint8Array</td>
                <td>x</td>
                <td>x</td>
                <td>이 매개변수는 주파수 도메인 분석 데이터가 복제될 곳입니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dt><i>getByteTimeDomainData(array)</i></dt>
          <dd>인자로서 전달된 Uint8Array에 의해 소유되는 reference to the bytes를 얻습니다. 현재 시간 도메인 데이터 (파형 데이터) 를 이 bytes에 복제합니다. 만약 배열이 fftSize의 값보다 적은 원소를 가지고 있다면, 초과되는 원소는 탈락될 것입니다. 만약 배열이 fftSize보다 많은 원소를 가지고 있다면, 초과되는 원소는 무시될 것입니다. 가장 최근의 fftSize 프레임이 바이트 데이터를 계산하는 데 사용됩니다.</dd>
          <br>
          <dd>unsigned byte array에 저장된 값들은 다음의 방식으로 계산됩니다. x[k]를 시간 도메인 데이터라고 합시다. 그럼 바이트 값 b[k]는 다음과 같습니다.</dd>
          <br>
          <dd>b[k] = Math.floor(128(1+x[k]))</dd>
          <br>
          <dd>만약 b[k]가 0에서 255 범위 바깥에 있다면, b[k]는 이 범위 안에 들도록 clip됩니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AnalyserNode.getByteTimeDomainData() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>array</td>
                <td>Uint8Array</td>
                <td>x</td>
                <td>x</td>
                <td>이 매개변수는 시간 도메인 샘플 데이터가 복제될 곳입니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>getFloatFrequencyData(array)</i></dt>
          <dd>인자로서 전달된 Float32Array에 의해 소유되는 reference to the bytes를 얻습니다. 현재 주파수 데이터를 이 bytes에 복제합니다. 만약 배열이 frequencyBinCount보다 적은 원소를 가지고 있다면, 초과되는 원소는 탈락됩니다. 만약 배열이 frequencyBinCount보다 많은 원소를 가지고 있다면, 초과되는 원소는 무시됩니다. 가장 최근의 fftSize 프레임이 주파수 데이터를 계산하는 데 사용됩니다.</dd>
          <br>
          <dd>같은 render quantum 내에서 이전 호출로서 또 다른 getFloatFrequencyData() 나 getByteFrequencyData() 가 발생한다면, 현재 주파수 데이터는 같은 데이터를 가지고 갱신되지 않습니다. 대신, 이전에 계산된 결과가 반환됩니다.</dd>
          <br>
          <dd>주파수 데이터는 dB 단위입니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AnalyserNode.getFloatFrequencyData() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>array</td>
                <td>Float32Array</td>
                <td>x</td>
                <td>x</td>
                <td>이 매개변수는 주파수 도메인 분석 데이터가 복제될 곳입니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: undefined</dd>
          <br>
          <dt><i>getFloatTimeDomainData(array)</i></dt>
          <dd>인자로서 전달된 Float32Array에 의해 소유되는 reference to the bytes를 얻습니다. 현재 시간 도메인 데이터 (파형 데이터) 를 이 bytes에 복제합니다. 만약 배열이 fftSize의 값보다 적은 원소를 가지고 있다면, 초과되는 요소는 탈락됩니다. 만약 배열이 fftSize보다 많은 원소를 가지고 있다면, 초과되는 원소는 무시될 것입니다. 가장 최근의 fftSize 프레임이 (다운믹싱 후에) 반환됩니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AnalyserNode.getFloatTimeDomainData() 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>array</td>
                <td>Float32Array</td>
                <td>x</td>
                <td>x</td>
                <td>이 매개변수는 시간 도메인 샘플 데이터가 복제될 곳입니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: undefined</dd>
        </dl>
        <br>
        <h3>1.8.4. AnalyserOptions</h3>
        <br>
        <p>이것은 AnalyserNode를 생성할 때 사용될 옵션을 명시합니다. 모든 멤버는 선택적입니다. 만약 명시되지 않았다면, 일반 기본값이 사용되어 노드를 생성합니다.</p>
        <br>
        <pre>
dictionary AnalyserOptions : AudioNodeOptions {
  unsigned long fftSize = 2048;
  double maxDecibels = -30;
  double minDecibels = -100;
  double smoothingTimeConstant = 0.8;
};
        </pre>
        <br>
        <h4>1.8.4.1. 딕셔너리 AnalyserOptions 멤버</h4>
        <br>
        <dl>
          <dt><i>ffiSize</i> / 자료형: unsigned long / 기본값: 2048</dt>
          <dd>주파수 도메인 분석을 위한 FFT의 원하는 초기 사이즈</dd>
          <br>
          <dt><i>maxDecibels</i> / 자료형: double / 기본값: -30</dt>
          <dd>FFT 분석을 위한 원하는 초기 최대 파워 (단위: dB)</dd>
          <br>
          <dt><i>minDecibels</i> / 자료형: double / 기본값: -100</dt>
          <dd>FFT 분석을 위한 원하는 초기 최소 파워 (단위: dB)</dd>
          <br>
          <dt><i>smoothingTimeConstant / 자료형: double / 기본값: 0.8</i></dt>
          <dd>FFT 분석을 위한 원하는 초기 smoothing 상수</dd>
        </dl>
        <br>
        <h3>1.8.5. 시간 영역 다운 믹싱</h3>
        <p>현재 시간 영역 데이터가 계산될 때, 입력 신호는 반드시 마치 channelCount가 1이고, channelCountMode가 "max"이고, channelInterpretation이 "speakers"인 것처럼 모노로 다운믹스되어야만 합니다. 이것은 AnalyserNode 자체의 설정과는 무관합니다. 가장 최신의 fftSize 프레임이 다운믹싱 연산에 사용됩니다.</p>
        <br>
        <h3>1.8.6. 시간에 따른 FFT windowing과 smoothing</h3>
        <p>현재 주파수 데이터가 계산될 때, 다음의 연산이 수행될 것입니다.</p>
        <br>
        <ol>
          <li>현재 시간 영역 데이터를 계산합니다.</li>
          <li>Blackman window를 시간 영역 입력 데이터에 적용합니다.</li>
          <li>푸리에 변환을 window된 시간 영역 입력 데이터에 적용하여 real과 imaginary 주파수 데이터를 얻습니다.</li>
          <li>시간에 따라 주파수 영역 데이터를 smooth합니다.</li>
          <li>dB로 변환합니다.</li>
        </ol>
        <br>
        <p>다음에서, N을 이 AnalyserNode의 fftSize 어트리뷰트의 값이라고 합시다.</p>
        <br>
        <p><strong><i>블랙맨 윈도우를 적용한다는 것</i></strong>은 입력 시간 도메인 데이터에서의 다음의 연산으로 이루어져 있습니다. n = 0, ..., N - 1에 대해 x[n]을 시간 도메인 데이터라고 합시다. 블랙맨 윈도우는 다음과 같이 정의됩니다.</p>
        <pre>
α = 0.16
a0 = (1 - α) / 2
a1 = 1 / 2
a2 = α / 2
w[n] = a0 - a1cos(2πn/N) + a2cos(4πn/N), n = 0, ..., N - 1에 대해서
        </pre>
        <br>
        <p>창이 적용된 신호 x^[n]은</p>
        <pre>
x^[n] = x[n]w[n], n = 0, ..., N - 1에 대해서
        </pre>
        <br>
        <p><strong><i>푸리에 변환을 적용한다는 것</i></strong>은 다음의 방법으로 푸리에 변환을 계산하는 것으로 이루어져 있습니다. X[k]를 복소 주파수 도메인 데이터라고 하고 x^[n]을 위에서 계산된 창이 적용된 시간 도메인 데이터라고 합시다. 그럼 X[k]는 다음과 같습니다.</p>
        <br>
        <pre>
X[k] = (1 / N) * sigma(from = n, to = N - 1, x^[n] * WN^(-kn))
        </pre>
        <p>k = 0, ..., (N / 2) - 1에 대해서, 그리고 WN = e^(2πi / N).</p>
        <br>
        <p>주파수 데이터를 <strong><i>smoothing over time한다는 것</i></strong>은 다음의 동작으로 이루어져 있습니다.</p>
        <br>
        <ul>
          <li>X^_(-1)[k]를 이전 블럭의 이 동작의 결과라고 합시다. <strong><i>이전 블럭</i></strong>은 이전의 smoothing over time 동작에 의해 계산된 버퍼로 혹은 만약 smoothing over time이 처음이라면 N개의 0으로 이루어진 배열으로 정의됩니다.</li>
          <li>τ를 이 AnalyserNode의 smoothingTimeConstant 어트리뷰트의 값이라고 합시다.</li>
          <li>X[k]를 현재 블럭의 푸리에 변환을 적용한 것의 결과라고 합시다.</li>
        </ul>
        <br>
        <p>그럼 smooth된 값인 X^[k]는 다음과 같이 계산됩니다.</p>
        <br>
        <pre>
X^[k] = τX^[k]_(-1)[k] + (1 - τ)|X[k]|
        </pre>
        <br>
        <p>k = 0, ..., N - 1에 대해서.</p>
        <br>
        <p><strong><i>dB로의 전환</i></strong>은 다음의 동작을 포함하는데, 여기서 X^[k]는 smoothing over time에서 계산된 것입니다.</p>
        <br>
        <pre>
Y[k] = 20log_10(X^[k])
        </pre>
        <br>
        <p>k = 0, ..., N - 1에 대해서.</p>
        <br>
        <p>getFloatFrequencyData()의 경우 이 배열 Y[k]는 출력 배열로 복사됩니다. getByteFrequencyData()의 경우, Y[k]는 minDecibels와 maxDecibels 사이에 있도록 clip되며 minDecibels가 0의 값으로 표현되고 maxDecibels가 255의 값으로 표현되는 unsigned byte에 fit되게 scale됩니다.</p>
        <br>
        <h2>1.9. AudioBufferSourceNode 인터페이스</h2>
        <p>이 인터페이스는 Audiobuffer 내의 in-memory 오디오 에셋으로부터의 오디오 소스를 나타냅니다. 이것은 높은 정도의 스케쥴링 유연성과 정확성을 요구하는 오디오 에셋 재생에 유용합니다. 만약 network- 또는 disk-backed 에셋의 sample-accurate 재생이 요구된다면, 구현자는 AudioWorkletNode를 사용하여 재생을 구현해야 합니다.</p>
        <br>
        <p>start() 메서드는 사운드 재생이 일어날 때를 예정하기 위해 사용됩니다. start() 메서드는 여러 번 호출될 수는 없을지도 모릅니다. 재생은 버퍼의 오디오 데이터가 완전히 재생되었을 때 (만약 loop 어트리뷰트가 false라면), 혹은 stop() 메서드가 호출되고 명시된 시간에 도달했을 때 자동적으로 정지할 것입니다. 자세한 사항은 start()와 stop() 설명을 참고해 보세요.</p>
        <br>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력의 채널의 수는 buffer 어트리뷰트에 할당된 AudioBuffer의 채널의 수와 동일하거나, 만약 buffer가 null이면 무음인 채널 하나입니다.</p>
        <br>
        <p>추가적으로, 만약 버퍼가 하나 이상의 채널을 가지고 있다면, AudioBufferSourceNode 출력은 다음의 조건 중 하나가 부합하는 시간 이후에 반드시 render quantum의 시작에 무음인 하나의 채널로 변경되어야만 합니다.</p>
        <br>
        <ul>
          <li>버퍼의 끝에 도달했습니다.</li>
          <li>지속 기간에 도달했습니다.</li>
          <li>정지 시간에 도달했습니다.</li>
        </ul>
        <br>
        <p>AudioBufferSourceNode의 <strong>playhead 포지션</strong>은 시간 오프셋 (초) 을 나타내는 어떠한 양으로써 정의되고, 이 오프셋은 버퍼의 첫번째 샘플 프레임의 시간 좌표계에 관련있습니다. 이러한 값은 노드의 playbackRate와 detune 파라미터와는 별도로 생각되어져야 할 것입니다. 일반적으로, playhead 포지션은 subsample-accurate할 수 있고 정확한 샘플 프레임 포지션을 나타낼 필요가 없습니다. 이것은 0과 버퍼의 지속 기간 사이에서 유효한 값을 추정할 수 있습니다.</p>
        <br>
        <p>playbackRate와 detune 어트리뷰트는 복합적인 파라미터를 형성합니다. 이것들은 함께 사용되어 <strong><i>computedPlaybackRate</i></strong> 값을 결정합니다.</p>
        <br>
        <p>computedPlaybackRate(t) = playbackRate(t) * pow(2, detune(t) / 1200)</p>
        <br>
        <p>이 복합적인 파라미터의 명목상의 범위는 (−∞,∞)입니다.</p>
        <br>
        <p>AudioBufferSourceNode는 내부 boolean 슬롯 [[buffer set]]과 함께 생성되는데, 이는 초기적으로 false로 설정됩니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioBufferSourceNode : AudioScheduledSourceNode {
  constructor (BaseAudioContext context,
               optional AudioBufferSourceOptions options = {});
  attribute AudioBuffer? buffer;
  readonly attribute AudioParam playbackRate;
  readonly attribute AudioParam detune;
  attribute boolean loop;
  attribute double loopStart;
  attribute double loopEnd;
  undefined start (optional double when = 0,
                   optional double offset,
                   optional double duration);
};
        </pre>
        <br>
        <h3>1.9.1. 생성자</h3>
        <br>
        <dl>
          <dt><i>AudioBufferSourceNode(context, options)</i></dt>
          <dd>생성자가 BaseAudioContext c와 옵션 객체 option과 함께 호출되었을 때, user agent는 반드시 AudioNode this를, context와 options를 인자로서 함께 초기화해야만 합니다.</dd>
        </dl>
        <br>
        <table>
          <caption>AudioBufferSourceNode.constructor() 메서드의 인자</caption>
          <tr>
            <td>매개변수</td>
            <td>자료형</td>
            <td>nullable</td>
            <td>optional</td>
            <td>설명</td>
          </tr>
          <tr>
            <td>context</td>
            <td>BaseAudioContext</td>
            <td>x</td>
            <td>x</td>
            <td>이 새로운 AudioBufferSourceNode가 연관될 BaseAudioContext.</td>
          </tr>
          <tr>
            <td>options</td>
            <td>AudioBufferSourceOptions</td>
            <td>x</td>
            <td>v</td>
            <td>이 AudioBufferSourceNode의 선택적인 초기 파라미터 값.</td>
          </tr>
        </table>
        <br>
        <h3>1.9.2. 어트리뷰트</h3>
        <br>
        <dl>
          <dt><i>buffer</i> / 자료형: AudioBuffer / nullable</dt>
          <dd>재생될 오디오 에셋을 나타냅니다.</dd>
          <br>
          <dd>buffer 어트리뷰트를 설정하기 위해서, 다음의 절차를 수행합니다.
            <ol>
              <li>new buffer를 AudioBuffer 혹은 buffer에 할당된 null 값이라고 하자.</li>
              <li>만약 new buffer가 null이 아니고 [[buffer set]]이 true라면, InvalidStateError를 발생시키고 이 절차를 중단한다.</li>
              <li>만약 new buffer가 null이라면, [[buffer set]]을 true로 설정한다.</li>
              <li>new buffer를 buffer 어트리뷰트에 할당한다.</li>
              <li>만약 start()가 이 노드에서 이전에 호출되었었다면, 이 buffer에서 acquire the content 동작을 수행한다.</li>
            </ol>
          </dd>
          <br>
          <dt><i>detune</i> / 자료형: AudioParam / 읽기 전용</dt>
          <dd>렌더링되는 오디오 스트림의 속도를 조절하기 위한 추가적인 파라미터 (단위: 센트). 이 파라미터는 playbackRate와 함께 computedPlaybackRate를 형성하는 compound parameter입니다.</dd>
          <br>
          <dd>
            <table>
              <tr>
                <td>매개변수</td>
                <td>값</td>
                <td>참고사항</td>
              </tr>
              <tr>
                <td>defaultValue</td>
                <td>0</td>
                <td></td>
              </tr>
              <tr>
                <td>minValue</td>
                <td>most-negative-single-float</td>
                <td>약 -3.4028235e38</td>
              </tr>
              <tr>
                <td>maxValue</td>
                <td>most-positive-single-float</td>
                <td>약 3.4028235e38</td>
              </tr>
              <tr>
                <td>automationRate</td>
                <td>"k-rate"</td>
                <td>automation rate constraints를 가짐</td>
              </tr>
            </table>
          </dd>
          <br>
          <dt><i>loop</i> / 자료형: boolean</dt>
          <dd>loopStart와 loopEnd에 의해 지정된 오디오 데이터 영역이 루프 내에서 계속 재생되어야 하는지를 나타냅니다. 기본값은 false입니다.</dd>
          <br>
          <dt><i>loopEnd</i> / 자료형: double</dt>
          <dd>만약 loop 어트리뷰트가 참이라면 루프가 끝나야 하는 선택적인 playhead position. 이것의 값은 루프의 내용을 제외합니다. 이것의 기본값은 0이고, 0과 버퍼의 duration 사이의 어떤 값으로도 유용하게 설정될 수 있습니다. 만약 loopEnd가 0보다 작거나 같거나, 버퍼의 duration보다 크다면, 루프는 버퍼의 끝에서 끝날 것입니다.</dd>
          <br>
          <dt><i>loopStart</i> / 자료형: double</dt>
          <dd>만약 loop 어트리뷰트가 참이라면 루프가 시작해야 할 추가적인 playhead position. 기본값은 0이며, 0과 버퍼의 duration 사이의 어떠한 값으로도 유용하게 설정될 수 있습니다. 만약 loopStart가 0보다 작으면, 루프는 0에 시작할 것입니다. 만약 loopStart가 버퍼의 duration보다 크다면, 루프는 버퍼의 끝에서 시작할 것입니다.</dd>
          <br>
          <dt><i>playbackRate / 자료형: AudioParam / 읽기 전용</i></dt>
          <dd>오디오 스트림을 렌더링할 속도. 이것은 detune과 함께 computedPlaybackRate를 형성하는 compound parameter입니다.</dd>
          <br>
          <dd>
            <table>
              <tr>
                <td>매개변수</td>
                <td>값</td>
                <td>참고사항</td>
              </tr>
              <tr>
                <td>defaultValue</td>
                <td>1</td>
                <td></td>
              </tr>
              <tr>
                <td>minValue</td>
                <td>most-negative-single-float</td>
                <td>약 -3.4028235e38</td>
              </tr>
              <tr>
                <td>maxValue</td>
                <td>most-positive-single-float</td>
                <td>약 3.4028235e38</td>
              </tr>
              <tr>
                <td>automationRate</td>
                <td>"k-rate"</td>
                <td>automation rate constraints를 가짐</td>
              </tr>
            </table>
          </dd>
        </dl>
        <br>
        <h3>1.9.3. 메서드</h3>
        <br>
        <dl>
          <dt><i>start(when, offset, duration)</i></dt>
          <dd>사운드가 정확한 시간에 재생되도록 예정합니다.</dd>
          <br>
          <dd>⌛ 이 메서드가 호출되었을 때, 아래의 절차를 수행합니다.
            <ol>
              <li>⌛ 만약 이 AudioBufferSourceNode의 내부 슬롯 [[source started]]가 true라면, InvalidStateError 예외가 반드시 발생되어야만 합니다.</li>
              <li>아래에서 설명되는 parameter constraints에 기인해 반드시 발생되어야만 하는 오류에 대해서 확인합니다. 만약 이 과정에서 어떠한 오류라도 발생된다면, 이 과정들을 중단합니다.</li>
              <li>이 AudioBufferSourceNode의 내부 슬롯 [[source started]]을 true로 설정합니다.</li>
              <li>이 AudioBufferSourceNode를 시작시키기 위해서, 메시지 내의 파라미터 값들을 포함해, 컨트롤 메시지를 queue합니다.</li>
              <li>만약 버퍼가 설정되어 있다면 버퍼의 컨텐츠를 획득합니다.</li>
              <li>오직 아래의 모든 조건들이 만족되었을 때만 연관된 AudioContext에 컨트롤 메시지를 보내 이 AudioContext의 렌더링 스레드의 실행을 시작합니다.
                <ol>
                  <li>컨텍스트의 [[control thread state]]가 suspended입니다.</li>
                  <li>컨텍스트가 시작되도록 허용되었습니다.</li>
                  <li>[[suspended by user]] 플래그가 false입니다.</li>
                </ol>
                <br>
                <div class="note">
                  <p>이것은 start()가 start()가 호출되지 않았다면 시작되도록 허용되지 않았을 AudioContext를 시작하도록 허용합니다.</p>
                </div>
              </li>
            </ol>
          </dd>
          <br>
          <dd>컨트롤 메시지를 실행해 AudioBufferSourceNode를 시작시킨다는 것은 다음의 재생 알고리즘의 handleStart()를 호출한다는 것을 의미합니다.</dd>
          <br>
          <dd>
            <table>
              <caption>AudioBufferSourceNode.start(when, offset, duration) 메서드의 인자</caption>
              <tr>
                <td>매개변수</td>
                <td>자료형</td>
                <td>nullable</td>
                <td>optional</td>
                <td>설명</td>
              </tr>
              <tr>
                <td>when</td>
                <td>double</td>
                <td>x</td>
                <td>v</td>
                <td>when 매개변수는 어떤 시간에 (단위: 초) 소리가 재생을 시작해야 하는지를 기술합니다. 이것은 AudioContext의 currentTime 어트리뷰트와 같은 시간 좌표계에 있습니다. 만약 이 값에 0이 전달되었거나 이 값이 <strong>currentTime</strong>보다 작다면, 소리는 즉시 재생되어야 합니다. ⌛ 만약 when이 음수라면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
              </tr>
              <tr>
                <td>offset</td>
                <td>double</td>
                <td>x</td>
                <td>v</td>
                <td>offset 매개변수는 재생이 시작될 playhead position을 제공합니다. 만약 이 값에 0이 전달되었다면, 재생은 버퍼의 시작점에서부터 시작될 것입니다. ⌛ 만약 offset이 음수라면 RangeError 예외가 반드시 발생되어야만 합니다. 만약 offset이 loopEnd보다 크고, playbackRate가 양수거나 0이고, loop이 true라면, 재생은 loopEnd에서 시작될 것입니다. 만약 offset이 loopStart보다 크고, playbackRate가 음수이고, loop이 true라면, 재생은 loopStart에서 시작될 것입니다. offset은 startTime에 도달했을 때 조용히 [0, duration]으로 clamp되며, 여기서 duration은 이 AudioBufferSourceNode의 buffer 어트리뷰트로 설정된 AudioBuffer의 duration 어트리뷰트의 값입니다.</td>
              </tr>
              <tr>
                <td>duration</td>
                <td>double</td>
                <td>x</td>
                <td>v</td>
                <td>duration 매개변수는 재생될 사운드의 duration을 기술하며, 출력될 전체 버퍼 컨텐츠의 초로서 표현되고, 전체 혹은 부분 loop 반복을 포함합니다. duration의 단위는 playbackRate의 효과에 독립적입니다. 예를 들어, 0.5의 playbackRate를 가진 5초의 duration은 5초의 버퍼 컨텐츠를 절반의 속도로 출력할 것이며, 이는 10초의 들을 수 있는 출력을 생성합니다. ⌛ 만약 duration이 음수면 RangeError 예외가 반드시 발생되어야만 합니다.</td>
              </tr>
            </table>
          </dd>
          <br>
          <dd>반환 유형: undefined</dd>
        </dl>
        <br>
        <h3>1.9.4. AudioBufferSourceOptions</h3>
        <br>
        <p>이것은 AudioBufferSourceNode를 생성하는 옵션을 명시합니다. 모든 멤버는 선택적입니다. 만약 명시되지 않는다면, 일반적인 기본값이 이 노드를 생성하는 데 사용됩니다.</p>
        <br>
        <pre>
dictionary AudioBufferSourceOptions {
  AudioBuffer? buffer;
  float detune = 0;
  boolean loop = false;
  double loopEnd = 0;
  double loopStart = 0;
  float playbackRate = 1;
};
        </pre>
        <br>
        <h4>1.9.4.1. 딕셔너리 AudioBufferSourceOptions 멤버</h4>
        <br>
        <dl>
          <dt><i>buffer</i> / 자료형: AudioBuffer / nullable</dt>
          <dd>재생될 오디오 에셋. 이것은 AudioBufferSourceNode의 buffer 어트리뷰트에 버퍼를 할당하는 것과 동일합니다.</dd>
          <br>
          <dt><i>detune</i> / 자료형: float / 기본값: 0</dt>
          <dd>detune AudioParam의 초기값.</dd>
          <br>
          <dt><i>loop</i> / 자료형: boolean / 기본값: false</dt>
          <dd>loop 어트리뷰트의 초기값.</dd>
          <br>
          <dt><i>loopEnd</i> / 자료형: double / 기본값: 0</dt>
          <dd>loopEnd 어트리뷰트의 초기값.</dd>
          <br>
          <dt><i>loopStart</i> / 자료형: double / 기본값: 0</dt>
          <dd>loopStart 어트리뷰트의 초기값.</dd>
          <br>
          <dt><i>playbackRate</i> / 자료형: float / 기본값: 1</dt>
          <dd>playbackRate AudioParam의 초기값.</dd>
        </dl>
        <br>
        <h3>1.9.5. 반복</h3>
        <br>
        <p><i>이 섹션은 비규범적입니다. 규범적 요구사항에 대해서는 playback algorithm을 참고하십시오.</i></p>
        <br>
        <p>loop 어트리뷰트를 true로 설정하는 것은 loopStart와 loopEnd 엔드포인트에 의해 정의된 버퍼의 영역의 재생이, 한 번 반복 영역의 어떠한 부분이라도 재생되었다면 무기한으로 계속되게 유발합니다. loop이 true로 유지되는 동안, 반복되는 재생은 다음 중 하나가 발생할 때까지 계속될 것입니다.</p>
        <br>
        <ul>
          <li>stop()이 호출되었습니다.</li>
          <li>예정된 stop time에 도달했습니다.</li>
          <li>start()가 duration 값과 함께 호출되었고, duration이 초과되었습니다.</li>
        </ul>
        <br>
        <p>반복되는 부분은 loopStart까지로부터의 영역을 차지하는 것으로 생각되나, loopEnd는 포함하지 않습니다. 반복 영역의 재생의 방향은 노드의 playback rate의 부호를 따릅니다. 양수 playback rate에 대해서, 반복은 loopStart에서 loopEnd까지 발생합니다. 음수 rate에 대해서, 반복은 loopEnd에서 loopStart까지 발생합니다.</p>
        <br>
        <p>반복은 start()의 offset 인자의 구현에 영향을 주지 않습니다. 재생은 언제나 요청된 offset에서 시작하고, 반복은 오직 한 번 반복의 부분이 재생 중에 조우되면 시작합니다.</p>
        <br>
        <p>효과적인 반복의 시작점과 끝점은, 아래의 알고리즘에 명시된 대로 0과 버퍼의 duration 범위 내에 있을 것이 요구됩니다. loopEnd는 loopStart에 또는 loopStart의 이후에 있을 것이 추가적으로 제약됩니다. 만약 이 제약 사항 중 어느 것이라도 위반되었다면, 반복은 전체 버퍼 내용을 포함하는 것으로 고려됩니다.</p>
        <br>
        <p>반복의 끝점은 subsample accuracy를 가집니다. 끝점이 정확한 sample frame offset에 떨어지지 않을 때, 또는 playback rate가 1과 같지 않을 때, 반복의 재생은 보간되어 반복의 시작과 끝을 함께 splice해 마치 딱 반복된 오디오가 버퍼의 순차적이고, 비반복된 영역에서 발생한 것처럼 합니다.</p>
        <br>
        <p>반복에 관련된 속성들은 버퍼의 재생 중에 변할 수도 있고, 일반적으로 다음 rendering quantum에 영향을 미칩니다. 실제 결과는 다음의 규범적인 재생 알고리즘에 의해 정의됩니다.</p>
        <br>
        <p>loopStart와 loopEnd 어트리뷰트의 기본값은 0입니다. 0의 loopEnd 값은 버퍼의 길이와 같기 때문에, 기본 끝점은 전체 버퍼가 반복에 포함되는 것을 유발합니다.</p>
        <br>
        <p>반복 끝점의 값은 버퍼의 샘플 레이트의 측면에서 time offset으로서 표현되며, 이는 이 값은 재생의 과정 중에 동적으로 변화할 수 있는 노드의 playbackRate 파라미터에 독립적임을 의미한다는 것에 주의하십시오.</p>
        <br>
        <h3>1.9.6. AudioBuffer 내용의 재생</h3>
        <br>
        <p>이 규범적인 섹션은 버퍼의 내용의 재생을 명시하는데, 재생이 합동하여 작동하는 다음의 요소들에 의해 영향받는다는 사실을 설명합니다.</p>
        <br>
        <ul>
          <li>sub-sample 정밀도로 표현될 수 있는, 시작하는 offset.</li>
          <li>sub-sample 정밀도로 표현될 수 있고 재생 도중에 동적으로 변화할 수 있는, 반복 포인트.</li>
          <li>양수일 수도 있고 음수일 수도 있는 유한한 값을 취할 수 있는 하나의 computedPlaybackRate를 산출하기 위해 결합하는, playback rate와 detuning 파라미터.</li>
        </ul>
        <br>
        <p>AudioBufferSourceNode로부터 내부적으로 출력을 생성하는 다음의 알고리즘은 다음의 원칙들을 따릅니다.</p>
        <br>
        <ul>
          <li>버퍼의 리샘플링은 UA에 의해 원해지는 지점 어디서든 임의적으로 수행되어 출력의 품질 또는 효율성을 증가시킬 수 있습니다.</li>
          <li>sub-sample start offset 또는 반복 포인트는 샘플 프레임간의 추가적인 보간을 요구할 수도 있습니다.</li>
          <li>반복된 버퍼의 재생은 반복된 오디오 컨텐츠의 연이은 발생을 포함하고 있는 반복되지 않은 버퍼와 동일하게 동작해야 하며, 보간에 의한 효과를 제외합니다.</li>
        </ul>
        <br>
        <p>알고리즘의 설명은 아래와 같습니다.</p>
        <br>
        <pre>
let buffer; // 이 노드에 의해 사용되는 AudioBuffer
let context; // 이 노드에 의해 사용되는 AudioContext
// 다음의 변수들은 이 노드의 어트리뷰트와 AudioParam 값들을 포착합니다.
// 이 변수들은 k-rate 기반으로 갱신되며, 각 process() 호출에 앞섭니다.
let loop;
let detune;
let loopStart;
let loopEnd;
let playbackRate;
// 노드의 재생 파라미터에 대한 변수들
let start = 0, offset = 0, duration = Infinity; // Set by start()
let stop = Infinity; // Set by stop()
// 노드의 재생 상태를 추적하기 위한 변수들
let bufferTime = 0, started = false, enteredLoop = false;
let bufferTimeElapsed = 0;
let dt = 1 / context.sampleRate;
// start 메서드 호출의 호출을 다룸
function handleStart(when, pos, dur) {
  if (arguments.length >= 1) {
    start = when;
  }
  offset = pos;
  if (arguments.length >= 3) {
    duration = dur;
  }
}
// stop 메서드 호출의 호출을 다룸
function handleStop(when) {
  if (arguments.length >= 1) {
    stop = when;
  } else {
    stop = context.currentTime;
  }
}
// 몇 개의 샘플 프레임에 대해 다(多)채널 신호를 보간합니다.
// 신호 값들의 배열을 반환합니다.
function playbackSignal(position) {
  /*
     이 함수는 버퍼의 재생 신호 함수를 제공하는데, 이는 playhead 포지션을
     출력 신호의 집합에, 각 출력 신호에 대해 하나씩, 대응시키는(map) 함수입니다.
     만약 |position|이 버퍼 내의 정확한 샘플 프레임의 위치와 일치한다면,
     이 함수는 그 프레임을 반환합니다. 그렇지 않을 경우, 이것의 반환 값은
     |position|의 이웃에 있는 샘플 프레임들을 보간하는 UA에 의해 제공되는 알고리즘에 의해
     결정됩니다.
     만약 |position|이 |loopEnd|보다 크거나 같고 버퍼 내에 연이은 샘플 프레임이 없다면,
     보간은 |loopStart|에서 시작하는 연이은 프레임들의 시퀸스에 기반하여야 합니다.
    */
   ...
}
// 출력에 의해 정이되는 채널 배열 내에
// 배치될 하나의 오디오 render quantum을 생성합니다.
// 출력될 |numberOfFrames| 샘플 프레임들의 배열을 반환합니다.
function process(numberOfFrames) {
  let currentTime = context.currentTime;// 다음 렌더링된 프레임의 context time
  const output = []; // 렌더링된 샘플 프레임을 누적시킵니다
  // playback rate에 영향을 미치는 두 개의 k-rate 파라미터를 결합합니다.
  const computedPlaybackRate = playbackRate * Math.pow(2, detune / 1200);
  // 해당될 때 loop endpoint를 결정합니다
  let actualLoopStart, actualLoopEnd;
  if (loop && buffer != null) {
    if (loopStart >= 0 && loopEnd > 0 && loopStart < loopEnd) {
      actualLoopStart = loopStart;
      actualLoopEnd = Math.min(loopEnd, buffer.duration);
    } else {
      actualLoopStart = 0;
      actualLoopEnd = buffer.duration;
    }
  } else {
    // 만약 loop flag가 false라면, 입력된 반복의 모든 기록을 제거합니다.
    enteredLoop = false;
  }
  // buffer가 null인 경우를 다룹니다
  if (buffer == null) {
    stop = currentTime; // 모든 시간에 대해 0의 출력을 강제합니다
  }
  // quantum 내의 각 샘플 프레임을 렌더링합니다.
  for (let index = 0; index < numberOfFrames; index++) {
    // currentTime과 bufferTimeElapsed가
    // 허용되는 재생 범위 내에 있는지 검사합니다.
    if (currentTime < start || currentTime >= stop || bufferTimeElapsed >= duration) {
      output.push(0); // 이 샘플 프레임은 무음입니다
      currentTime += dt;
      continue;
    }
    if (!started) {
      // buffer가 재생을 시작하였고 초기
      // playhead poisition을 얻었음에 주의하십시오.
      if (loop && computedPlaybackRate >= 0 && offset >= actualLoopEnd) {
        offset = actualLoopEnd;
      }
      if (computedPlaybackRate < 0 && loop && offset < actualLoopStart) {
        offset = actualLoopStart;
      }
      bufferTime = offset;
      started = true;
    }
    // 반복에 관련된 계산을 다룹니다
    if (loop) {
      // 반복된 부분이 처음으로 입력되었는지를 결정합니다.
      if (!enteredLoop) {
        if (offset < actualLoopEnd && bufferTime >= actualLoopStart) {
          // 재생이 반복 전에 혹은 내에서 시작되었고, playhead가
          // 이제 지난 loop start입니다
          enteredLoop = true;
        }
        if (offset >= actualLoopEnd && bufferTime < actualLoopEnd) {
          // 재생이 반복 후에 시작되었고, playhead가 이제
          // loop end에 앞섭니다.
          enteredLoop = true;
        }
      }
      // 필요할 때 loop 반복(iteration)을 wrap합니다. enteredLoop이
      // 선행하는 조건문 내에서 true가 될 수 있음에 주의하십시오.
      if (enteredLoop) {
        while (bufferTime >= actualLoopEnd) {
          bufferTime -= actualLoopEnd - actualLoopStart;
        }
        while (bufferTime < actualLoopStart) {
          bufferTime += actualLoopEnd - actualLoopStart;
        }
      }
    }
    if (bufferTime >= 0 && bufferTime < buffer.duration) {
      output.push(playbackSignal(bufferTime));
    } else {
      output.push(0); // 지난 버퍼의 끝, 그러므로 무음의 프레임을 출력합니다
    }
    bufferTime += dt * computedPlaybackRate;
    bufferTimeElapsed += dt * computedPlaybackRate;
    currentTime += dt;
  } // render quantum 반복의 끝
  if (currentTime >= stop) {
    // 이 노드의 재생 상태의 끝. 추가적인 process() 호출이
    // 발생하지 않을 것입니다. 출력 채널의 숫자를 1로 설정하는 변경을 예정합니다.
  }
  return output;
}
        </pre>
        <br>
        <h2>1.10. AudioDestinationNode 인터페이스</h2>
        <p>이것은 최종 오디오 목적지를 나타내는 AudioNode이며 유저가 최종적으로 듣게 될 것입니다. 이것은 종종 스피커에 연결되는 오디오 출력 장치로써 여겨집니다. 들릴 모든 렌더링된 오디오는 이 노드로 라우팅될 것입니다. 즉, AudioContext의 라우팅 그래프의 "최종" 노드입니다. AudioContext당 오직 하나의 AudioDestinationNode가 있는데, 이는 AudioContext의 destination 어트리뷰트를 통해 제공됩니다.</p>
        <br>
        <p>AudioDestinationNode의 출력은 이것의 입력을 합침으로써 생산되며, MediaStreamAudioDestinationNode 또는 MediaRecorder로 가는 AudioContext의 출력을 캡쳐할 수도 있습니다 ([mediastream-recording] 에서 기술됩니다).</p>
        <br>
        <p>AudioDestinationNode는 AudioContext나 OfflineAudioContext의 목적지일 수 있고, 채널 속성들은 컨텍스트가 무엇인지에 달려 있습니다.</p>
        <br>
        <p>AudioContext에 대해, 기본값은 아래와 같습니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"explicit"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>channelCount는 maxChannelCount보다 작거나 같은 값 무엇으로든 설정될 수 있습니다. 만약 이 값이 유효한 범위 내에 없다면 IndexSizeError는 반드시 발생되어야 합니다. 구체적인 예제로써, 만약 오디오 하드웨어가 8 채널의 출력을 지원한다면, channelCount는 8로 설정되고, 출력의 8 채널을 렌더링할 수 있을 것입니다.</p>
        <br>
        <p>OfflineAudioContext에 대해, 기본값은 아래와 같습니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>numberOfChannels</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"explicit"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>numberOfChannels는 OfflineAudioContext를 생성할 때 명시됩니다. 이 값은 변하지 않을 것입니다. 만약 channelCount가 다른 값으로 변경된다면 NotSupportedError 예외가 반드시 발생되어야 합니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioDestinationNode : AudioNode {
  readonly attribute unsigned long maxChannelCount;
};
        </pre>
        <br>
        <h3>1.10.1. 어트리뷰트</h3>
        <p>추후 번역</p>
        <br>
        <h2>1.11. AudioListener 인터페이스</h2>
        <p>이 인터페이스는 오디오 장면을 듣고 있는 사람의 위지와 방향을 나타냅니다. 모든 PannerNode 객체는 BaseAudioContext의 listener에 관하여 공간화합니다. § 6 공간화에 대한 더욱 자세한 사항은 공간화/패닝을 참고하세요.</p>
        <br>
        <p>positionX, positionY, positionZ 파라미터는 3D 카르테시안 좌표공간에서의 listener의 위치를 나타냅니다. PannerNode 객체는 공간화를 위해 각 오디오 소스에 관련된 이 위치를 사용합니다.</p>
        <br>
        <p>forwardX, forwardY, forwardZ 파라미터는 3D 공간에서의 방향 벡터를 나타냅니다. forward 벡터와 up 벡터 둘 다 listener의 방향을 결정하기 위해 사용됩니다. 간단한 말로 하자면, forward 벡터는 사람의 코가 가리키고 있는 방향을 나타냅니다. up 벡터는 사람의 머리 위가 가리키고 있는 방향을 나타냅니다. 이 두 벡터는 선형적으로 독립적일 것이 기대됩니다. 어떻게 이 값들이 해석되어야 하는지에 대한 규범적인 요구 조건에 대해서는, § 6 공간화/패닝 섹션을 참고해 보세요.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioListener {
  readonly attribute AudioParam positionX;
  readonly attribute AudioParam positionY;
  readonly attribute AudioParam positionZ;
  readonly attribute AudioParam forwardX;
  readonly attribute AudioParam forwardY;
  readonly attribute AudioParam forwardZ;
  readonly attribute AudioParam upX;
  readonly attribute AudioParam upY;
  readonly attribute AudioParam upZ;
  undefined setPosition (float x, float y, float z);
  undefined setOrientation (float x, float y, float z, float xUp, float yUp, float zUp);
};
        </pre>
        <br>
        <h3>1.11.1. 어트리뷰트</h3>
        <h3>1.11.2. 메서드</h3>
        <h3>1.11.3. 프로세싱</h3>
        <br>
        <h2>1.12. AudioProcessingEvent 인터페이스 -- 폐기됨</h2>
        <h3>1.12.1. 어트리뷰트</h3>
        <h3>1.12.2. AudioProcessingEventInit</h3>
        <h4>1.12.2.1. 딕셔너리 AudioProcessingEventInit 멤버</h4>
        <br>
        <h2>1.13. BiquadFilterNode 인터페이스</h2>
        <p>BiquadFilterNode는 아주 일반적인 low-order 필터를 구현하는 AudioNode 프로세서입니다.</p>
        <br>
        <p>low-order 필터들은 기본적인 음색 제어 (bass, mid, treble), 그래픽 이퀼라이저, 더욱 발전된 필터의 구성 요소입니다. 다수의 BiquadFilterNode 필터는 결합되어 더욱 복잡한 필터를 형성할 수 있습니다. 주파수와 같은 필터의 파라미터는 filter sweep등을 위해 시간에 따라 변할 수 있습니다. 각 BiquadFilterNode는 아래의 IDL에 보여지는 몇 개의 일반적인 필터 유형 중 하나로써 설정될 수 있습니다. 기본 필터 유형은 "lowpass"입니다.</p>
        <br>
        <p>추후 번역</p>
        <h2>1.14. ChannelMergerNode 인터페이스</h2>
        <p>ChannelMergerNode는 더욱 고급의 어플리케이션에서의 사용을 위한 것이고 종종 ChannelSplitterNode와 함께 사용될 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 인터페이스는 다수의 오디오 스트림으로부터 하나의 오디오 스트림으로 채널을 합치기 위한 AudioNode를 나타냅니다. 이것은 가변적인 입력의 수 (기본값 6)을 가지지만, 이것의 모두가 연결될 필요가 있는 것은 아닙니다. 하나의 출력이 있는데, 이 출력의 오디오 스트림은 입력들 중 어느 것이 활발히 처리 중일 때 입력의 수와 동일한 채널의 수를 가집니다. 만약 입력 중 아무 것도 활발히 처리 중이 아니라면, 출력은 무음인 채널 하나입니다.</p>
        <br>
        <h2>1.15. ChannelSplitterNode 인터페이스</h2>
        <p>ChannelSplitterNode는 더욱 고급의 어플리케이션에서의 사용을 위한 것이고 종종 ChannelMergerNode와 함께 사용될 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 인터페이스는 라우팅 그래프에서 오디오 스트림의 각 채널에 접근하기 위한 AudioNode를 나타냅니다. 이것은 하나의 입력을 가지고 있고, 입력 오디오 스트림의 채널의 수와 같은 "작동하는" 출력의 수를 가지고 있습니다. 예를 들어, 스테레오 입력이 ChannelSplitterNode에 연결되었다면 작동하는 출력의 수는 2일 것입니다 (좌측 채널로부터 1개, 우측 채널로부터 1개). 항상 총 수 N의 출력이 있으며 (AudioContext 메서드 createChannelSplitter()의 numberOfOutputs 매개변수에 의해 결정됨), 기본값은 만약 이 값이 제공되지 않았다면 6입니다. "작동"하지 않는 모든 출력은 무음을 출력할 것이고 통상적으로 어떠한 것에도 연결되지 않을 것입니다.</p>
        <br>
        <h2>1.16. ConstantSourceNode 인터페이스</h2>
        <p>이 인터페이스는 출력이 명목상 상수 값인 고정된 오디오 소스를 나타냅니다. 이것은 일반적으로 고정된 소스 노드로서 유용하고 이것은 마치 이것의 오프셋을 자동화함으로써 또는 이것에 다른 노드를 연결함으로써 생성 가능한 AudioParam인 것처럼 사용될 수 있습니다.</p>
        <br>
        <p>이 노드의 하나의 출력은 한 개의 채널 (모노) 로 이루어져 있습니다.</p>
        <br>
        <h2>1.17. ConvolverNode 인터페이스</h2>
        <p>이 인터페이스는 임펄스 응답이 주어졌을 때 선형 콘볼루션 이펙트를 적용하는 프로세싱 노드를 나타냅니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 모노 (1 채널) 혹은 스테레오 (2 채널) 이고 증가될 수 없습니다. 더욱 많은 채널을 가진 노드로부터의 연결은 적절하게 다운믹스될 것입니다.</p>
        <p>이 노드에는 channelCount 제약과 channelCountMode 제약이 있습니다. 이 제약들은 이 노드에의 입력이 모노나 스테레오임을 보장합니다.</p>
        <br>
        <h2>1.18. DelayNode 인터페이스</h2>
        <p>딜레이 라인은 오디오 어플리케이션에서 기본적인 구성 요소입니다. 이 인터페이스는 하나의 입력과 하나의 출력을 가진 AudioNode입니다.</p>
        <p>표는 추후 번역</p>
        <p>출력의 채널 수는 항상 입력의 채널 수와 같습니다.</p>
        <p>이것은 들어오는 오디오 신호를 특정한 양만큼 지연시킵니다. 정확히 말하자면, 각 시간 t, 입력 신호 input(t), 지연 시간 delayTime(t), 출력 신호 output(t)에서, 출력은 output(t) = input(t - delayTime(t))일 것입니다. 기본 delayTime은 0초 (지연 없음) 입니다.</p>
        <p>DelayNode의 입력의 채널 수가 바뀔 때 (따라서 출력 채널 수도 바뀜), 노드에 의해 아직 출력되지 않았고 이것의 내부 상태의 일부인 지연된 오디오 샘플들이 있을 수 있습니다. 만약 이 샘플들이 다른 채널 카운트와 함께 일찍 수신되었다면, 이것들은 반드시 새롭게 수신된 입력과 혼합되기 전에 업믹스되거나 다운믹스되어 모든 내부 딜레이 라인 믹싱이 하나의 지배적인 채널 레이아웃을 사용하여 일어나도록 해야 합니다.</p>
        <div class="note">
          <p>노트: 정의에 의해, DelayNode는 딜레이의 양과 동일한 오디오 프로세싱 레이턴시를 삽입합니다.</p>
        </div>
        <br>
        <pre>
[Exposed=Window]
interface DelayNode : AudioNode {
  constructor (BaseAudioContext context, optional DelayOptions options = {});
  readonly attribute AudioParam delayTime;
};
        </pre>
        <br>
        <h2>1.19. DynamicsCompressorNode 인터페이스</h2>
        <p>DynamicsCompressorNode는 dynamics compression 이펙트를 구현하는 AudioNode 프로세서입니다.</p>
        <p>dynamics compression은 음악 프로덕션과 게임 오디오에서 아주 일반적으로 사용됩니다. 이것은 신호의 가장 소리가 큰 부분의 볼륨을 줄이고 가장 작은 부분의 볼륨을 높입니다. 전반적으로, 크고, 풍부하고, 꽉 찬 소리가 달성될 수 있습니다. 이것은 다수의 각 사운드가 동시에 재생되어 전반적인 신호 레벨을 제어하고 스피커로 가는 오디오 출력이 클리핑(왜곡)되는 것을 방지하는 것을 돕는 게임과 뮤지컬 어플리케이션에서 보통 중요합니다.</p>
        <br>
        <h2>1.20. GainNode 인터페이스</h2>
        <p>오디오 신호의 gain을 변경하는 것은 오디오 어플리케이션에서 기본적인 작동입니다. 이 인터페이스는 하나의 입력과 하나의 출력을 가진 AudioNode입니다.</p>
        <p>표는 추후에 번역</p>
        <p>GainNode의 입력 데이터의 각 채널의 각 샘플은 반드시 gain AudioParam의 computedValue에 의해 곱해져야 합니다.</p>
        <pre>
[Exposed=Window]
interface GainNode : AudioNode {
  constructor (BaseAudioContext context, optional GainOptions options = {});
  readonly attribute AudioParam gain;
};
        </pre>
        <br>
        <h2>1.21. IIRFilterNode 인터페이스</h2>
        <p>IIRFilterNode는 일반적인 IIR 필터를 구현하는 AudioNode 프로세서입니다. 일반적으로, 다음의 이유로 higher-order 필터를 구현함에 있어 BiquadFilterNode를 사용하는 것이 최선입니다.</p>
        <ul>
          <li>일반적으로 수치적 문제에 덜 민감함</li>
          <li>필터 파라미터들이 자동화될 수 있음</li>
          <li>모든 even-ordered IIR 필터를 생성하기 위해 사용될 수 있음</li>
        </ul>
        <br>
        <p>그러나, odd-ordered 필터는 생성될 수 없으므로, 만약 그러한 필터가 필요하거나 자동화가 필요하지 않다면, IIR 필터가 적절할 수 있습니다.</p>
        <p>한 번 생성되고 나면, IIR 필터의 계수들은 변경될 수 없습니다.</p>
        <p>표는 추후 번역</p>
        <p>출력 채널의 수는 항상 입력 채널의 수와 같습니다.</p>
        <pre>
[Exposed=Window]
interface IIRFilterNode : AudioNode {
  constructor (BaseAudioContext context, IIRFilterOptions options);
  undefined getFrequencyResponse (Float32Array frequencyHz,
                                  Float32Array magResponse,
                                  Float32Array phaseResponse);
};
        </pre>
        <br>
        <h2>1.22. MediaElementAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 &lt;audio&gt; 혹은 &lt;video&gt; 요소로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 HTMLMediaElement에 의해 참조되는 미디어 채널의 수와 일치합니다. 따라서, 미디어 요소의 src 어트리뷰트에의 변화는 이 노드에 의해 출력되는 채널의 수를 변화시킬 수 있습니다.</p>
        <p>만약 HTMLMediaElement의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, HTMLMediaElement로부터의 출력은 반드시 리샘플링되어 컨텍스트의 샘플 레이트와 일치해야만 합니다.</p>
        <p>MediaElementAudioSourceNode는 AudioContext의 createMediaElementSource() 메서드 혹은 생성자의 MediaElementAudioSourceOptions 딕셔너리의 mediaElement 멤버를 사용함으로써 HTMLMediaElement가 주어졌을 때 생성됩니다.</p>
        <p>하나의 출력의 채널의 수는 createMediaElementSource()에의 인자로써 전달된 HTMLMediaElement에 의해 참조되는 오디오 채널의 수와 동일하거나, 만약 HTMLMediaElement가 오디오를 가지고 있지 않다면 1입니다.</p>
        <p>MediaElementAudioSourceNode가 생성된 이후에 HTMLMediaElement는 반드시 동일한 양식으로 행동해야만 하나, 하나의 예외가 있습니다. 그 예외란 렌더링된 오디오가 더 이상 직접적으로 들리지 않지만, 대신 라우팅 그래프를 통해 연결되어 있는 MediaElementAudioSourceNode의 결과로써 들리는 것입니다. 따라서 정지, 탐색, 볼륨, src 어트리뷰트 변화, HTMLMediaElement의 다른 측면들은 만약 MediaElementAudioSourceNode와 함께 사용되지 않는다면 반드시 이것들이 일반적으로 행동하는 것처럼 행동해야 합니다.</p>
        <pre>
예제 11
const mediaElement = document.getElementById('mediaElementID');
const sourceNode = context.createMediaElementSource(mediaElement);
sourceNode.connect(filterNode);
        </pre>
        <pre>
[Exposed=Window]
interface MediaElementAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaElementAudioSourceOptions options);
  [SameObject] readonly attribute HTMLMediaElement mediaElement;
};
        </pre>
        <h2>1.23. MediaStreamAudioDestinationNode 인터페이스</h2>
        <p>이 인터페이스는 kind가 "audio"인 하나의 MediaStreamTrack을 가진 MediaStream을 나타내는 오디오 목적지입니다. 이 MediaStream은 노드가 생성될 때 생성되고 stream 어트리뷰트를 통해 접근 가능합니다. 이 스트림은 getUserMedia()를 통해 얻어진 MediaStream과 비슷한 방식으로 사용될 수 있고, 예를 들자면 RTCPeerConnection ([webrtc]에서 기술됨) addStream() 메서드를 사용해 원격 peer에게 전송될 수 있습니다.</p>
        <p>표는 추후 번역</p>
        <p>입력 채널의 수는 기본적으로 2 입니다 (스테레오).</p>
        <pre>
[Exposed=Window]
interface MediaStreamAudioDestinationNode : AudioNode {
  constructor (AudioContext context, optional AudioNodeOptions options = {});
  readonly attribute MediaStream stream;
};
        </pre>
        <br>
        <h2>1.24. MediaStreamAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 MediaStream으로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 MediaStreamTrack의 채널의 수와 일치합니다. MediaStreamTrack이 끝났을 때, 이 AudioNode는 무음인 하나의 채널을 출력합니다.</p>
        <p>만약 MediaStreamTrack의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, MediaStreamTrack의 출력은 컨텍스트의 샘플 레이트와 일치되기 위해 리샘플링됩니다.</p>
        <pre>
[Exposed=Window]
interface MediaStreamAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaStreamAudioSourceOptions options);
  [SameObject] readonly attribute MediaStream mediaStream;
};
        </pre>
        <br>
        <h2>1.25. MediaStreamTrackAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 MediaStreamTrack으로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 mediaStreamTrack의 채널의 수와 일치합니다.</p>
        <p>만약 MediaStreamTrack의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, mediaStreamTrack의 출력은 컨텍스트의 샘플 레이트와 일치되도록 리샘플링될 것입니다.</p>
        <pre>
[Exposed=Window]
interface MediaStreamTrackAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaStreamTrackAudioSourceOptions options);
};
        </pre>
        <br>
        <h2>1.26. OscillatorNode 인터페이스</h2>
        <p>OscillatorNode는 주기적인 파형을 생성하는 오디오 소스를 나타냅니다. 이것은 몇 가지의 일반적으로 사용되는 파형으로 설정될 수 있습니다. 추가적으로, PeriodicWave 객체의 사용을 통해 이것은 임의의 주기적인 파형으로 설정될 수 있습니다.</p>
        <p>오실레이터는 오디오 합성에서 일반적인 기본 구성 요소입니다. OscillatorNode는 start() 메서드에 의해 명시된 시간에 소리를 발산하기 시작할 것입니다.</p>
        <p>수학적으로 말해서, 연속 시간 주기 파형은 주파수 영역에서 고려되었을 때 아주 높은 (혹은 무한히 높은) 주파수 정보를 가질 수 있습니다. 이 파형이 특정한 샘플 레이트에서 이산 시간 디지털 오디오 신호로 샘플링되었을 때, 파형을 디지털 형태로 변환하기 전에 나이퀴스트 주파수보다 높은 고주파 정보를 폐기하기 (거르기) 위해 관심이 반드시 기울여져야 합니다. 만약 이러한 작업이 이루어지지 않는다면, (나이퀴스트 주파수보다) 높은 주파수의 에일리어싱이 나이퀴스트 주파수보다 낮은 주파수로 mirror image로서 fold back될 것입니다. 많은 경우에서 이것은 청각적으로 불쾌한 결과를 초래할 것입니다. 이것은 오디오 디지털 신호 처리의 기본적이고 잘 이해된 원칙입니다.</p>
        <p>구현이 이 에일리어싱을 방지하기 위해 취할 수 있는 몇 가지 실용적인 접근이 있습니다. 접근에 상관없이, 이상적인 이산 시간 디지털 오디오 신호는 수학적으로 잘 정의됩니다. 이 구현에 대한 trade-off는 이 이상을 달성하는 것에 대한 충실도(fidelity)에 대한 (CPU 사용의 측면에서의) 구현 비용의 문제입니다.</p>
        <p>구현이 이 이상을 달성함에 있어 얼마간의 주의를 취할 것이 기대되지만, lower-end 하드웨어에서 낮은 품질과 덜 비용이 드는 접근을 고려하는 것은 타당합니다.</p>
        <p>frequency와 detune은 둘 다 a-rate 파라미터이고, 복합적인 파라미터를 형성합니다. 이것들은 함께 사용되어 computedOscFrequency 값을 결정합니다.</p>
        <pre>computedOscFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)</pre>
        <br>
        <p>각 시간에서의 OscillatorNode의 instantaneous 단계는 computedOscFrequency의 definite time integral이며, 노드의 정확한 시작 시간에서 0의 phase angle을 상정합니다. 이것의 명목상의 범위는 [-Nyquist 주파수, Nyquist 주파수] 입니다.</p>
        <p>이 노드의 하나의 출력은 하나의 채널로 이루어져 있습니다 (모노).</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterpretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <pre>
enum OscillatorType {
  "sine",
  "square",
  "sawtooth",
  "triangle",
  "custom"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>sine</i>"</td>
              <td>사인파</td>
            </tr>
            <tr>
              <td>"<i>square</i>"</td>
              <td>duty period 0.5의 사각파</td>
            </tr>
            <tr>
              <td>"<i>sawtooth</i>"</td>
              <td>톱니파</td>
            </tr>
            <tr>
              <td>"<i>triangle</i>"</td>
              <td>삼각파</td>
            </tr>
            <tr>
              <td>"<i>custom</i>"</td>
              <td>사용자 정의 주기파</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface OscillatorNode : AudioScheduledSourceNode {
  constructor (BaseAudioContext context, optional OscillatorOptions options = {});
  attribute OscillatorType type;
  readonly attribute AudioParam frequency;
  readonly attribute AudioParam detune;
  undefined setPeriodicWave (PeriodicWave periodicWave);
};
        </pre>
        <br>
        <h2>1.27. PannerNode 인터페이스</h2>
        <p>이 인터페이스는 들어오는 오디오 스트림을 3차원 공간에 위치시키는 / 공간화시키는 프로세싱 노드를 나타냅니다. 공간화는 BaseAudioContext의 AudioListener (listener 어트리뷰트) 에 관련이 있습니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 모노 (1 채널) 이거나 스테레오 (2 채널) 이고 증가될 수 없습니다. 더 적거나 많은 채널을 가진 노드로부터의 연결은 적절하게 다운믹스되거나 업믹스될 것입니다.</p>
        <p>만약 노드가 활발히 처리 중이라면, 이 노드의 출력은 스테레오 (2 채널) 로 hard-coded되고 설정될 수 없습니다. 만약 이 노드가 활발히 처리 중이 아니라면, 출력은 한 채널의 무음입니다.</p>
        <p>PanningModelType 열거형은 어떤 공간화 알고리즘이 사용되어 3D 공간에서의 오디오를 위치시킬지 결정합니다. 기본값은 "equalpower" 입니다.</p>
        <br>
        <h2>1.28. PeriodicWave 인터페이스</h2>
        <p>PeriodicWave는 OscillatorNode와 함께 쓰일 임의의 주기 파형을 나타냅니다.</p>
        <p>(명세를?) 따르는 구현은 반드시 적어도 8192 요소까지의 PeriodicWave를 지원해야만 합니다.</p>
        <pre>
[Exposed=Window]
interface PeriodicWave {
  constructor (BaseAudioContext context, optional PeriodicWaveOptions options = {});
};
        </pre>
        <br>
        <h2>1.29. ScriptProcessorNode 인터페이스 - 폐기됨</h2>
        <p>이 인터페이스는 스크립트를 사용해 오디오를 직접적으로 생성하거나, 처리하거나, 분석할 수 있는 AudioNode입니다. 이 노드 유형은 폐기되었고, AudioWorkletNode에 의해 대체됩니다. 이 글은 오직 정보를 주는 목적으로 남아있으며 구현이 이 노드 유형을 제거할 때까지 여기 있을 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>ScriptProcessorNode는 반드시 다음의 값 중 하나인 bufferSize와 함께 생성됩니다: 256, 512, 1024, 2048, 4096, 8192, 16384. 이 값은 얼마나 자주 onaudioprocess 이벤트가 전파되고 얼마나 많은 샘플 프레임이 각 호출당 처리될 필요가 있는지를 제어합니다. onaudiorprocess 이벤트는 오직 ScriptProcessorNode가 적어도 한 개의 입력 혹은 한 개의 연결된 출력을 가지고 있을 때만 전파됩니다. bufferSize가 적은 숫자를 가지는 것은 낮은 (더 좋은) 레이턴시의 결과를 낳습니다. 높은 숫자는 오디오 breakup 혹은 glitch를 방지하기 위해 필수적일 것입니다. 이 값은 만약 createScriptProcessor()로의 bufferSize 인자가 전달되지 않았거나, 0으로 설정되었다면 구현에 의해 선택될 것입니다.</p>
        <p>numberOfInputChannels와 numberOfOutputChannels는 입력과 출력 채널의 수를 결정합니다. numberOfInputChannels와 numberOfOutputChannels 모두 0의 값을 가지는 것은 유효하지 않습니다.</p>
        <pre>
[Exposed=Window]
interface ScriptProcessorNode : AudioNode {
  attribute EventHandler onaudioprocess;
  readonly attribute long bufferSize;
};
        </pre>
        <br>
        <h2>1.30. StereoPannerNode 인터페이스</h2>
        <p>이 인터페이스는 low-cost 패닝 알고리즘을 사용하여 스테레오 이미지에서, 들어오는 오디오 스트림을 위치시키는 프로세싱 노드를 나타냅니다. 이 패닝 효과는 스테레오 스트림에서 오디오 컴포넌트를 위치시킴에 있어 일반적입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 스테레오 (2 채널) 이고 증가될 수 없습니다. 더 적거나 더 많은 채널을 가진 노드로부터의 연결은 적절하게 업믹스되거나 다운믹스될 것입니다.</p>
        <p>이 노드의 출력은 스테레오 (2 채널) 로 hard-coded되고 설정될 수 없습니다.</p>
        <pre>
[Exposed=Window]
interface StereoPannerNode : AudioNode {
  constructor (BaseAudioContext context, optional StereoPannerOptions options = {});
  readonly attribute AudioParam pan;
};
        </pre>
        <br>
        <h2>1.31. WaveShaperNode 인터페이스</h2>
        <p>WaveShaperNode는 비선형 왜곡 효과를 구현하는 AudioNode 프로세서입니다.</p>
        <p>비선형 waveshaping 왜곡은 미묘한 비선형 warming 혹은 더욱 분명한 왜곡 효과 둘 다에 보통 사용됩니다. 임의 비선형 shaping curve는 명시될 수 있습니다.</p>
        <br>
        <h2>1.32. AudioWorklet 인터페이스</h2>
        <br>
        <pre>
[Exposed=Window, SecureContext]
interface AudioWorklet : Worklet {
};
        </pre>
        <h3>1.32.1. 개념</h3>
        <p>AudioWorklet 객체는 개발자로 하여금 (JavaScript나 WebAssembly 코드와 같은) 스크립트를 제공하여 렌더링 스레드에서 오디오를 처리할 수 있게 해 주며, 사용자 정의 AudioNode를 지원합니다. 이 프로세싱 메커니즘은 오디오 그래프의 다른 내장 AudioNode와 스크립트 코드의 동기적인 실행을 보장합니다.</p>
        <p>연관된 쌍의 객체는 반드시 정의되어 이 메커니즘을 실현시켜야만 합니다. 연관된 쌍의 객체란 AudioWorkletNode와 AudioWorkletProcessor입니다. AudioWorkletNode (전자) 는 다른 AudioNode 객체와 유사한 메인 글로벌 스코프에 대한 인터페이스를 나타내고 AudioWorkletProcessor (후자) 는 AudioWorkletGlobalScope라는 특별한 스코프 내에서의 내부적인 오디오 처리를 구현합니다.</p>
        <figure>
          <img src="img/audioworklet-concept.png" width="756" height="144">
          <figcaption><strong>도식 16</strong> AudioWorkletNode와 AudioWorkletProcessor</figcaption>
        </figure>
        <p>각 BaseAudioContext는 정확히 하나의 AudioWorklet을 소유합니다.</p>
        <p>AudioWorklet의 worklet 글로벌 스코프 자료형은 AudioWorkletGlobalScope입니다.</p>
        <p>AudioWorklet의 worklet 목적지 유형은 "audioworklet" 입니다.</p>
        <p>스크립트를 addModule(moduleUrl) 메서드를 통해 스크립트를 import하는 것은 AudioWorkletGlobalScope 하에서 AudioWorkletProcessor의 클래스 정의를 등록합니다. imported된 클래스 생성자와 이 생성자로부터 실제 생성된 인스턴스에 대한 두 개의 내부 저장 영역이 있습니다.</p>
        <br>
        <h1>2. 프로세싱 모델</h1>
        <h2>2.1. 배경</h2>
        <p><i>이 섹션은 비규범적입니다.</i></p>
        <p>낮은 레이턴시를 요구하는 실시간 오디오 시스템은 종종 콜백 함수를 사용하여 구현되는데, 여기서 운영 체제는 많은 오디오가 방해받지 않은 채로 재생되기 위해 계산되어야만 할 때 프로그램을 콜백합니다. 그런 콜백은 이상적으로 높은 우선도의 스레드에서 호출됩니다 (종종 시스템의 가장 높은 우선도). 이는 오직 오디오만을 다루는 프로그램은 코드를 이 콜백으로부터 실행해야 함을 의미합니다. 스레드 경계선은 가로지르거나 렌더링 스레드와 콜백 사이에 얼마간의 버퍼링을 추가하는 것은 자연히 레이턴시를 추가할 것이거나 시스템을 글리치에 덜 탄력성 있게 만들 것입니다.</p>
        <p>이러한 이유에서, Web Platform에서의 전통적인 비동기 작업 실행법인 이벤트 루프는 여기서는 작동하지 않습니다. 왜냐하면 이 스레드는 계속해서 실행 중이지 않기 때문입니다. 또한, 많은 불필요하고 잠재적으로 가로막는(blocking) 작업이 전통적인 수행 컨텍스트 (Windows 그리고 Workers)로부터 가능한데, 이는 수용할 만한 퍼포먼스 수준에 도달하기에 바람직한 것이 아닙니다.</p>
        <p>추가적으로, Worker 모델은 스크립트 실행 컨텍스트에 필수적인 작업용 스레드를 생성하는 것을 만드는 반면, 모든 AudioNode는 보통 같은 실행 컨텍스트를 공유합니다.</p>
        <div class="note">
          <p>알림: 이 섹션은 end result가 어떻게 보여야 하는지를 명시하지, 어떻게 이것이 구현되어야 하는지를 명시하는 것이 아닙니다. 특히, 메시지 큐를 사용하는 대신, 구현자는 메모리 연산이 reordered되지 않는 이상 스레드 간에서 공유되는 메모리를 사용할 수 있습니다.</p>
        </div>
        <br>
        <h2>2.2. 컨트롤 스레드와 렌더링 스레드</h2>
        <p>Web Audio API는 반드시 컨트롤 스레드와 렌더링 스레드를 사용하여 구현되어야만 합니다.</p>
        <p><strong>컨트롤 스레드</strong>란 AudioContext가 인스턴스화되고, author가 오디오 그래프를 조작하는 스레드입니다. 즉, BaseAudioContext에서의 작동이 호출되는 곳입니다. <strong>렌더링 스레드</strong>란 실제 오디오 출력이 컨트롤 스레드로부터의 호출에 응답하여 계산되는 스레드입니다. 만약 AudioContext에 대한 오디오 계산이면 이것은 실시간, 콜백 기반의 오디오 스레드일수도 있고, 만약 OfflineAudioContext에 대한 오디오 계산이면 일반 스레드입니다.</p>
        <p>컨트롤 스레드는 [HTML] 에서 기술되었듯이 전통적인 이벤트 루프를 사용합니다.</p>
        <p>렌더링 스레드는 전문화된 렌더링 루프를 사용하는데, 이는 2.4. 오디오 그래프 렌더링하기 섹션에서 기술됩니다.</p>
        <p>컨트롤 스레드로부터 렌더링 스레드로의 소통은 컨트롤 메시지 전달을 사용하여 이루어집니다. 다른 방향으로의 소통은 일반 이벤트 루프 태스크를 사용하여 이루어집니다.</p>
        <p>각 AudioContext는 컨트롤 메시지의 리스트인 하나의 <strong>컨트롤 메시지 큐</strong>를 가지고 있습니다. <strong>컨트롤 메시지</strong>는 렌더링 스레드에서 실행되는 동작입니다.</p>
        <p><strong>컨트롤 메시지를 queuing한다는 것</strong>은 BaseAudioContext의 컨트롤 메시지 큐의 끝에 메시지를 추가한다는 것을 의미합니다.</p>
        <div class="note">
          <p>알림: 예를 들어, AudioBufferSourceNode에서 성공적으로 start()를 호출하는 것은 연관된 BaseAudioContext의 컨트롤 메시지 큐에 컨트롤 메시지를 추가합니다.</p>
        </div>
        <br>
        <p>컨트롤 메시지 큐의 컨트롤 메시지는 삽입 시간에 의해 정렬됩니다. <strong>가장 오래된 메시지</strong>는 따라서 컨트롤 메시지의 큐의 맨 앞에 있는 메시지입니다.</p>
        <p><strong>컨트롤 메시지 큐 Qa를 또 다른 컨트롤 메시지 큐 Qb와 swapping한다는 것</strong>은 다음의 단계를 실행한다는 것을 의미합니다:</p>
        <ol>
          <li>Qc를 새롭고 빈 컨트롤 메시지 큐라고 하자.</li>
          <li>모든 컨트롤 메시지 Qa를 Qc로 옮긴다.</li>
          <li>모든 컨트롤 메시지 Qb를 Qa로 옮긴다.</li>
          <li>모든 컨트롤 메시지 Qc를 Qb로 옮긴다.</li>
        </ol>
        <br>
        <h2>2.3. 비동기적 작동</h2>
        <p>AudioNode에서 메서드를 호출하는 것은 효과적으로 비동기적이며, 반드시 두 과정을 거쳐 이루어져야 합니다. 그것은 동기적인 부분과 비동기적인 부분입니다. 각 메서드에 대해, 실행의 몇 부분은 컨트롤 스레드에서 발생하고 (예를 들어, 유효하지 않는 매개변수의 경우 예외를 던진다든지), 몇 부분은 렌더링 스레드에서 발생합니다 (예를 들자면, AudioParam의 값을 변경하는 것).</p>
        <p>AudioNode와 BaseAudioContext의 각 동작의 설명에서, 동기적 부분은 ⌛로 마킹되어 있습니다. 모든 다른 동작은 병렬적으로 수행되는데, 이는 [HTML]에 기술되어 있습니다.</p>
        <p>동기적 부분은 컨트롤 스레드에서 수행되고, 즉시 발생합니다. 만약 이것이 실패하면, 메서드 수행은 중단되고, 아마 예외를 발생시킬 것입니다. 만약 이것이 성공하면, 렌더링 스레드에서 실행될 작동을 부호화하는 컨트롤 메시지가 이 렌더링 스레드의 컨트롤 메시지 큐에 enque될 것입니다.</p>
        <p>다른 이벤트들과 관련된 동기적 그리고 비동기적 부분 순서는 반드시 같아야만 합니다. 각각 동기적이고 비동기적인 섹션 A<sub>Sync</sub>와 A<sub>Async</sub> 그리고 B<sub>Sync</sub>와 B<sub>Async</sub>인 두 작동 A와 B가 주어졌을 때, 만약 A가 B보다 먼저 발생한다면, A<sub>Sync</sub>는 B<sub>Sync</sub>보다 먼저 발생하고, A<sub>Async</sub>가 B<sub>Async</sub>보다 먼저 발생합니다. 다른 말로 하자면, 동기적이고 비동기적인 부분은 reorder될 수 없습니다.</p>
        <br>
        <h2>2.4. 오디오 그래프 렌더링하기</h2>
        <p>오디오 그래프를 렌더링한다는 것은 128 샘플 프레임의 블럭 내에서 이루어집니다. 128 샘플 프레임의 블럭은 <strong>렌더 퀀텀</strong>(render quantum)이라고 불리며, <strong>렌더 퀀텀 크기</strong>(~ size) 는 128입니다.</p>
        <p>주어진 스레드에서 <strong>atomically</strong>하게 발생하는 작동은 다른 atomic한 작동이 또 다른 스레드에서 작동하고 있지 않을때에만 오직 수행 가능합니다.</p>
        <p>BaseAudioContext에서 오디오 블럭을 렌더링하는 알고리즘 G 그리고 컨트롤 메시지 큐 Q는 다수의 단계로 구성되며 추가적인 디테일은 그래프 렌더링의 알고리즘에서 설명됩니다.</p>
        <div class="note">
          <p>실제로는, AudioContext 렌더링 스레드는 종종 시스템 수준의 오디오 콜백을 run off하는데, 이는 등시적인(isochronous) 방식으로 수행됩니다.</p>
          <p>OfflineAudioContext는 시스템 수준의 오디오 콜백을 가질 것이 요구되지 않으나, 이전 콜백이 종료되자마자 발생하는 콜백과 함께 이것이 그러한 콜백을 가지고 있는 것 처럼 행동합니다.</p>
        </div>
        <br>
        <p>오디오 콜백은 또한 컨트롤 메시지 큐에서 태스크로서 queue됩니다. UA는 반드시 다음의 알고리즘을 수행하여 렌더 퀀텀들을 처리해 요구된 버퍼 사이즈를 채움으로써 그러한 태스크를 이행해야만 합니다. 컨트롤 메시지 큐와 함게, 각 AudioContext는 일반적인 태스크 큐를 가지고 있으며, 컨트롤 스레드로부터 렌더링 스레드에 post된 태스크들에 대해 이것의 <strong>연관된 태스크 큐</strong>를 호출합니다. 추가적인 마이크로 태스크 체크 포인트는 렌더 퀀텀을 처리한 후에 수행되어 AudioWorkletProcessor의 process 메서드의 실행 중에 queue되었을 지도 모르는 어떠한 마이크로태스크들을 실행합니다.</p>
        <p>AudioWorkletNode로부터 post된 모든 태스크들은 이것의 연관된 BaseAudioContext의 연관된 태스크 큐에 post됩니다.</p>
        <p>다음의 과정은 렌더링 루프가 시작되기 전에 반드시 한 번 수행되어야만 합니다.</p>
        <p>과정은 추후에 번역</p>
        <p>AudioNode를 <strong>mute</strong>한다는 것은 이것의 출력이 반드시 이 오디오 블럭의 렌더링에 대해 무음을 출력해야만 한다는 것을 의미합니다.</p>
        <p><strong>AudioNode로부터 읽는 것에 대해 버퍼를 사용 가능하게 만든다는 것</strong>은 이 AudioNode에 연결된 다른 AudioNode들이 이것으로부터 안전하게 읽을 수 있는 상태에 이것을 놓는다는 것을 의미합니다.</p>
        <div class="note">
          <p>참고: 예를 들어, 구현은 새로운 버퍼를 할당하거나, 더욱 정교한 메커니즘을 가지는 것을 선택할 수 있습니다. 그 메커니즘이란 지금은 사용되지 않는 존재하는 버퍼를 재사용하는 것입니다.</p>
        </div>
        <br>
        <p><strong>AudioNode의 입력을 record한다는 것</strong>은 이 AudioNode의 입력 데이터를 미래의 사용을 위해 복사한다는 것을 의미합니다.</p>
        <p><strong>오디오 블럭을 계산한다는 것</strong>은 이 AudioNode에 대해 알고리즘을 실행하여 128 샘플 프레임을 생성하는 것을 의미합니다.</p>
        <p><strong>입력 버퍼를 처리한다는 것</strong>은 AudioNode에 대해 알고리즘을 실행하는 것을 의미하는데, 입력 버퍼와 이 AudioNode의 AudioParam(들)의 값(들)을 이 알고리즘에 대한 입력으로서 사용합니다.</p>
        <br>
        <h2>2.5. 문서 unload하기</h2>
        <p>추가적인 document 언로딩 cleanup 단계가 BaseAudioContext를 사용하는 문서에 대해 정의됩니다:</p>
        <ul>
          <li>관련 있는 global object가 document의 연관된 Window와 같은 각 AudioContext와 OfflineAudioContext에 대해 InvalidStateError와 함께 [[pending promises]]의 모든 프로미스를 reject시킵니다.</li>
          <li>모든 decoding 스레드를 정지시킵니다.</li>
          <li>AudioContext 또는 OfflineAudioContext를 close()하기 위한 컨트롤 메시지를 queue시킵니다.</li>
        </ul>
        <br>
        <h1>3. 동적 생명 주기</h1>
        <h2>3.1. 배경</h2>
        <div class="note">
          <p>알림: AudioContext와 AudioNode 생명 주기 특성에 대한 규범적인 설명은 AudioContext 생명주기와 AudioNode 생명주기에서 설명되고 있습니다.</p>
        </div>
        <br>
        <p><i>이 섹션은 비규범적입니다.</i></p>
        <p>정적 라우팅 설정의 생성을 허용함에 추가적으로, 제한된 생명 주기를 가지는 동적으로 할당된 소리에 사용자 정의 효과 라우팅을 적용하는 것 또한 가능합니다. 이 논의의 목적을 위해, 이 짧게 사는 소리를 "노트" (note) 라고 부릅시다. 많은 오디오 어플리케이션들은 노트의 개념을 포함하며, 그 예로는 드럼 머신, 시퀸서, 게임 플레이에 따라 발동되는 많은 한 번의 (one-shot) 소리를 가진 3D 게임을 들 수 있습니다.</p>
        <p>전통적인 소프트웨어 신디사이저에서는, 노트는 동적으로 할당되고 사용 가능한 자원의 풀(pool)에서 해제되었습니다. 노트는 MIDI note-on 메시지가 받아졌을 때 할당됩니다. 노트가 샘플 데이터의 끝에 도달했거나 (만약 반복되지 않는다면), 엔벨로프의 서스테인 단계에 도달했는데 서스테인이 0이거나, 혹은 엔벨로프의 릴리즈 단계로 가게 하는 MIDI note-off 메시지에 기인해 노트가 재생을 마쳤을 때 해제됩니다. MIDI note-off 경우에서, 노트는 바로 해제되지 않고, 오직 릴리즈 엔벨로프 단계가 마쳤을 때에만 해제됩니다. 어떠한 주어진 시간에서라도, 다수의 재생되는 노트가 있을 수 있으나 노트의 집합은 계속 바뀝니다. 왜냐하면 새로운 노트들이 라우팅 그래프에 추가되고, 오래된 것들은 해제되기 때문입니다.</p>
        <p>오디오 시스템은 자동적으로 개개의 "노트" 이벤트에 대한 라우팅 그래프의 부분을 해체(tear down)하는 것을 다룹니다. "노트" 는 AudioBufferSourceNode에 의해 표현되는데, 이는 직접적으로 다른 프로세싱 노드에 연결될 수 있습니다. 노트가 재생을 마쳤을 때, 컨텍스트는 자동적으로 이 AudioBufferSourceNode에 대한 참조를 해제하는데, 이는 차례로 이것이 연결된 모든 노드들에 대한 참조를 해제합니다. 노드들은 자동적으로 그래프에서 연결 해제되고 더 이상 참조를 가지고 있지 않을 때 삭제될 것입니다. 동적인 소리들 사이에서 공유되고 오래 사는 그래프 상의 노드들은 명시적으로 관리될 수 있습니다. 비록 이것이 복잡하게 들리긴 하지만, 추가적인 요구되는 처리 없이 이 모든 것은 자동적으로 발생합니다.</p>
        <br>
        <h2>3.2. 예제</h2>
        <figure>
          <img src="img/dynamic-allocation.png" width="671" height="221">
          <figcaption><strong>도식 18</strong> 이르게 해제(release)될 서브그래프를 나타내는 그래프</figcaption>
        </figure>
        <p>위의 로우패스필터, 패너, 두번째 게인 노드는 직접적으로 한 번의(one-shot) 소리로부터 연결되어 있습니다. 따라서 이 소리가 재생을 마쳤을 때 컨텍스트는 자동적으로 이것들을 해제할 것입니다 (점선 내에 있는 모든 것). 만약 이 소리와 연결된 노드들에 대한 어떠한 참조도 더 이상 없다면, 이것들은 즉시 그래프로부터 해제되고 삭제됩니다. 스트리밍 소스는 전역 참조를 가지고 있고 이것이 명시적으로 연결 해제되기 전까지는 연결이 유지될 것입니다. 여기 이것이 JavaScript에서 어떻게 보일지에 대한 코드입니다.</p>
        <pre>예제 20
let context = 0;
let compressor = 0;
let gainNode1 = 0;
let streamingAudioSource = 0;
// 라우팅 그래프의 "길게 사는" 부분에 대한 초기 설정
function setupAudioContext() {
    context = new AudioContext();
    compressor = context.createDynamicsCompressor();
    gainNode1 = context.createGain();
    // 스트리밍 오디오 소스를 생성합니다.
    const audioElement = document.getElementById('audioTagID');
    streamingAudioSource = context.createMediaElementSource(audioElement);
    streamingAudioSource.connect(gainNode1);
    gainNode1.connect(compressor);
    compressor.connect(context.destination);
}
// 나중에 유저 액션에 응답하여 (보통 마우스나 키 이벤트)
// 한 번의 소리가 재생될 수 있습니다.
function playSound() {
    const oneShotSound = context.createBufferSource();
    oneShotSound.buffer = dogBarkingBuffer;
    // 필터, 패너, 게인 노드를 생성합니다.
    const lowpass = context.createBiquadFilter();
    const panner = context.createPanner();
    const gainNode2 = context.createGain();
    // 연결을 만듭니다
    oneShotSound.connect(lowpass);
    lowpass.connect(panner);
    panner.connect(gainNode2);
    gainNode2.connect(compressor);
    // 지금으로부터 0.75초 후에 재생합니다 (즉시 재생하려면 0을 전달하십시오)
    oneShotSound.start(context.currentTime + 0.75);
}
        </pre>
        <br>
        <h1>4. 채널 업믹싱과 다운믹싱</h1>
        <p><i>이 섹션은 규범적입니다.</i></p>
        <p>AudioNode 입력은 자신에게 연결된 모든 입력들로부터 채널들을 결합하는 믹싱 규칙을 가지고 있습니다. 단순한 예제로, 만약 입력이 모노 출력과 스테레오 출력으로부터 연결되어 있다면 보통 모노 입력이 스테레오로 업믹스되고 스테레오 연결로 합쳐질 것입니다. 하지만, 물론, 모든 입력에 대해 모든 AudioNode에 정확한 믹싱 규칙을 정의하는 것은 중요합니다. 모든 입력에 대한 기본 믹싱 규칙은 선택되어 있어 일들은 "그냥 작동" 합니다. 세부 사항에 대해 너무 걱정할 필요 없이 말이죠. 특히 모노와 스테레오 스트림의 아주 일반적인 경우에서 말입니다. 물론, 특히 멀티 채널처럼, 규칙은 고급의 사용 경우에 대해 변경될 수 있습니다.</p>
        <br>
        <p>몇가지 용어를 정의하자면, 업믹싱이란 더 적은 채널의 수를 가진 스트림을 취하고 그것을 더 큰 수의 채널을 가진 스트림으로 변환하는 과정을 의미합니다. 다운믹싱이란 더 큰 채널의 수를 가진 스트림을 취하고 그것을 더 적은 수의 채널을 가진 스트림으로 변환하는 과정을 의미합니다.</p>
        <br>
        <p>AudioNode 입력은 이 입력에 연결된 모든 출력을 믹스할 필요가 있습니다. 이 과정의 일부로 이것은 내부 값인 computedNumberOfChannels를 계산하는데 이는 주어진 시간에서의 입력 채널의 실제 수를 나타냅니다.</p>
        <p>AudioNode의 각 입력에 대해, 구현은 반드시 아래를 수행해야 합니다:</p>
        <ol>
          <li>computedNumberOfChannels를 계산합니다.</li>
          <li>입력의 각 연결에 대해서:
            <ol>
              <li>노드의 channelInterpretation 어트리뷰트에 의해 주어진 ChannelInterpretation 값에 따른 computedNumberOfChannels에의 연결을 업믹스 또는 다운믹스합니다.</li>
              <li>(다른 연결로부터의) 모든 다른 믹스된 스트림과 함께 모두 믹스합니다. 이것은 각 연결에 대해 윗 단계에서 업믹스나 다운믹스된 해당하는 각 채널을 함께 간단하게 합치는 것입니다.</li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>4.1. 스피커 채널 레이아웃</h2>
        <p>channelInterpretation이 "speakers" 라면, 특정한 채널 레이아웃에 대해 업믹싱과 다운믹싱이 정의됩니다.</p>
        <p>모노 (1채널), 스테레오 (2채널), 쿼드 (4채널), 5.1 (6채널) 이 반드시 지원되어야만 합니다. 다른 채널 레이아웃은 이 명세의 미래 버전에서 지원될 수도 있습니다.</p>
        <br>
        <h2>4.2. 채널 순서</h2>
        <p>채널 순서는 다음 표에 의해 정의된다. 개개의 다채널 포맷은 모든 중간 채널들을 지원하지 않을 수 있다. 구현은 반드시 아래에 정의된 순서로 제공된 채널들을 나타내야만 하며, 나타나지 않은 채널은 생략한다.</p>
        <table>
          <thead>
            <tr>
              <td>순서</td>
              <td>라벨</td>
              <td>모노</td>
              <td>스테레오</td>
              <td>쿼드</td>
              <td>5.1</td>
            </tr>
            <tr>
              <td>0</td>
              <td>SPEAKER_FRONT_LEFT</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>SPEAKER_FRONT_RIGHT</td>
              <td></td>
              <td>1</td>
              <td>1</td>
              <td>1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>SPEAKER_FRONT_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td>2</td>
            </tr>
            <tr>
              <td>3</td>
              <td>SPEAKER_LOW_FREQUENCY</td>
              <td></td>
              <td></td>
              <td></td>
              <td>3</td>
            </tr>
            <tr>
              <td>4</td>
              <td>SPEAKER_BACK_LEFT</td>
              <td></td>
              <td></td>
              <td>2</td>
              <td>4</td>
            </tr>
            <tr>
              <td>5</td>
              <td>SPEAKER_BACK_RIGHT</td>
              <td></td>
              <td></td>
              <td>3</td>
              <td>5</td>
            </tr>
            <tr>
              <td>6</td>
              <td>SPEAKER_FRONT_LEFT_OF_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>7</td>
              <td>SPEAKER_FRONT_RIGHT_OF_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>8</td>
              <td>SPEAKER_BACK_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>9</td>
              <td>SPEAKER_SIDE_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>10</td>
              <td>SPEAKER_SIDE_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>11</td>
              <td>SPEAKER_TOP_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>12</td>
              <td>SPEAKER_TOP_FRONT_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>13</td>
              <td>SPEAKER_TOP_FRONT_CENTER	</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>14</td>
              <td>SPEAKER_TOP_FRONT_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>15</td>
              <td>SPEAKER_TOP_BACK_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>16</td>
              <td>SPEAKER_TOP_BACK_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>17</td>
              <td>SPEAKER_TOP_BACK_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
        </table>
        <br>
        <h2>4.3. 입력과 출력 채널 카운트의 꼬리 시간의 구현</h2>
        <p>AudioNode가 0이 아닌 꼬리 시간을 가지고, 입력 채널 카운트에 좌우되는 출력 채널 카운트를 가졌을 때, 입력 채널 카운트가 변화할 때 AudioNode의 꼬리 시간은 반드시 고려되어져야만 합니다.</p>
        <p>입력 채널 카운트에 감소가 있을 때, 더 큰 채널 카운트와 함께 받아진 입력이 더 이상 출력에 영향을 주지 않을 때 출력 채널 카운트에서의 변화는 반드시 발생해야만 합니다.</p>
        <p>입력 채널 카운트에서의 증가가 있을 때, 동작은 AudioNode 유형에 따라 다릅니다:</p>
        <ul>
          <li>DelayNode나 DynamicsCompressorNode에 대해서, 출력 채널의 수는 더 큰 채널 카운트와 함께 받아진 입력이 출력에 영향을 주기 시작했을 때 반드시 증가해야만 합니다.</li>
          <li>꼬리 시간을 가진 다른 AudioNode에 대해서, 출력 채널의 수는 반드시 즉시 증가해야만 합니다.
            <div class="note">
              <p>참고: ConvolverNode에 대해서, 이것은 오직 임펄스 응답이 모노인 경우에만 적용됩니다. 그렇지 않으면, ConvolverNode는 자신의 입력 채널 카운트에 관계없이 항상 스테레오 신호를 출력합니다.</p>
            </div>
          </li>
        </ul>
        <br>
        <div class="note">
          <p>직감적으로, 이것은 프로세싱의 부분으로서 스테레오 정보를 잃지 않게 해 줍니다: 다른 채널 카운트의 다수의 입력 render quantum들이 출력 render quantum에 공헌할 때 출력 render quantum의 채널 카운트는 입력 render quantum들의 입력 채널 카운트의 상위 집합입니다.</p>
        </div>
        <br>
        <h2>4.4. 업믹싱 스피커 레이아웃</h2>
        <pre>

모노 업믹스:

  1 -> 2 : 모노에서 스테레오로 업믹스
    output.L = input;
    output.R = input;

  1 -> 4 : 모노에서 쿼드로 업믹스
  output.L = input;
  output.R = input;
  output.SL = 0;
  output.SR = 0;

  1 -> 5.1 : 모노에서 5.1로 업믹스
    output.L = 0;
    output.R = 0;
    output.C = input; // put in center channel
    output.LFE = 0;
    output.SL = 0;
    output.SR = 0;

스테레오 업믹스:

  2 -> 4 : 스테레오에서 쿼드로 업믹스
    output.L = input.L;
    output.R = input.R;
    output.SL = 0;
    output.SR = 0;

  2 -> 5.1 : 스테레오에서 5.1로 업믹스
    output.L = input.L;
    output.R = input.R;
    output.C = 0;
    output.LFE = 0;
    output.SL = 0;
    output.SR = 0;

쿼드 업믹스:

4 -> 5.1 : 쿼드에서 5.1로 업믹스
  output.L = input.L;
  output.R = input.R;
  output.C = 0;
  output.LFE = 0;
  output.SL = input.SL;
  output.SR = input.SR;
        </pre>
        <br>
        <h2>4.5. 다운믹싱 스피커 레이아웃</h2>
        <p>예를 들어, 5.1 소스 material을 프로세싱했지만, 스테레오로 재생할 때 다운믹스가 필요할 것입니다.</p>
        <pre>

모노 다운믹스:

  2 -> 1 : 스테레오에서 모노로
    output = 0.5 * (input.L + input.R);

  4 -> 1 : 쿼드에서 모노로
    output = 0.25 * (input.L + input.R + input.SL + input.SR);

  5.1 -> 1 : 5.1에서 모노로
    output = sqrt(0.5) * (input.L + input.R) + input.C + 0.5 * (input.SL + input.SR)

스테레오 다운믹스:

  4 -> 2 : 쿼드에서 스테레오로
    output.L = 0.5 * (input.L + input.SL);
    output.R = 0.5 * (input.R + input.SR);

  5.1 -> 2 : 5.1에서 스테레오로
    output.L = L + sqrt(0.5) * (input.C + input.SL)
    output.R = R + sqrt(0.5) * (input.C + input.SR)

쿼드 다운믹스:

  5.1 -> 4 : 5.1에서 쿼드로
    output.L = L + sqrt(0.5) * input.C
    output.R = R + sqrt(0.5) * input.C
    output.SL = input.SL
    output.SR = input.SR
        </pre>
        <br>
        <h2>4.6. 채널 규칙 예제</h2>
        <pre>

// gain 노드를 explicit 2채널 (스테레오) 로 설정합니다.
gain.channelCount = 2;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";
// 2개의 스테레오 출력 버스를 가진 DJ 앱을 위해 "하드웨어 출력"을 4채널로 설정합니다.
context.destination.channelCount = 4;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";
// 사용자 정의 다채널 스피커 배열을 위해 "하드웨어 출력"을 8채널로 설정합니다,
// 사용자 정의 매트릭스 믹싱과 함께.
context.destination.channelCount = 8;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";
// HTMLAudioElement를 재생하기 위해 "하드웨어 출력"을 5.1로 설정합니다.
context.destination.channelCount = 6;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "speakers";
// 명시적으로 모노로 다운믹스합니다.
gain.channelCount = 1;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";
        </pre>
        <br>
        <h1>5. 오디오 신호 값</h1>
        <h2>5.1. 오디오 샘플 포맷</h2>
        <p><strong>선형 펄스 부호 변조</strong>(linear pulse code modulation; linear PCM) 은 하나의 포맷인데, 이 포맷에서는 오디오 값들이 정기적인 구간에서 샘플링되며, 두 개의 연이은 값 사이의 양자화 레벨이 선형적으로 균일합니다.</p>
        <p>이 명세에서 신호 값들이 스크립트에 노출될 때는 언제든지, 그 값들은 선형 32비트 부동 소수점 펄스 부호 변조 포맷 (선형 32비트 부동 소수점 PCM) 이며, 종종 Float32Array 객체의 형태로 존재합니다.</p>
        <br>
        <h2>5.2. 렌더링</h2>
        <p>무슨 오디오 그래프에서든지 목적지 노드에서의 모든 오디오 신호의 범위는 명목상(공칭상) [1, 1]입니다. 이 범위 바깥, NaN값, 양의 무한, 음의 무한의 신호 값의 오디오 렌더링은 이 명세에 의해 정의되지 않습니다.</p>
        <br>
        <h1>6. 공간화/패닝</h1>
        <h2>6.1. 배경</h2>
        <p>현대 3D 게임에서의 일반적인 기능 요구는 3D 공간에서의 다수의 오디오 소스의 동적 공간화와 이동입니다. 예를 들자면 OpenAL이 이러한 기능을 가지고 있습니다.</p>
        <p>PannerNode를 사용하여, 오디오 스트림은 AudioListener에 관해 공간화되거나 공간에서 위치가 지정될 수 있습니다. BaseAudioContext는 하나의 AudioListener를 가지고 있습니다. 패너와 리스너 둘 다 오른손 카르테시안 좌표계를 사용하여 3D 공간에서의 위치를 가집니다. 좌표계에서 사용되는 단위는 정의되지 않았고, 그럴 필요도 없는 것이 이 좌표들과 함께 계산되는 효과는 미터나 피트와 같은 특정한 단위에 독립적이거나 변함이 없기 때문입니다. PannerNode 객체 (소스 스트림을 나타냄) 는 소리가 어떤 방향을 비추고 있는지를 나타내는 방향 벡터를 가지고 있습니다. 추가적으로, PannerNode는 소리가 얼마나 방향성인지를 나타내는 sound cone을 가지고 있습니다. 예를 들어, 소리가 전방향이어서 방향에 관계없이 어디서든 들릴 수 있고, 또는 소리가 지향성이라 소리가 리스너를 바라볼 때에만 들릴 수 있습니다. AudioListener 객체 (사람의 귀를 나타냄) 은 앞과 위 벡터를 가지고 있고 이 벡터들은 사람이 바라보고 있는 방향을 나타냅니다.</p>
        <p>아래의 도식은 공간화에서의 좌표계와 기본값을 보여줍니다. AudioListener와 PannerNode의 위치는 우리가 상황을 더 잘 파악할 수 있도록 기본 위치에서 이동되어 있습니다.</p>
        <figure>
          <img src="img/panner-coord.svg">
          <figcaption><strong>도식 19</strong> AudioListener와 PannerNode 어트리뷰트가 보이는 좌표계를 나타내는 도식</figcaption>
        </figure>
        <p>렌더링 중에, PannerNode는 방위각(azimuth)과 고도(elevation)를 계산합니다. 이 값들은 구현에 의해 내부적으로 사용되어 공간화 효과를 렌더링합니다. 이 값들이 어떻게 쓰이는지에 대한 자세한 사항에 대해서는 Panning Algorithm 섹션을 참고하세요.</p>
        <br>
        <h2>6.2. 방위각과 고도</h2>
        <p>다음의 알고리즘은 PannerNode에서 방위각과 고도를 계산하기 위해 반드시 사용되어야만 합니다. 구현은 반드시 적절하게 아래의 다양한 AudioParam들이 "a-rate" 인지 "k-rate" 인지 고려해야만 합니다.</p>
        <pre>

// |context|를 BaseAudioContext, |panner|를
// |context|내에서 생성된 PannerNode라고 합시다
// 소스-리스너 벡터를 계산합니다.
const listener = context.listener;
const sourcePosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                panner.positionZ.value);
const listenerPosition =
    new Vec3(listener.positionX.value, listener.positionY.value,
              listener.positionZ.value);
const sourceListener = sourcePosition.diff(listenerPosition).normalize();
if (sourceListener.magnitude == 0) {
  // 소스와 리스너가 같은 지점에 있는 degenerate 경우를 다룹니다.
  azimuth = 0;
  elevation = 0;
  return;
}
// 축들을 정렬합니다.
const listenerForward = new Vec3(listener.forwardX.value, listener.forwardY.value,
                                  listener.forwardZ.value);
const listenerUp =
    new Vec3(listener.upX.value, listener.upY.value, listener.upZ.value);
const listenerRight = listenerForward.cross(listenerUp);
if (listenerRight.magnitude == 0) {
  // 리스너의 '위' 와 '앞' 벡터가 선형적으로 독립적이어서,
  // '오른쪽' 이 결정될 수 없는 경우를 다룹니다
  azimuth = 0;
  elevation = 0;
  return;
}
// 리스너의 오른쪽, 앞에 직교하는 단위벡터를 결정합니다
const listenerRightNorm = listenerRight.normalize();
const listenerForwardNorm = listenerForward.normalize();
const up = listenerRightNorm.cross(listenerForwardNorm);
const upProjection = sourceListener.dot(up);
const projectedSource = sourceListener.diff(up.scale(upProjection)).normalize();
azimuth = 180 * Math.acos(projectedSource.dot(listenerRightNorm)) / Math.PI;
// 리스너의 앞 또는 뒤의 소스.
const frontBack = projectedSource.dot(listenerForwardNorm);
if (frontBack &lt; 0)
  azimuth = 360 - azimuth;
// 방위각을 "오른쪽" 리스너 벡터가 아니라 "앞" 벡터에 관계되게 만듭니다.
if ((azimuth >= 0) && (azimuth &lt;= 270))
  azimuth = 90 - azimuth;
else
  azimuth = 450 - azimuth;
elevation = 90 - 180 * Math.acos(sourceListener.dot(up)) / Math.PI;
if (elevation > 90)
  elevation = 180 - elevation;
else if (elevation &lt; -90)
  elevation = -180 - elevation;
        </pre>
        <br>
        <h2>6.3. 패닝 알고리즘</h2>
        <p>모노→스테레오 그리고 스테레오→스테레오 패닝이 반드시 지원되어야만 합니다. 모노→스테레오 프로세싱은 입력에의 모든 연결이 모노일 때 사용됩니다. 그렇지 않으면 스테레오→스테레오 프로세싱이 사용됩니다.</p>
        <h3>6.3.1. PannerNode "equalpower" 패닝</h3>
        <p>이것은 기본적이나, 합리적인 결과를 제공하는 단순하고 비교적 저렴한 알고리즘입니다. 이것은 panningModel 어트리뷰트가 "equalpower" 로 설정되어 있어, 고도 값이 무시되는 상황에 PannerNode가 사용합니다. 이 알고리즘은 automationRate에 의해 명시된 것과 같은 적절한 비율을 사용해 구현되어야만 합니다. 만약 PannerNode의 AudioParam이나 AudioListener의 AudioParam 중 하나라도 "a-rate"라면, a-rate 프로세싱이 반드시 사용되어야만 합니다.</p>
        <ol>
          <li>이 AudioNode에 의해 계산될 각 샘플에 대해:
            <ol>
              <li>azimuth가 방위각과 고도 섹션에서 계산된 값이라고 합시다.</li>
              <li>방위각 값은 우선 다음에 따라 범위 [-90, 90] 내에 포함됩니다:
                <pre>
// 우선, azimuth를 [-180, 180]의 허용된 범위 내로 조정합니다.
azimuth = max(-180, azimuth);
azimuth = min(180, azimuth);

// 그리고 나서 범위 [-90, 90] 으로 조정합니다.
if (azimuth < -90)
  azimuth = -180 - azimuth;
else if (azimuth > 90)
  azimuth = 180 - azimuth;
                </pre>
              </li>
              <li>정규화된 값 x는 모노 입력에 대해 azimuth로부터 다음과 같이 계산됩니다:
                <pre>
x = (azimuth + 90) / 180;
                </pre>
                또는 스테레오 입력에 대해서 다음과 같이 계산됩니다:
                <pre>
if (azimuth &lt;= 0) { // -90 -> 0
  // [-90, 0] 각도 내의 azimuth 값을 [-90, 90] 범위 내로 변환합니다.
  x = (azimuth + 90) / 90;
} else { // 0 -> 90
  // [0, 90] 각도 내의 azimuth 값을 [-90, 90] 범위 내로 변환합니다.
  x = azimuth / 90;
}
                </pre>
              </li>
              <li>좌측과 우측 gain 값은 다음과 같이 계산됩니다:
                <pre>
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
                </pre>
              </li>
              <li>모노 입력에 대해, 스테레오 출력은 다음과 같이 계산됩니다:
                <pre>
outputL = input * gainL;
outputR = input * gainR;
                </pre>
                또는 스테레오 입력에 대해, 출력은 다음과 같이 계산됩니다:
                <pre>
if (azimuth <= 0) {
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
                </pre>
              </li>
              <li>거리의 계산이 6.4. 거리 효과에서 기술되었고 cone gain이 6.5. 소리 cone에서 기술된 거리 gain과 cone gain을 적용합니다.
                <pre>
let distance = distance();
let distanceGain = distanceModel(distance);
let totalGain = coneGain() * distanceGain();
outputL = totalGain * outputL;
outputR = totalGain * outputR;
                </pre>
              </li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>6.3.2. PannerNode "HRTF" 패닝 (스테레오만 해당)</h2>
        <p>이것은 다양한 방위각과 고도에서 녹음된 HRTF (Head-related Transfer Function) 임펄스 응답의 집합을 요구합니다. 구현은 대단히 최적화된 컨볼루션 함수를 요구합니다. 이것은 "equalpower" 보다 다소 비싸지만, 인지적으로 더욱 공간화된 소리를 제공합니다.</p>
        <figure>
          <image src="img/HRTF_panner.png" width="644" height="419"></image>
          <figcaption><strong>도식 20</strong> HRTF를 사용하여 소스를 패닝하는 과정을 보여주는 도식</figcaption>
        </figure>
        <br>
        <h2>6.3.3. StereoPannerNode 패닝</h2>
        <p>StereoPannerNode에 대해서, 다음의 알고리즘이 반드시 구현되어야만 합니다.</p>
        <ol>
          <li>이 AudioNode에 의해 계산될 각 샘플에 대해서
            <ol>
              <li>이 StereoPannerNode의 pan AudioParam의 computedValue를 pan이라고 합시다.</li>
              <li>pan을 [-1, 1] 로 조정합니다.
                <pre>
pan = max(-1, pan);
pan = min(1, pan);
                </pre>
              </li>
              <li>pan 값을 [0, 1] 로 정규화함으로써 x를 계산합니다. 모노 입력에 대해:
                <pre>
x = (pan + 1) / 2;
                </pre>
                스테레오 입력에 대해:
                <pre>
if (pan &lt;= 0)
  x = pan + 1;
else
  x = pan;
                </pre>
              </li>
              <li>좌측과 우측 gain 값은 다음과 같이 계산됩니다:
                <pre>
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
                </pre>
              </li>
              <li>모노 입력에 대해, 스테레오 출력은 다음과 같이 계산됩니다:
                <pre>
outputL = input * gainL;
outputR = input * gainR;
                </pre>
                혹은 스테레오 입력에 대해, 출력은 다음과 같이 계산됩니다.
                <pre>
if (pan &lt;= 0) {
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
                </pre>
              </li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>6.4. 거리 효과</h2>
        <p>가까운 소리는 더 큰 반면, 멀리 떨어진 소리는 조용합니다. 소리의 음량이 리스너로부터의 거리에 따라 바뀌는 방법은 정확히 distanceModel 어트리뷰트에 달려 있습니다.</p>
        <p>오디오 렌더링 동안, 거리 값은 다음에 따른 패너와 리스너 위치에 기반해 계산될 것입니다.</p>
        <pre>
function distance(panner) {
  const pannerPosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                  panner.positionZ.value);
  const listener = context.listener;
  const listenerPosition =
      new Vec3(listener.positionX.value, listener.positionY.value,
               listener.positionZ.value);
  return pannerPosition.diff(listenerPosition).magnitude;
}
        </pre>
        <p>거리는 그리고 나서 distanceGain을 계산하기 위해 사용되는데 distanceGain은 distanceModel 어트리뷰트에 좌우됩니다. 각 거리 모델이 계산되는 방법에 대한 자세한 자상은 DistanceModelType 섹션을 참고하세요.</p>
        <p>이것의 프로세싱의 일부로서, PannerNode는 입력 오디오 신호를 distanceGain만큼 크기를 조정/곱해서 멀리 있는 소리를 조용하게 하고 가까운 소리를 크게 만듭니다.</p>
        <br>
        <h2>6.5. 소리 cone</h2>
        <p>리스너와 각 음원은 어떤 방향으로 자신이 향하고 있는지를 기술하는 방향 벡터를 가지고 있습니다. 각 음원의 소리 사영 특징은 음원의 방향 벡터로부터의 음원/리스너 각도의 함수로서 소리 강도를 기술하는 안 그리고 바깥 "cone"에 의해 기술됩니다. 따라서, 리스너를 직접적으로 가리키는 음원은 off-axis를 가리킬 때보다 더 큽니다. 음원은 또한 전방향일 수 있습니다.</p>
        <p>다음의 도식은 리스너에 대한 음원의 cone 사이의 관계를 나타냅니다. 이 도식에서, coneInnerAngle = 50이고 coneOuterAngle = 120입니다. 즉, 내부 cone은 방향 벡터의 각 측면에서 25도 확장됩니다. 유사하게, 외부 cone은 각 측면에서 60도 확장됩니다.</p>
        <figure>
          <image src="img/cone-diagram.svg" width="50%"></image>
          <figcaption><strong>도식 21</strong> 음원 방향과 리스너의 위치와 방향에 관한 음원의 cone 각도</figcaption>
        </figure>
        <p>음원 (PannerNode) 과 리스너가 주어졌을 때 cone 효과에 기인한 gain 기여를 계산하기 위해 다음의 알고리즘이 반드시 사용되어야만 합니다.</p>
        <pre>
function coneGain() {
  const sourceOrientation =
      new Vec3(source.orientationX, source.orientationY, source.orientationZ);
  if (sourceOrientation.magnitude == 0 ||
      ((source.coneInnerAngle == 360) && (source.coneOuterAngle == 360)))
    return 1; // cone이 명시되지 않음 - unity gain
  // 정규화된 음원-리스너 벡터
  const sourcePosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                  panner.positionZ.value);
  const listenerPosition =
      new Vec3(listener.positionX.value, listener.positionY.value,
                listener.positionZ.value);
  const sourceToListener = sourcePosition.diff(listenerPosition).normalize();
  const normalizedSourceOrientation = sourceOrientation.normalize();
  // 음원 방향 벡터와 음원-리스너 벡터 사이의 각도
  const angle = 180 *
                Math.acos(sourceToListener.dot(normalizedSourceOrientation)) /
                Math.PI;
  const absAngle = Math.abs(angle);
  // API는 전체 각도이므로 (반각이 아님) 여기서 2로 나눔
  const absInnerAngle = Math.abs(source.coneInnerAngle) / 2;
  const absOuterAngle = Math.abs(source.coneOuterAngle) / 2;
  let gain = 1;
  if (absAngle &lt;= absInnerAngle) {
    // 감쇠 없음
    gain = 1;
  } else if (absAngle &gt;= absOuterAngle) {
    // 최대 감쇠
    gain = source.coneOuterGain;
  } else {
    // 내부와 외부 cone 사이
    // 내부 -&gt; 외부, x는 0에서 1로 감
    const x = (absAngle - absInnerAngle) / (absOuterAngle - absInnerAngle);
    gain = (1 - x) + source.coneOuterGain * x;
  }
  return gain;
}
        </pre>
        <br>
        <h1>7. 성능 고려</h1>
        <h2>7.1. 레이턴시</h2>
        <figure>
          <img src="img/latency.png" width="700" height="201">
          <figcaption><strong>도식 22</strong> 레이턴시가 중요할 수 있는 사용 경우</figcaption>
        </figure>
        <p>웹 어플리케이션에서, 마우스와 키보드 이벤트 (키다운, 마우스다운 등) 과 들리는 소리 사이의 시간 지연은 중요합니다.</p>
        <p>이 시간 지연을 레이턴시라고 부르며 몇 가지 요소들에 의해 유발되고 (입력 장치 레이턴시, 내부 버퍼링 레이턴시, DSP 프로세싱 레이턴시, 출력 장치 레이턴시, 스피커와 사용자의 귀 사이의 거리 등), 이것은 누적됩니다. 레이턴시가 클수록 유저의 경험은 덜 만족스러워질 것입니다. 극단적으로는, 레이턴시는 음악 생성이나 게임 플레이를 불가능하게 만들 수도 있습니다. 적당한 수준에서 레이턴시는 타이밍에 영향을 끼칠 수 있고 소리가 처지거나 게임이 반응적이지 않은 느낌을 줄 수 있습니다. 음악 어플리케이션에서 타이밍 문제는 리듬에 영향을 줍니다. 게임에서 타이밍 문제는 게임 플레이의 정확성에 영향을 줍니다. 상호작용을 하는 어플리케이션에서, 레이턴시는 아주 느린 애니메이션 프레임 레이트가 하는 것과 같은 방식으로 유저의 경험을 대단히 낮춥니다. 어플리케이션에 따라, 합리적인 레이턴시는 3-6 밀리초의 낮은 것부터 25-50 밀리초까지 될 수 있습니다.</p>
        <p>구현은 보통 전반적인 레이턴시를 최소화하는 것을 추구할 것입니다.</p>
        <p>전반적인 레이턴시를 최소화하는 것과 함께, 구현은 보통 AudioContext의 currentTime과 AudioProcessingEvent의 playbackTime 사이의 차이를 최소화하기를 추구할 것입니다. ScriptProcessorNode의 폐지는 시간이 흐름에 따라 이 고려를 덜 중요하게 만들 것입니다.</p>
        <p>추가적으로, 몇몇 AudioNode들은 오디오 그래프의 몇몇 경로에 레이턴시를 추가할 수 있습니다. 주목할만한 것으로는,</p>
        <ul>
          <li>AudioWorkletNode는 내부적으로 버퍼링을 해, 신호 경로에 지연을 추가하는 스크립트를 실행할 수 있습니다.</li>
          <li>DelayNode의 역할은, 제어된 레이턴시를 추가하는 것입니다.</li>
          <li>BiquadFilterNode와 IIRFilterNode 필터 디자인은 인과적 필터링 프로세스의 자연스러운 결과로서 들어오는 샘플을 지연시킬 수 있습니다.</li>
          <li>ConvolverNode는, 임펄스에 따라, 컨볼루션 연산의 자연적인 결과로서, 들어오는 샘플을 지연시킬 수 있습니다.</li>
          <li>DynamicsCompressorNode는 신호 경로에서 딜레이를 유발하는 내다보기(look ahead) 알고리즘을 가지고 있습니다.</li>
          <li>MediaStreamAudioSourceNode, MediaStreamTrackAudioSourceNode, MediaStreamAudioDestinationNode는, 구현에 따라, 지연을 추가하는 버퍼를 내부적으로 추가할 수 있습니다.</li>
          <li>ScriptProcessorNode는 컨트롤 스레드와 렌더링 스레드 사이에서 버퍼를 가질 수 있습니다.</li>
          <li>WaveShaperNode는, 오버샘플링일 때, 그리고 오버샘플링 기법에 따라, 신호 경로에 지연을 추가합니다.</li>
        </ul>
        <br>
        <h2>7.2. 오디오 버퍼 복사</h2>
        <p>컨텐츠 획득 동작이 AudioBuffer에서 수행되었을 때, 전체 동작은 보통 채널 데이터를 복사하는 일 없이 구현됩니다. 특히, 마지막 단계는 다음 getChannelData() 호출에서 느리게 수행되어야 합니다. 이것은 개입하는 getChannelData()가 없는 일련의 연이은 컨텐츠 획득 동작 (예: 같은 AudioBuffer를 재생하는 다수의 AudioBufferSourceNode들) 이 할당이나 복사 없이 구현될 수 있음을 의미합니다.</p>
        <p>구현은 추가적인 최적화를 수행할 수 있습니다: 만약 getChannelData()가 AudioBuffer에서 호출된다면, 새로운 ArrayBuffer들은 아직 할당되지 않았지만, AudioBuffer에서의 이전 컨텐츠 획득 동작의 모든 호출자들은 AudioBuffer의 데이터를 사용하는 것을 멈추고, 원시 데이터 버퍼가 새로운 AudioBuffer와의 사용을 위해 재사용되며, 채널 데이터의 어떠한 재할당이나 복사를 방지합니다.</p>
        <br>
        <h2>7.3. AudioParam 전이</h2>
        <p>AudioParam의 value 어트리뷰트를 직접 설정할 때 어떠한 자동적 smoothing도 이루어지지 않는 반면, 특정한 파라미터에 대해, 부드러운 전이가 값을 직접 설정하는 것에 비해 선호됩니다.</p>
        <p>setTargetAtTime() 메서드를 낮은 timeConstant와 사용하는 것은 작성자로 하여금 부드러운 전이를 수행할 수 있게 해 줍니다.</p>
        <br>
        <h2>7.4. 오디오 깨짐(glitch)</h2>
        <p>오디오 깨짐은 보통의 연속적인 오디오 스트림의 중단에 의해 유발되고, 큰 소리의 클릭 소리와 팝 소리를 유발합니다. 오디오 깨짐은 멀티미디어 시스템의 재앙적 실패로 여겨지고 반드시 방지되어야만 합니다. 오디오 깨짐은 오디오 스트림을 하드웨어에 전송하는 책임이 있는 스레드에서의 문제에 의해 유발될 수 있는데, 그 예로는 적절한 우선도와 시간 제약을 가지고 있지 않은 스레드에 의해 유발되는 레이턴시 스케쥴링과 같은 것이 있습니다. 이것은 또한 CPU 속도를 고려했을 때 가능한 것보다 더 많은 작업을 하려고 시도하는 오디오 DSP에 의해 유발될 수도 있습니다.</p>
        <br>
        <h1>8. 보안과 프라이버시 고려 사항</h1>
        <p>셀프 리뷰 질문표: 보안과 프라이버시 §질문들에 대해:</p>
        <ol>
          <li>이 명세는 개인을 식별할 수 있는 정보를 다룹니까?
            <p>Web Audio API를 사용함으로써 청각 테스트를 수행하여, 따라서 개인이 들을 수 있는 주파수 범위 (이것은 노화에 따라 감소합니다) 를 드러내는 것은 가능합니다. 이것이 유저의 인지와 동의 없이 이루어질 수 있다고 보는 것은 어렵습니다. 왜냐하면 이것은 능동적인 참여를 요구하기 때문입니다.</p>
          </li>
          <li>이 명세는 가치가 높은 데이터를 다룹니까?
            <p>아닙니다. 신용 카드 정보와 같은 것은 Web Audio에서 사용되지 않습니다. Web Audio를 사용해 음성 데이터를 처리하고 분석하는 것은 가능하고, 이는 프라이버시 우려일 수 있지만, 사용자의 마이크에의 접근은 getUserMedia()를 통한 허가 기반입니다.</p>
          </li>
          <li>이 명세는 브라우징 세션을 가로질러 지속되는 origin에 대한 새로운 상태를 도입합니까?
            <p>아닙니다. AudioWorklet은 브라우징 세션을 가로질러 지속되지 않습니다.</p>
          </li>
          <li>이 명세는 웹에 지속되고, cross-origin인 상태를 노출시킵니까?
            <p>그렇습니다. 지원되는 오디오 샘플 레이트와 출력 장치 채널 카운트가 노출됩니다. AudioContext를 참고하십시오.</p>
          </li>
          <li>이 명세는 origin에 origin이 현재 접근하고 있지 않은 다른 데이터를 노출시킵니까?
            <p>그렇습니다. 사용 가능한 AudioNode에서의 다양한 정보를 고려할 때, Web Audio API는 잠재적으로 클라이언트의 특징적인 특성들에 대한 정보 (이를테면 오디오 하드웨어 샘플 레이트와 같은) 를 AudioNode 인터페이스를 사용하는 모든 페이지에 노출시킵니다. 추가적으로, 타이밍 정보가 AnalyserNode 또는 ScriptProcessorNode 인터페이스를 통해 수집될 수 있습니다. 이 정보는 나중에 클라이언트의 fingerprint를 생성하는 데 사용될 수 있습니다.</p>
            <br>
            <p>Princeton CITP의 웹 투명성 및 책임 프로젝트에 의한 연구는 DynamicsCompressorNode와 OscillatorNode가 클라이언트로부터 엔트로피를 수집하는 데 사용되어 기기를 fingerpirnt할 수 있음을 보여주었습니다. 이것은 작고, 보통 들리지 않는, 상이한 구현들 사이의 DSP 아키텍쳐, 리샘플링 전략, rounding trade-off에서의 차이에 기인합니다. 사용된 정밀한 컴파일러 플래그와 CPU 아키텍쳐 (ARM 대 x86) 또한 이 엔트로피에 기여합니다.</p>
            <br>
            <p>그러나 실제로는, 이것은 단지 더 쉬운 수단 (User Agent string) 에 의해 이미 손쉽게 사용 가능한 정보의 추론, 예를 들자면 "이것은 플랫폼 Y에서 실행 중인 브라우저 X이다", 만을 허용할 뿐입니다. 그러나, 추가적인 fingerprinting의 가능성을 줄이기 위해, 우리는 브라우저에게 모든 노드의 출력으로부터 가능할지도 모르는 fingerprinting 문제를 완화하도록 조치를 취할 것을 지시합니다.</p>
            <br>
            <p>clock skew를 통한 fingerprinting이 Steven J Murdoch와 Sebastian Zander에 의해 설명되었습니다. getOutputTimestamp로부터 이것을 결정하는 것이 가능할지도 모릅니다. 또한 skew 기반의 fingerprinting은 HTML에 대해 Nakibly et. al. 에 의해 시연되었습니다. High-Resolution Time §7 프라이버시 및 보안 섹션이 clock resolution과 drift에 대한 추가적인 정보를 위해 참고될 수 있습니다.</p>
            <br>
            <p>레이턴시를 통한 fingerprinting이 또한 가능합니다. 이것을 baseLatency와 outputLatency로부터 추론하는 것이 가능할지도 모릅니다. 완화 전략은 jitter (디더링) 와 양자화를 추가하여 정확한 skew가 올바르지 않게 기록되도록 하는 것을 포함합니다. 그러나 대부분의 오디오 시스템은 Web Audio에 의해 생성된 오디오와, 다른 오디오나 비디오 소스 또는 시각적 신호 (예를 들자면 게임, 오디오 녹음, 음악 제작 환경에서) 를 동기화하기 위해 낮은 레이턴시를 지향한다는 것에 주의하십시오. 지나친 레이턴시는 유용성을 낮추고 접근성 문제가 될 수도 있습니다.</p>
            <br>
            <p>AudioContext의 샘플 레이트를 통한 fingerprinting이 또한 가능합니다. 우리는 이것을 최소화하기 위해 다음의 절차가 취해질 것을 추천합니다.</p>
            <ol>
              <li>44.1 kHz와 48 kHz가 기본 레이트로 허용됩니다. 시스템은 최선의 적용성을 위해 둘 사이에서 선택할 것입니다. (분명히, 만약 오디오 장치가 원래 44.1이라면, 44.1이 선택될 것이지만, 또한 시스템은 가장 "호환이 되는" 레이트를 선택할 수도 있습니다—예를 들어, 만약 시스템이 원래 96kHz라면, 44.1kHz가 아니라, 48kHz가 선택될 공산이 큽니다.</li>
              <li>시스템은 원래부터 다른 레이트인 장치들에 대해 이 두 레이트 중 하나로 리샘플링해야 하고, 이것이 리샘플링된 오디오에 기인한 추가적인 배터리 drain을 유발할 수도 있다는 사실에도 불구하고 그렇습니다. (다시 말하지만, 시스템은 가장 호환되는 레이트를 선택할 것입니다—만약 네이티브 시스템이 16kHz라면, 48kHz가 선택될 것이 기대됩니다.)</li>
              <li>(명령되는 것은 아니지만) 브라우저가 네이티브 레이트의 사용을 강제하는 사용자 여유를 제공하는 것이 기대됩니다—예를 들어 장치의 브라우저에서 플래그를 설정함으로써. 이 설정은 API에 의해 노출되지 않을 것입니다.</li>
              <li>상이한 레이트가 명시적으로 AudioContext의 생성자에서 요구될 수 있음이 또한 기대되는 동작이며 (이것은 이미 명세에 있습니다. 이것은 보통 오디오 렌더링이 요청된 sampleRate에서 이루어지고, 그리고 나서 장치 출력에 대해 업샘플링 혹은 다운샘플링되는 것을 유발합니다), 만약 이 레이트가 원래 지원된다면, 렌더링은 바로 통과될 수 있습니다. 이것은 앱이 사용자의 조정 없이 더 높은 레이트로 렌더링하는 것을 가능케 할 것입니다. (물론 오디오 출력이 출력에서 다운샘플링되는 것은 Web Audio에서 관측불가능합니다)—예를 들어, 만약 MediaDevices 가능성이 (사용자의 조정과 함께) 읽혔고 나타났었다면 더 높은 레이트는 지원되었을 것입니다.</li>
            </ol>
            <p>AudioContext의 출력 채널의 수를 통한 fingerprinting이 또한 가능합니다. 우리는 maxChannelCount가 2 (스테레오) 로 설정될 것을 추천합니다. 스테레오는 단연 가장 흔한 채널 수입니다.</p>
          </li>
          <li>이 명세는 새로운 스크립트 실행/로딩 메커니즘을 가능케 합니까?
            <p>아닙니다. 이것은 물론 [HTML] 스크립트 실행 메서드를 사용하고, 이는 저 명세에 정의되어 있습니다.</p>
          </li>
          <li>이 명세는 사용자의 위치에 대한 origin 접근을 허용합니까?
            <p>아닙니다.</p>
          </li>
          <li>이 명세는 사용자의 기기의 센서에 대한 origin 접근을 허용합니까?
            <p>직접적으로는 아닙니다. 현재, 오디오 입력은 이 문서에서 명시되지 않았지만, 이것은 클라이언트 기계의 오디오 입력 또는 마이크에의 접근을 얻는 것을 포함합니다. 이것은 유저에게 적절한 방법으로 허락을 물어보는 것을 요구할 것입니다. 그 방법은 아마도 getUserMedia() API를 통한 것입니다.</p>
            <p>추가적으로, Media Capture and Streams 명세로부터의 보안 그리고 프라이버시 고려 사항이 주목되어야 합니다. 특히, 주변 오디오의 분석 또는 고유한 오디오를 재생하는 것은 사용자의 위치의 식별을 가능하게 하며, 그 수준은 방(room)의 수준에까지 혹은 심지어 다른 사용자나 기기에 의한 방의 동시 점유까지입니다. 또한 오디오 출력과 오디오 입력에의 동시 접근은 한 브라우저에서 분할된 다른 컨텍스트 사이의 소통을 가능케 할 지도 모릅니다.</p>
          </li>
          <li>이 명세는 사용자의 로컬 컴퓨팅 환경의 측면에 대한 origin 접근을 허용합니까?
            <p>직접적으로는 아닙니다. 모든 요청된 샘플 레이트는 지원되고, 만약 필요하다면 업샘플링됩니다. MediaTrackSupportedConstraints와 동반하는 지원되는 오디오 샘플 레이트를 찾기 위해 Media Capture and Streams를 사용하는 것은 가능합니다. 이것은 명시적인 사용자 동의를 요구합니다. 이것은 물론 조금 정도의 fingerprinting을 제공합니다. 그러나, 실제로는 대부분의 소비자와 프로슈머 기기는 두 개의 표준화된 샘플 레이트 중 하나를 사용합니다. 그것은 44.1kHz (CD에서 원래 사용됨) 와 48kHz (DAT에서 원래 사용됨) 입니다. highly resource constrained 장치는 음성 품질의 11kHz 샘플 레이트를 지원할 수도 있고, higher-end 장치는 종종 88.2, 96, 또는 오디오 애호가의 192kHz 레이트를 지원합니다.</p>
            <p>48kHz와 같은 하나의, 공통적으로 지원되는 레이트를 업샘플링하기 위해 모든 구현을 요구하는 것은 특별한 이득 없이 CPU 비용을 증가시킬 것이고, higher-end 장치에서 낮은 레이트를 사용하기를 요구하는 것은 단지 Web Audio를 전문적인 사용에 적합하지 않다고 딱지붙이는 결과를 낳을 뿐일 것입니다.</p>
          </li>
          <li>이 명세는 다른 장치에 대한 origin 접근을 허용합니까?
            <p>이것은 보통 다른 네트워크 장치에 대한 접근을 허용하지 않습니다 (하나의 예외로 high-end 녹음 스튜디오는 Dante 네트워크 장치일지도 모릅니다. 비록 이 장치들이 보통 별도의, 특정 작업용 네트워크를 사용하긴 하지만요). 이것은 필연적으로 사용자의 오디오 출력 장치나 때때로 컴퓨터에 별도의 유닛인 장치들에 대한 접근을 허용합니다.</p>
            <p>음성이나 소리를 작동시키는 장치에 대해서, Web Audo API는 다른 장치들을 제어하기 위해 사용될 지도 모릅니다. 추가적으로, 만약 소리를 생성하는 장치가 초음파에 근접한 주파수에 민감하다면, 그러한 제어는 들리지 않을 지도 모릅니다. 이 가능성은 또한 HTML과 함께 존재하고, 이는 &lt;audio&gt; 또는 &lt;video&gt; 요소를 통한 것입니다. 일반적인 오디오 샘플링 레이트에서, 많은 초음파 정보에 대해 (디자인에 따라) 불충분한 헤드룸이 있습니다.</p>
            <p>인간 청각의 한계는 보통 20kHz로 서술됩니다. 44.1kHz 샘플링 레이트에 대해, 나이퀴스트 한계는 22.05kHz입니다. true brickwall 필터가 물리적으로 실현될 수 없음을 고려할 때, 20kHz와 22.05kHz 사이의 공간은 빠른 rolloff 필터에 사용되어 나이퀴스트 위의 모든 주파수를 강력하게 약화시킵니다.</p>
            <p>48kHz 샘플링 레이트에서, 20kHz에서 24kHz 밴드에서 여전히 빠른 약화가 있습니다 (하지만 passband에서의 위상 ripple error를 방지하는 것이 더 쉽습니다).</p>
          </li>
          <li>이 명세는 origin에게 user agent의 native UI에 대한 얼마간의 제어를 허용합니까?
            <p>만약 UI가 voice assistant 혹은 스크린리더같은 오디오 컴포넌트를 가지고 있다면, Web Audio API는 공격이 더욱 로컬 시스템 이벤트처럼 보이도록 만들기 위해 native UI의 측면을 모방하기 위해 사용될 지도 모릅니다. 이 가능성은 또한 &lt;audio&gt;요소에 의해 HTML에서도 존재합니다.</p>
          </li>
          <li>이 명세는 웹에 temporary identifier를 노출시킵니까?
            <p>아닙니다.</p>
          </li>
          <li>이 명세는 first-party와 third-party 컨텍스트에서의 행동을 구별합니까?
            <p>아닙니다.</p>
          </li>
          <li>이 명세는 user agent의 "incognito" 모드에서 어떻게 다르게 작동합니까?
            <p>다르지 않습니다.</p>
          </li>
          <li>이 명세는 사용자의 로컬 기기에 데이터를 지속시킵니까?
            <p>아닙니다.</p>
          </li>
          <li>이 명세는 "보안 고려 사항"과 "프라이버시 고려 사항" 섹션을 가지고 있습니까?
            <p>네 (지금 읽고 계십니다).</p>
          </li>
          <li>이 명세는 기본 보안 특징들을 다운그레이딩하는 것을 허용합니까?
            <p>아닙니다.</p>
          </li>
        </ol>
        <br>
        <h1>9. 필요 조건 및 사용 경우</h1>
        <p>[webaudio-usecases]를 참고하십시오.</p>
        <br>
        <h1>10. 예제 코드에서 쓰이는 코드 정의</h1>
        <p>이 섹션은 이 명세에서 쓰이는, 자바스크립트로 구현된 공통적인 함수와 클래스를 기술합니다.</p>
        <pre>
// 삼차원 벡터 클래스
class Vec3 {
  // 3개의 좌표를 가지고 생성
  constructor(x, y, z) {
    this.x = x;
    this.y = y;
    this.z = z;
  }
  // 다른 벡터와이 dot product
  dot(v) {
    return (this.x * v.x) + (this.y * v.y) + (this.z * v.z);
  }
  // 다른 벡터와의 cross product
  cross(v) {
    return new Vec3((this.y * v.z) - (this.z * v.y),
      (this.z * v.x) - (this.x * v.z),
      (this.x * v.y) - (this.y * v.x));
  }
  // 다른 벡터와의 차
  diff(v) {
    return new Vec3(this.x - v.x, this.y - v.y, this.z - v.z);
  }
  // 이 벡터의 magnitude를 획득
  get magnitude() {
    return Math.sqrt(dot(this));
  }
  // 스칼라만큼 곱해진 이 벡터의 복제를 획득
  scale(s) {
    return new Vec3(this.x * s, this.y * s, this.z * s);
  }
  // 이 벡터의 normalized된 복제를 획득
  normalize() {
    const m = magnitude;
    if (m == 0) {
      return new Vec3(0, 0, 0);
    }
    return scale(1 / m);
  }
}
        </pre>
        <br>
        <h1>11. 변경 역사</h1>
        <p>생략</p>
        <br>
        <h1>12. 감사의 글</h1>
        <p>생략</p>
        <br>
        <h1>Conformance</h1>
        <p>생략</p>
        <br>
        <h1>색인</h1>
        <p>생략</p>
        <br>
        <h1>참고 목록</h1>
        <p>생략</p>
        <br>
        <h1>IDL 색인</h1>
        <p>생략</p>
      </div>
    </main>
  </body>
</html>
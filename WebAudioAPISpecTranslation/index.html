<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Web Audio API Spec Korean Translation</title>
    <link rel="stylesheet" href="index.css">
  </head>
  <body>
    <header>
      <p><a href="../index.html">return to home</a></p>
      <br>
      <p id="title">W3C Web Audio API Specification Korean Translation</p>
      <p class="description">It would be appreciated if you note that this translation is <strong>unofficial</strong>; it is just for the pure sake of my studying Web Audio API.</p>
      <p class="description">이 번역이 <strong>비공식</strong>임에 주목해주신다면 감사하겠습니다. 이 번역은 단지 순수하게 제가 Web Audio API를 공부하기 위한 것입니다.</p>
      <a href="https://www.w3.org/TR/webaudio/">link to the original document</a>
      <p>notice: currently on work: 21.11.18 ~ </p>
    </header>
    <hr>
    <main>
      <div id="frame">
        <h1>개요</h1>
        <p>이 명세는 웹 애플리케이션에서 오디오를 프로세싱하고 합성하기 위한 고수준의 WEB API를 기술합니다. 이 API의 주요한 패러다임은 오디오 라우팅 그래프에 관한 것인데, 오디오 라우팅 그래프란 다수의 AudioNode 객체들이 함께 연결되어 전반적인 오디오 렌더링을 정의하는 것입니다. 실제적인 프로세싱은 근본적인 구현 (보통 최적화된 Assembly / C / C++ 코드) 에서 주로 일어나지만, 직접적인 스크립트 프로세싱과 합성 또한 지원됩니다.</p>
        <p>아래의 '소개' 절은 이 명세의 동기를 다룹니다.</p>
        <p>이 API는 웹 플랫폼상의 다른 API 및 요소와 함께 사용되도록 설계되었습니다. 주요한 것으로는 XMLHttpRequest [XHR] 이 있습니다 (responseType과 response 어트리뷰트를 사용). 게임과 상호작용이 일어나는 어플리케이션에 대해서는, canvas 2D [2dcontext] 와 WebGL [WEBGL] 3D 그래픽 API와 함께 사용되는 것이 기대됩니다.</p>
        <h1>이 문서의 상태</h1>
        <p>생략</p>
        <h1>소개</h1>
        <p>웹에서의 오디오는 이 시점까지 상당히 원시적이었고 아주 최근까지는 Flash나 QuickTime과 같은 플러그인을 통해 전송되었습니다. HTML5에서의 audio 요소의 소개는 아주 중요했는데, 이는 기본적인 스트리밍 오디오 재생을 가능케 했습니다. 그러나, 더욱 복잡한 오디오 애플리케이션을 다루기에는 충분히 강력하지 못했습니다. 정교한 웹 기반의 게임과 상호작용이 일어나는 애플리케이션에 대해서, 또 다른 해결책이 요구되었습니다. 현대의 데스크톱 오디오 프로덕션 애플리케이션에서 발견되는 몇몇 믹싱, 프로세싱, 필터링 태스크에 더불어 현대 게임의 오디오 엔진에서 발견되는 능력들을 포함하는 것이 이 명세의 목표입니다.</p>
        <p>Web Audio API는 사용 경우 <a href="https://www.w3.org/TR/webaudio-usecases/" target="_blank">[webaudio-usecases]</a> 의 넓은 다양성을 염두에 두고 설계되었습니다. 이상적으로, 이 API는 스크립트를 통해 제어되고 브라우저에서 실행되는 최적화된 C++ 엔진으로 적절하게 구현될 수 있는 모든 사용 경우를 지원할 수 있어야 합니다. 그렇긴 하지만, 현대의 데스크톱 오디오 소프트웨어는 아주 발전된 능력을 가지고 있어서, 그것들 중 몇몇은 이 시스템으로 개발하기 어렵거나 불가능할지도 모릅니다. Apple의 Logic Audio가 그런 애플리케이션 중 하나인데, 이 애플리케이션은 외부 MIDI 컨트롤러, 임의 플러그인 오디오 이펙트, 신디사이저, 대단히 최적화된 직접 녹음형(direct-to-disk) 오디오 파일 읽기/쓰기, 단단히 통합된 time-stretching 등등을 지원합니다. 그럼에도 불구하고, 이 제안된 시스템은 적절히 복잡한 게임과 음악적인 애플리케이션을 포함하는 상호작용이 일어나는 애플리케이션의 넓은 범위를 꽤 지원할 수 있을 것입니다. 그리고 이 API는 WebGL에 의해 제공되는 아주 발전된 그래픽 기능의 아주 훌륭한 보완물이 될 수 있습니다. 이 API는 더욱 발전된 능력이 추후에 더해질 수 있도록 설계되었습니다.</p>
        <h2>기능</h2>
        <p>Web Audio API는 아래의 주요한 기능들을 지원합니다.</p>
        <ul>
          <li>간단하거나 복잡한 믹싱/이펙트 아키텍처를 위한 모듈러 라우팅</li>
          <li>높은 dynamic 범위. 내부 프로세싱에서 32비트 float을 사용.</li>
          <li>드럼 머신과 시퀸서와 같은 아주 높은 정도의 리듬 정밀도를 요구하는 음악 애플리케이션을 위한 낮은 레이턴시를 가지는 sample-accurate scheduled 사운드 재생. 이 기능은 또한 이펙트의 동적 생성의 가능성을 포함합니다.</li>
          <li>엔벨로프, 페이드인/페이드아웃, granular 이펙트, 필터 sweep, LFO 등을 위한 오디오 파라미터의 automation</li>
          <li>오디오 스트림에서의 유연한 채널 핸들링. 채널이 나눠지거나 합쳐질 수 있게 함.</li>
          <li>&lt;audio&gt; 또는 &lt;video&gt; 미디어 요소로부터의 오디오 소스 프로세싱
            <ul>
              <li>MediaStreamTrackAudioSourceNode와 [webrtc]를 사용한 원격 peer로부터 받은 오디오 프로세싱</li>
              <li>MediaStreamAudioDestinationNode와 [webrtc]를 사용해 원격 peer로 생성되거나 프로세싱된 오디오 스트림 전송</li>
            </ul>
          </li>
          <li>스크립트를 사용한 직접적 오디오 스트림 합성과 프로세싱</li>
          <li>3D 게임과 immersive한 환경의 넓은 범위를 지원하는 공간화된(spatialized) 오디오
            <ul>
              <li>패닝 모델: equalpower, HRTF, pass-through</li>
              <li>거리 감쇠(attenuation)</li>
              <li>sound cone</li>
              <li>방해/폐쇄 (obstuction/occlusion)</li>
              <li>음원(source)/청자(listener) 기반</li>
            </ul>
          </li>
          <li>선형 이펙트의 넓은 범위를, 특히 아주 높은 품질의 room 이펙트를 위한 convolution 엔진. 아래는 가능한 효과의 몇 가지 예입니다.
            <ul>
              <li>작은/큰 방(room)</li>
              <li>대성당(cathedral)</li>
              <li>콘서트 홀</li>
              <li>동굴(cave)</li>
              <li>터널(tunnel)</li>
              <li>복도(hallway)</li>
              <li>숲(forest)</li>
              <li>원형 극장(amphitheater)</li>
              <li>출입구를 거치는 멀리 있는 방으로부터의 소리</li>
              <li>extreme 필터</li>
              <li>기묘한 backward 이펙트</li>
              <li>extreme 빗(comb) 필터 이펙트</li>
            </ul>
          </li>
          <li>믹스의 전반적인 제어와 sweetening을 위한 dynamics compression</li>
          <li>효율적인 실시간 시간 영역(time-domain) 그리고 주파수 영역(frequency-domain) 분석 / 음악 시각화 지원</li>
          <li>lowpass, highpass, 그리고 다른 일반적인 필터를 위한 효율적인 biquad 필터</li>
          <li>distortion을 위한 waveshaping 효과와 다른 비선형적 효과</li>
          <li>오실레이터(oscilator)</li>
        </ul>
        <h3>모듈러 라우팅</h3>
        <p>모듈러 라우팅은 다른 AudioNode 객체 사이의 임의적인 연결을 가능케 합니다. 각 노드는 입력과 출력 양 쪽 모두 또는 어느 한 쪽을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력만을 가지고 있습니다. 목적지(destination) 노드는 하나의 입력을 가지고 출력이 없는 노드입니다. 필터와 같은 다른 노드들은 소스 노드와 목적지 노드 사이에 놓일 수 있습니다. 개발자는 두 객체가 함께 연결될 때의 저수준의 스트림 포맷 디테일에 대해 걱정할 필요가 없습니다. 옳은 일이 단지 일어나니까요. 예를 들자면, 만약 모노 오디오 스트림이 스테레오 입력에 연결되었다면 API는 단지 좌측 채널과 우측 채널을 적절히 믹스할 것입니다.</p>
        <p>가장 단순한 경우, 하나의 소스가 바로 출력으로 route될 수 있습니다. 모든 라우팅은 하나의 AudioDestinationNode를 포함하고 있는 AudioContext내에서 발생합니다.</p>
        <figure>
          <img src="img/modular-routing1.png" width="305" height="128">
          <figcaption><strong>도식 1</strong> 모듈러 라우팅의 단순한 예제</figcaption>
        </figure>
        <p>위 그림은 이 단순한 라우팅을 보여줍니다. 아래는 하나의 사운드를 재생하는 간단한 예제입니다.</p>
        <pre>예제 1
const context = new AudioContext();

function playSound() {
  const source = context.createBufferSource();
  source.buffer = dogBarkingBuffer;
  source.connect(context.destination);
  source.start(0);
}
        </pre>
        <p>아래는 세 개의 소스와 최종 출력 스테이지에서 dynamics compressor로 보내지는 convolution reverb를 보여주는 더욱 복잡한 예시입니다.</p>
        <figure>
          <img src="img/modular-routing2.png" width="561" height="411">
          <figcaption><strong>도식 2</strong> 모듈러 라우팅의 더욱 복잡한 예제</figcaption>
        </figure>
        <pre>예제 2
let context;
let compressor;
let reverb;
let source1, source2, source3;
let lowpassFilter;
let waveShaper;
let panner;
let dry1, dry2, dry3;
let wet1, wet2, wet3;
let mainDry;
let mainWet;
function setupRoutingGraph () {
  context = new AudioContext();
  // 이펙트 노드를 생성합니다.
  lowpassFilter = context.createBiquadFilter();
  waveShaper = context.createWaveShaper();
  panner = context.createPanner();
  compressor = context.createDynamicsCompressor();
  reverb = context.createConvolver();
  // main wet과 dry를 생성합니다.
  mainDry = context.createGain();
  mainWet = context.createGain();
  // 마지막 compressor를 마지막 목적지에 연결합니다.
  compressor.connect(context.destination);
  // main dry와 wet을 compressor에 연결합니다.
  mainDry.connect(compressor);
  mainWet.connect(compressor);
  // reverb를 main wet에 연결합니다.
  reverb.connect(mainWet);
  // 몇 가지 소스를 생성합니다.
  source1 = context.createBufferSource();
  source2 = context.createBufferSource();
  source3 = context.createOscillator();
  source1.buffer = manTalkingBuffer;
  source2.buffer = footstepsBuffer;
  source3.frequency.value = 440;
  // source1을 연결합니다.
  dry1 = context.createGain();
  wet1 = context.createGain();
  source1.connect(lowpassFilter);
  lowpassFilter.connect(dry1);
  lowpassFilter.connect(wet1);
  dry1.connect(mainDry);
  wet1.connect(reverb);
  // source2를 연결합니다.
  dry2 = context.createGain();
  wet2 = context.createGain();
  source2.connect(waveShaper);
  waveShaper.connect(dry2);
  waveShaper.connect(wet2);
  dry2.connect(mainDry);
  wet2.connect(reverb);
  // source3을 연결합니다.
  dry3 = context.createGain();
  wet3 = context.createGain();
  source3.connect(panner);
  panner.connect(dry3);
  panner.connect(wet3);
  dry3.connect(mainDry);
  wet3.connect(reverb);
  // 소스들을 지금 시작시킵니다.
  source1.start(0);
  source2.start(0);
  source3.start(0);
}
        </pre>
        <p>모듈러 라우팅은 또한 AudioNode들의 출력이 다른 AudioNode의 행동을 제어하는 AudioParam 파라미터에 route될 수 있게 합니다. 아래의 시나리오에서, 노드의 출력은 입력 시그널보다는 조절 시그널로서 행동합니다.</p>
        <figure>
          <img src="img/modular-routing3.png" width="346" height="337">
          <figcaption><strong>도식 3</strong> 하나의 오실레이터가 다른 오실레이터의 주파수를 조절하는 모습을 보여주는 모듈러 라우팅 시나리오</figcaption>
        </figure>
        <pre>예제 3
function setupRoutingGraph() {
  const context = new AudioContext();
  // 조절 시그널을 공급하는 낮은 주파수의 오실레이터를 생성합니다
  const lfo = context.createOscillator();
  lfo.frequency.value = 1.0;
  // 조절될 높은 주파수의 오실레이터를 생성합니다
  const hfo = context.createOscillator();
  hfo.frequency.value = 440.0;
  // gain 노드를 생성하는데 이 노드의 gain은 조절 시그널의 진폭을 결정합니다
  const modulationGain = context.createGain();
  modulationGain.gain.value = 50;
  // 그래프를 설정하고 오실레이터들을 시작시킵니다
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
        </pre>
        <h2>API 개요</h2>
        <p>정의되는 인터페이스들은 아래와 같습니다.</p>
        <ul>
          <li>AudioContext 인터페이스. 이 인터페이스는 AudioNode간의 연결을 표현하는 오디오 시그널 그래프를 포함합니다.</li>
          <li>AudioNode 인터페이스. 이 인터페이스는 오디오 소스, 오디오 출력, 중간 프로세싱 모듈을 표현합니다. AudioNode는 모듈러 방식으로 동적으로 연결될 수 있습니다. AudioNode는 AudioContext의 컨텍스트 내에서 존재합니다.</li>
          <li>AnalyserNode 인터페이스. 이는 음악 시각화나 다른 시각화 애플리케이션에서의 사용을 위한 AudioNode입니다.</li>
          <li>AudioBuffer 인터페이스. 이는 메모리에 상주하는(memory-resident) 오디오 에셋으로 작업하기 위한 것입니다. 이 오디오 에셋은 한 번의 소리, 혹은 더 긴 오디오 클립을 표현할 수 있습니다.</li>
          <li>AudioBufferSourceNode 인터페이스. 이는 AudioBuffer에서 오디오를 생성하는 AudioNode입니다.</li>
          <li>AudioDestinationNode 인터페이스. 모든 렌더링된 오디오에 대한 마지막 목적지를 나타내는 AudioNode의 서브클래스입니다.</li>
          <li>AudioParam 인터페이스. 볼륨과 같은 AudioNode 기능의 개별적 측면을 제어합니다.</li>
          <li>AudioListener 인터페이스. 공간화를 위해 PannerNode와 함께 사용됩니다.</li>
          <li>AudioWorklet 인터페이스. 스크립트를 사용해 직접 오디오를 프로세싱할 수 있는 사용자 정의 노드를 생성하기 위한 팩토리를 나타냅니다.</li>
          <li>AudioWorkletGlobalScope 인터페이스. AudioWorkletProcessor의 프로세싱 스크립트가 실행되는 컨텍스트입니다.</li>
          <li>AudioWorkletNode 인터페이스. AudioWorkletProcessor 내에서 프로세싱되는 노드를 나타내는 AudioNode입니다.</li>
          <li>AudioWorkletProcessor 인터페이스. 오디오 worker 내부의 단일 노드 인스턴스를 나타냅니다.</li>
          <li>BiquadFilterNode 인터페이스. 다음과 같은 일반적인 low-order 필터들에 대한 AudioNode입니다.
            <ul>
              <li>Low Pass</li>
              <li>High Pass</li>
              <li>Band Pass</li>
              <li>Low Shelf</li>
              <li>High Shelf</li>
              <li>Peaking</li>
              <li>Notch</li>
              <li>Allpass</li>
            </ul>
          </li>
          <li>ChannelMergerNode 인터페이스. 다수의 오디오 스트림으로부터의 채널을 하나의 오디오 스트림으로 결합하기 위한 AudioNode.</li>
          <li>ChannelSplitterNode 인터페이스. 라우팅 그래프에서 오디오 스트림의 개개의 채널에 접근하기 위한 AudioNode.</li>
          <li>ConstantSourceNode 인터페이스. AudioParam이 명목상의 constant 출력값의 automation을 허용하게 하며, 저 값을 생성하기 위한 AudioNode.</li>
          <li>ConvolverNode 인터페이스. (콘서트 홀의 소리와 같이) 실시간 선형 이펙트를 적용하기 위한 AudioNode.</li>
          <li>DelayNode 인터페이스. 동적으로 조정 가능한 가변적인 딜레이를 적용하는 AudioNode.</li>
          <li>DynamicsCompressorNode 인터페이스. dynamics compression을 위한 AudioNode.</li>
          <li>GainNode 인터페이스. 명시적인 gain 제어를 위한 AudioNode.</li>
          <li>IIRFilterNode 인터페이스. 일반적인 IIR 필터를 위한 AudioNode.</li>
          <li>MediaElementAudioSourceNode 인터페이스. &lt;audio&gt;, &lt;video&gt;, 혹은 그 밖의 미디어 요소로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamTrackAudioSourceNode 인터페이스. MediaStreamTrack으로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamAudioDestinationNode 인터페이스. 원격 peer에게 전송되는 MediaStream으로의 오디오 목적지인 AudioNode.</li>
          <li>PannerNode 인터페이스. 3D 공간에서의 공간화/포지셔닝을 위한 AudioNode.</li>
          <li>PeriodicWave 인터페이스. OscillatorNode에 의한 사용을 위한 사용자 정의 주기 파형을 명시하기 위한 인터페이스입니다.</li>
          <li>OscillatorNode 인터페이스. 주기 파형을 생성하기 위한 AudioNode.</li>
          <li>StereoPannerNode 인터페이스. 스테레오 스트림에서 오디오 입력의 equal-power 포지셔닝을 위한 AudioNode.</li>
          <li>WaveShaperNode 인터페이스. distortion과 그 밖의 다른 subtle warming 효과를 위한 비선형적 waveshaping 효과를 적용하는 AudioNode.</li>
        </ul>
        <p>Web Audio API에서 폐기(deprecated)되었지만 이것들의 대체물의 구현 경험이 있을 때 까지는 아직 제거되지 않을 몇몇 기능들이 있습니다.</p>
        <ul>
          <li>ScriptProcessorNode 인터페이스. 스크립트를 사용해 직접적으로 오디오를 생성하거나 프로세싱하기 위한 AudioNode.</li>
          <li>AudioProcessingEvent 인터페이스. ScriptProcessorNode 객체와 함께 사용되는 이벤트 유형입니다.</li>
        </ul>
        <h1>1. Audio API</h1>
        <h2>1.1. BaseAudioContext 인터페이스</h2>
        <p>BaseAudioContext 인터페이스는 AudioNode 객체들의 집합과 이 객체들의 연결을 나타냅니다. 이 인터페이스는 AudioDestinationNode로 향하는 임의적인 신호 라우팅을 고려합니다. 노드들은 이 컨텍스트에서 생성되고 그리고 나서 함께 연결됩니다.</p>
        <p>BaseAudioContext는 직접적으로 인스턴스화되지 않지만, 대신 실체가 있는 인터페이스인 AudioContext (실시간 렌더링용) 와 OfflineAudioContext (오프라인 렌더링용) 에 의해 상속됩니다.</p>
        <p>BaseAudioContext는 내부 슬롯 [[pending promises]] 를 가지고 생성되는데, 이 내부 슬롯은 초기적으로는 빈 promise의 정렬된 리스트입니다.</p>
        <p>각 BaseAudioContext는 고유의 미디어 요소 이벤트 태스크 소스를 가지고 있습니다. 추가적으로, BaseAudioContext는 두 개의 private 슬롯 [[rendering thread state]] 와 [[control thread state]]를 가지고 있는데, 이 슬롯들은 AudioContextState로부터 값을 취하며, 둘 다 초기적으로 "suspended"로 설정되어 있습니다.</p>
        <pre>
enum AudioContextState {
  "suspended",
  "running",
  "closed"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>suspended</i>"</td>
              <td>이 컨텍스트는 현재 연기되었습니다 (context time은 진행되지 않으며, audio 하드웨어는 전원이 차단되었거나/released 되었을 수 있습니다).</td>
            </tr>
            <tr>
              <td>"<i>running</i>"</td>
              <td>오디오가 처리되고 있습니다.</td>
            </tr>
            <tr>
              <td>"<i>closed</i>"</td>
              <td>이 컨텍스트는 released되었고, 더 이상 오디오를 처리하기 위해 사용될 수 없습니다. 모든 시스템 오디오 자원이 released되었습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
callback DecodeErrorCallback = undefined (DOMException error);

callback DecodeSuccessCallback = undefined (AudioBuffer decodedData);

[Exposed=Window]
interface BaseAudioContext : EventTarget {
  readonly attribute AudioDestinationNode destination;
  readonly attribute float sampleRate;
  readonly attribute double currentTime;
  readonly attribute AudioListener listener;
  readonly attribute AudioContextState state;
  [SameObject, SecureContext]
  readonly attribute AudioWorklet audioWorklet;
  attribute EventHandler onstatechange;

  AnalyserNode createAnalyser ();
  BiquadFilterNode createBiquadFilter ();
  AudioBuffer createBuffer (unsigned long numberOfChannels,
                            unsigned long length,
                            float sampleRate);
  AudioBufferSourceNode createBufferSource ();
  ChannelMergerNode createChannelMerger (optional unsigned long numberOfInputs = 6);
  ChannelSplitterNode createChannelSplitter (
    optional unsigned long numberOfOutputs = 6);
  ConstantSourceNode createConstantSource ();
  ConvolverNode createConvolver ();
  DelayNode createDelay (optional double maxDelayTime = 1.0);
  DynamicsCompressorNode createDynamicsCompressor ();
  GainNode createGain ();
  IIRFilterNode createIIRFilter (sequence<double> feedforward,
                                 sequence<double> feedback);
  OscillatorNode createOscillator ();
  PannerNode createPanner ();
  PeriodicWave createPeriodicWave (sequence<float> real,
                                   sequence<float> imag,
                                   optional PeriodicWaveConstraints constraints = {});
  ScriptProcessorNode createScriptProcessor(
    optional unsigned long bufferSize = 0,
    optional unsigned long numberOfInputChannels = 2,
    optional unsigned long numberOfOutputChannels = 2);
  StereoPannerNode createStereoPanner ();
  WaveShaperNode createWaveShaper ();

  Promise<AudioBuffer> decodeAudioData (
    ArrayBuffer audioData,
    optional DecodeSuccessCallback? successCallback,
    optional DecodeErrorCallback? errorCallback);
};          
        </pre>
        <h3>1.1.1. 어트리뷰트</h3>
        <dl>
          <dt><i>audioWorklet</i> / 자료형: AudioWorklet / 읽기 전용</dt>
          <dd>Worklet 객체로의 접근을 가능케 하는데, Worklet 객체는 [HTML] 과 AudioWorklet에 의해 정의된 알고리즘을 통해 AudioWorkletProcessor 클래스 정의를 포함하고 있는 스크립트를 import할 수 있습니다.</dd>
          <br>
          <dt></i>currentTime</i> / 자료형: double / 읽기 전용</dt>
          <dd>currentTime은 컨텍스트의 렌더링 그래프에 의해 가장 최근에 처리된 오디오 블럭의 마지막 sample frame 바로 다음의 샘플 프레임의 시간 (초) 입니다. 만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, currentTime은 0의 값을 갖습니다.</dd>
          <br>
          <dd>currentTime의 시간 좌표계에서, 0의 값은 그래프에 의해 처리되는 첫번째 블럭의 첫번째 샘플 프레임과 일치합니다. 이 계(system)에서 경과된 시간은 BaseAudioContext에 의해 생성된 오디오 스트림에서의 경과된 시간과 일치하는데, 이 시간은 이 계의 다른 시계와 동기화되지 않았을 수 있습니다. (OfflineAudioContext의 경우, 스트림이 어떠한 장치에서도 현재 재생되고 있지 않으므로, 실제 시간에 대한 근사치조차도 존재하지 않습니다.</dd>
          <br>
          <dd>Web Audio API에서의 모든 예정된 시간들은 currentTime의 값에 관련이 있습니다.</dd>
          <br>
          <dd>BaseAudioContext가 "running" 상태일 때, 이 어트리뷰트의 값은, 하나의 render quantum에 일치하며, 단조적으로 증가하고 획일적인 증가 속에서 렌더링 스레드에 의해 갱신됩니다. 따라서, 실행 중인 컨텍스트의 경우, 이 계가 오디오 블럭을 처리해 가는 동안 currentTime은 꾸준히 증가하고, 항상 처리될 다음 오디오 블럭의 시작 시간을 나타냅니다. 현재 상태에서 예정된 어떠한 변화가 일어날지도 모를 때 이것은 또한 가능한 가장 빠른 시간입니다.</dd>
          <dd>currentTime은 반환되기 전에 반드시 control thread에서 atomically하게 read되어야만 합니다.</dd>
          <br>
          <dt><i>destination</i> / 자료형: AudioDestinationNode / 읽기 전용</dt>
          <dd>모든 오디오의 최종 목적지를 나타내는 하나의 입력을 가진 AudioDestinationNode. 보통 이것은 실제 오디오 하드웨어를 나타냅니다. 오디오를 현재 렌더링하고 있는 모든 AudioNode는 직접적으로 혹은 간접적으로 목적지에 연결됩니다.</dd>
          <br>
          <dt><i>listener</i> / 자료형: AudioListener / 읽기 전용</dt>
          <dd>3D 공간화에 사용되는 AudioListener</dd>
          <br>
          <dt><i>onstatechange</i> / 자료형: EventHandler</dt>
          <dd>AudioContext의 상태가 변경되었을 때 BaseAudioContext에 전파되는 이벤트에 대한 EventHandler를 설정하기 위해 사용되는 프로퍼티 (예: 해당하는 promise가 resolve되었을 때). Event 유형의 이벤트가 이벤트 핸들러에 전파될 것인데, 이는 AudioContext의 상태를 직접적으로 질의할 수 있습니다. 새롭게 생성된 AudioContext는 항상 suspended 상태에서 시작하고, 언제든지 다른 상태로 상태가 변경될 때 상태 변화 이벤트가 발생될 것입니다. 이 이벤트는 oncomplete 이벤트가 발생되기 이전에 발생됩니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>BaseAudioContext가 오디오를 다루는 (초당 샘플 프레임에서의) 샘플 레이트. 이것은 컨텍스트의 모든 AudioNode가 이 rate에서 실행될 것으로 추정합니다. 이 추정을 내림에 있어, 샘플 레이트 전환기 혹은 "varispeed" 프로세서는 실시간 프로세싱이 지원되지 않습니다. <strong><i>나이퀴스트 주파수</i></strong>는 이 샘플 레이트 값의 절반입니다.</dd>
          <br>
          <dt><i>state</i> / 자료형: AudioContextState / 읽기 전용</dt>
          <dd>BaseAudioContext의 현재 상태를 기술합니다. 이 어트리뷰트를 get하는 것은 [[control thread state]] 슬롯의 내용을 반환합니다.</dd>
        </dl>
        <br>
        <h3>1.1.2. 메서드</h3>
        <dl>
          <dt><i>createAnalyser()</i></dt>
          <dd>AnalyserNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AnalyserNode</dd>
          <br>
          <dt><i>createBiquadFilter()</i></dt>
          <dd>몇몇 일반적인 필터 유형 중 하나로써 설정될 수 있는 second order 필터를 나타내는 BiquadFilterNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: BiquadFilterNode</dd>
          <br>
          <dt><i>createBuffer(numOfChannels, length, sampleRate)</i></dt>
          <dd>주어진 크기의 AudioBuffer를 생성합니다. 버퍼 내의 오디오 데이터는 0으로 초기화될 것입니다 (silent). 만약 인자가 어느 하나라도 음수거나, 0이거나, 명목상의 범위 바깥에 있다면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createBuffer() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 구현은 반드시 적어도 32개의 채널을 지원해야 합니다.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼 크기를 결정합니다. 이것은 반드시 적어도 1이어야 합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 구현은 반드시 적어도 범위 8000에서 96000까지의 샘플 레이트를 지원해야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: AudioBuffer</dd>
          <br>
          <dt><i>createBufferSource()</i></dt>
          <dd>AudioBufferSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioBufferSourceNode</dd>
          <br>
          <dt><i>createChannelMerger(numberOfInputs)</i></dt>
          <dd>channel merger를 나타내는 ChannelMergerNode의 팩토리 메서드. 만약 numberOfInputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelMerger(numberOfInputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfInputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>입력의 수를 결정합니다. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelMergerNode</dd>
          <br>
          <dt><i>channelSplitterNode(numberOfOutputs)</i></dt>
          <dd>channel splitter를 나타내는 ChannelSplitterNode의 팩토리 메서드. 만약 numberOfOutputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelSplitter(numberOfOutputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfOutputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>출력의 수. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelSplitterNode</dd>
          <br>
          <dt><i>createConstantSource()</i></dt>
          <dd>ConstantSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConstantSourceNode</dd>
          <br>
          <dt><i>createConvolver()</i></dt>
          <dd>ConvolverNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConvolverNode</dd>
          <br>
          <dt><i>createDelay(maxDelayTime)</i></dt>
          <dd>DelayNode의 팩토리 메서드. 초기 기본 delay time은 0초입니다.</dd>
          <table>
            <caption>BaseAudioContext.createDelay(maxDelayTime) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>maxDelayTime</td>
              <td>double</td>
              <td>x</td>
              <td>v</td>
              <td>지연선(delay line)을 고려하는 최대 delay time (초) 를 명시합니다. 만약 명시되었다면, 이 값은 반드시 0보다 크고 3분보다 작아야 하며 그렇지 않다면 NotSupportedError 예외가 반드시 발생되어야 합니다. 만약 명시되지 않았다면, 1이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: DelayNode</dd>
          <br>
          <dt><i>createDynamicsCompressor()</i></dt>
          <dd>DynamicsCompressorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: DynamicsCompressorNode</dd>
          <br>
          <dt><i>createGain()</i></dt>
          <dd>GainNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: GainNode</dd>
          <br>
          <dt><i>createIIRFilter(feedforward, feedback)</i></dt>
          <table>
            <caption>BaseAudioContext.createIIRFilter(feedforward, feedback) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>feedforward</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedforward (분자) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 모든 값들이 0이라면, InvalidStateError가 반드시 발생되어야 합니다. 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
            <tr>
              <td>feedback</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedback (분모) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 이 배열의 첫번째 요소가 0이라면, InvalidStateError가 반드시 발생되어야 합니다. 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: IIRFilterNode</dd>
          <br>
          <dt><i>createOscillator()</i></dt>
          <dd>OscillatorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: OscillatorNode</dd>
          <br>
          <dt><i>createPanner()</i></dt>
          <dd>PannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: PannerNode</dd>
          <br>
          <dt><i>createPeriodicWave(real, imag, constraints)</i></dt>
          <dd>PeriodicWave를 생성하기 위한 팩토리 메서드</dd>
          <dd>이 메서드를 호출할 때, 다음의 단계를 실행합니다:</dd>
          <ol>
            <li>만약 real과 imag가 같은 길이가 아니면, IndexSizeError가 반드시 발생되어야 합니다.</li>
            <li>o가 PeriodicWaveOptions 유형의 새로운 객체이게 합니다.</li>
            <li>이 팩토리 메서드에 전달된 real과 imag 파라미터를 각각 o에 같은 이름의 어트리뷰트로 설정합니다.</li>
            <li>o에 disableNormalization 어트리뷰트를 이 팩토리 메서드에 전달된 constraints 어트리뷰트의 disableNormalization 어트리뷰트의 값으로 설정합니다.</li>
            <li>BaseAudioContext에 이 팩토리 메서드가 첫번째 인자로서 호출되었음을 전달하며, 새로운 PeriodicWave p와, o를 생성합니다.</li>
            <li>p를 반환합니다.</li>
          </ol>
          <table>
            <caption>BaseAudioContext.createPeriodicWave() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>real</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>코사인 파라미터의 sequence. 더욱 자세한 설명은 real 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>imag</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>사인 파라미터의 sequence. 더욱 자세한 설명은 imag 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>constraints</td>
              <td>PeriodicWaveConstraints</td>
              <td>x</td>
              <td>v</td>
              <td>만약 주어지지 않았다면, 파형은 정규화(normalize)됩니다. 그렇지 않으면, 파형은 constraints에 의해 주어진 값에 따라 정규화됩니다.</td>
            </tr>
          </table>
          <dd>반환 유형: PeriodicWave</dd>
          <br>
          <dt><i>createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels)</i></dt>
          <dd>ScriptProcessorNode의 팩토리 메서드. 이 메서드는 폐기되었습니다. 왜냐하면 이 메서드는 AudioWorkletNode에 의해 대체되는 것이 의도되었기 때문입니다.</dd>
          <dd>추후 번역</dd>
          <br>
          <dt><i>createStereoPanner()</i></dt>
          <dd>StereoPannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: StereoPannerNode</dd>
          <br>
          <dt><i>createWaveShaper()</i></dt>
          <dd>비선형 왜곡을 나타내는 WaveShaperNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: WaveShaperNode</dd>
          <br>
          <dt><i>decodeAudioData(audioData, successCallback, errorCallback)</i></dt>
          <dd>ArrayBuffer에 포함된 오디오 파일 데이터를 비동기적으로 디코드합니다.</dd>
          <dd>추후 번역</dd>
          <br>
          <h3>1.1.3. 콜백 DecodeSuccessCallback() 파라미터</h3>
          <dl>
            <dt>decodedData / 유형: AudioBuffer</dt>
            <dd>디코드된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
          </dl>
          <br>
          <h3>1.1.4. 콜백 DecodeErrorCallback() 파라미터</h3>
          <dl>
            <dt>error / 유형: DOMException</dt>
            <dd>디코드 도중에 발생한 에러</dd>
          </dl>
        </dl>
        <br>
        <h3>1.1.5. 생애 주기</h3>
        <p>한 번 생성되고 나면, AudioContext는 자신이 재생할 소리가 더 이상 남아있지 않거나, 페이지가 꺼질 때까지 계속 소리를 재생합니다.</p>
        <br>
        <h3>1.1.6. 자기 성찰(introspection) 또는 직렬화 primitives(기본 요소?)의 부족</h3>
        <p>Web Audio API는 오디오 소스 스케쥴링에 사용하고 나서 잊는(fire-and-forget) 접근 방식을 취합니다. 즉, 소스 노드는 AudioContext의 생애 주기 동안 각 노트에 대해 생성되고, 절대로 명시적으로 그래프에서 제거되지 않습니다. 이것은 직렬화 API와 호환되지 않습니다. 왜냐하면 직렬화될 수 있는 노드들의 안정된 집합이 존재하지 않기 때문입니다.</p>
        <p>게다가, 자기 성찰 API를 가지는 것은 content 스크립트가 가비지 컬렉션을 관찰할 수 있게 허용할 것입니다.</p>
        <h3>1.1.7. BaseAudioContext 서브클래스와 연관된 시스템 자원</h3>
        <p>서브클래스 AudioContext와 OfflineAudioContext는 비용이 많이 드는(expensive) 객체로 여겨져야 합니다. 이 객체들은 생성하는 것은 높은 우선도의 스레드를 생성하는 것이나, 낮은 레이턴시의 시스템 오디오 스트림을 사용하는 것을 포함할 수도 있는데, 양측 다 에너지 소모에 영향을 가집니다. 문서에서 하나 이상의 AudioContext를 생성하는 것은 보통 필요하지 않습니다.</p>
        <p>BaseAudioContext 서브클래스를 구성하거나 재개하는 것은 그 컨텍스트에 대해 시스템 자원을 취득하는 것을 포함합니다. AudioContext에 대해, 이것은 또한 시스템 오디오 스트림의 생성을 요구합니다. 이 연산들은 컨텍스트가 컨텍스트의 연관된 오디오 그래프로부터 출력을 생성하는 것을 시작할 때 반환됩니다.</p>
        <p>추가적으로, 유저 에이전트는 구현이 정의된(implementation-defined) AudioContext의 최대 수를 가질 수 있는데, 이는 새로운 AudioContext를 생성하려는 시도가 실패한 이후에, NotSupportedError를 발생시킵니다.</p>
        <p>suspend와 close는 작성자로 하여금 시스템 자원을 해방하게 할 수 있는데, 이는 스레드, 프로세스, 오디오 스트림을 포함합니다. BaseAudioContext를 연기(suspend)하는 것은 BaseAudioContext의 일부 자원을 해방하는 구현을 허용하고, BaseAudioContext가 이후에 resume을 호출함으로써 작동을 계속하도록 허락합니다. AudioContext를 닫는(close)것은 AudioContext의 모든 자원을 해방하는 구현을 허락하는데, 이는 AudioContext가 사용될 수 없거나 다시 재개될 수 없는 이후입니다.</p>
        <div class="note">
          <p>노트: 예를 들어, 이것은 audio callback이 정기적으로 발생되는 것을 기다리거나, 하드웨어가 프로세싱을 위해 준비되기를 기다리는 것을 포함할 수 있습니다.</p>
        </div>
        <br>
        <h2>1.2. AudioContext 인터페이스</h2>
        <p>이 인터페이스는 유저에게 향하는 신호를 생산하는 실시간 출력 장치로 라우팅되는 AudioDestinationNode를 가지고 있는 오디오 그래프를 나타냅니다. 대부분의 사용 경우에, 오직 하나의 AudioContext가 문서당 사용됩니다.</p>
        <pre>
enum AudioContextLatencyCategory {
  "balanced",
  "interactive",
  "playback"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>balanced</i>"</td>
              <td>오디오 출력 레이턴시와 파워 소모의 균형을 유지합니다.</td>
            </tr>
            <tr>
              <td>"<i>interactive</i>"</td>
              <td>glitching이 없는 가능한 가장 낮은 오디오 출력 레이턴시를 제공합니다. 이것이 기본값입니다.</td>
            </tr>
            <tr>
              <td>"<i>playback</i>"</td>
              <td>오디오 출력 레이턴시에 대한 중단 없이 일관된 재생을 우선적으로 처리합니다. 파워 소모가 가장 적습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface AudioContext : BaseAudioContext {
  constructor (optional AudioContextOptions contextOptions = {});
  readonly attribute double baseLatency;
  readonly attribute double outputLatency;
  AudioTimestamp getOutputTimestamp ();
  Promise<undefined> resume ();
  Promise<undefined> suspend ();
  Promise<undefined> close ();
  MediaElementAudioSourceNode createMediaElementSource (HTMLMediaElement mediaElement);
  MediaStreamAudioSourceNode createMediaStreamSource (MediaStream mediaStream);
  MediaStreamTrackAudioSourceNode createMediaStreamTrackSource (
    MediaStreamTrack mediaStreamTrack);
  MediaStreamAudioDestinationNode createMediaStreamDestination ();
};
        </pre>
        <p>AudioContext는 user agent가 컨텍스트 상태가 "suspended"에서 "running"으로 전이되는 것을 허용한 경우 <strong>시작되도록 허용</strong>됩니다. user agent는 이 초기 전이를 비허용할 수 있으며 이것을 허용하기 위해서는 AudioContext'의 관련된 전역 객체가 sticky activation을 가지고 있어야만 합니다.</p>
        <p>AudioContext는 내부 슬롯을 가지고 있습니다:</p>
        <dl>
          <dt><i>[[suspended by user]]</i></dt>
          <dd>context가 사용자 코드에 의해 suspended되었는지를 나타내는 boolean flag. 초기값은 false.</dd>
        </dl>
        <br>
        <h3>1.2.1. 생성자</h3>
        <dl>
          <dt><i>AudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단시킵니다.</dd>
          <br>
          <dd>AudioContext를 생성할 때, 아래의 과정들을 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <div class="note">
          <p>노트: 프로그래밍적으로 작성자에게 AudioContext의 생성이 실패했다는 것을 알리는 것이 불행하게도 가능하지 않습니다. 유저 에이전트는 만약 작성자가 로깅 메커니즘에 접근권한을 가지고 있다면 정보 메시지를 로그할 것이 장려됩니다 (예: 개발자 도구 콘솔).</p>
        </div>
        <br>
        <table>
          <caption>AudioContext.constructor(contextOptions) 메서드의 인자</caption>
          <tr>
            <td>매개변수</td>
            <td>자료형</td>
            <td>nullable</td>
            <td>optional</td>
            <td>설명</td>
          </tr>
          <tr>
            <td>contextOptions</td>
            <td>AudioContextOptions</td>
            <td>x</td>
            <td>v</td>
            <td>AudioContext가 어떻게 생성되어야 하는지를 제어하는 유저가 명시한 옵션</td>
          </tr>
        </table>
        <br>
        <h3>1.2.2. 어트리뷰트</h3>
        <dl>
          <dt><i>baseLatency</i> / 자료형: double / 읽기 전용</dt>
          <dd>baseLatency는 AudioDestinationNode에서 audio subsystem으로 오디오를 전달하는 AudioContext에 의해 발생되는 프로세싱 레이턴시의 초를 나타냅니다. 이것은 AudioDestinationNode의 출력과 오디오 하드웨어 사이의 다른 프로세싱에 의해 유발될 수 있는 다른 추가적인 레이턴시를 포함하지 않으며 분명히 오디오 그래프 그 자체에 의해 발생되는 모든 레이턴시를 포함하지 않습니다.</dd>
          <br>
          <dd>예를 들어, 만약 오디오 컨텍스트가 44.1 kHz로 실행되고 AudioDestinationNode가 내부적으로 double buffering을 구현하고 각 render quantum마다 오디오를 처리하고 출력할 수 있다면, 이 경우 프로세싱 레이턴시는 (2 * 128) / 44100 = 5.805ms (근사치) 입니다.</dd>
          <br>
          <dt><i>outputLatency</i> / 자료형: double / 읽기 전용 / 현재 파이어폭스 엔진에만 있음</dt>
          <dd>오디오 출력 레이턴시의 추정 (단위: 초). 예: UA가 호스트 시스템에게 버퍼를 재생하는 것과 버퍼 내의 첫번째 샘플이 실제로 오디오 출력 장치에 의해 처리되는 시간을 요구하는 것. 음향 시그널을 생산하는 스피커와 헤드폰과 같은 장치에 대해서, 이 후자의 시간은 샘플의 소리가 생산될 때의 시간을 나타냅니다.</dd>
          <br>
          <dd>outputLatency 어트리뷰트 값은 플랫폼과 연결된 하드웨어 오디오 출력 장치에 달려 있습니다. outputLatency 어트리뷰트 값은 연결된 오디오 출력 장치가 동일한 이상 컨텍스트의 생명 주기에 대해 변하지 않습니다. 만약 오디오 출력 장치가 변경되었다면 outputLatency 어트리뷰트 값은 그에 맞추어 갱신될 것입니다.</dd>
        </dl>
        <br>
        <h3>1.2.3. 메서드</h3>
        <dl>
          <dt><i>close()</i></dt>
          <dd>사용되고 있던 시스템 자원을 해방하며 AudioContext를 닫습니다. 이것은 자동적으로 AudioContext가 생성한 모든 객체를를 해방하지 않을 것이지만, AudioContext의 currentTime의 진행을 유예할 것이고, 오디오 데이터를 프로세싱하는 것을 멈춥니다.</dd>
          <br>
          <dd>close가 호출되었을 때, 아래의 단계를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <br>
          <div class="note">
            <p>AudioContext가 closed되었을 때, 구현은 suspending되었을 때보다 더 공격적으로 자원을 해방하는 것을 선택할 수 있습니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>createMediaElementSource(mediaElement)</i></dt>
          <dd>주어진 HTMLMediaElement로 MediaElementAudioSourceNode를 생성합니다. 이 메서드를 호출한 결과로서, HTMLMediaElement로부터의 오디오 재생이 AudioContext의 프로세싱 그래프 내로 재라우팅(re-route)될 것입니다.</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaElementSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaElement</td>
              <td>HTMLMediaElement</td>
              <td>x</td>
              <td>x</td>
              <td>재라우팅될 미디어 요소</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaElementAudioSourceNode</dd>
          <br>
          <dt><i>createMediaStreamDestination</i></dt>
          <dd>MediaStreamAudioDestinationNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: MediaStreamAudiodestinationNode</dd>
          <br>
          <dt><i>createMediaStreamSource(mediaStream)</i></dt>
          <dd>MediaStreamAudioSourceNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaStreamSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaStream</td>
              <td>MediaStream</td>
              <td>x</td>
              <td>x</td>
              <td>소스(source)로서 작동할 미디어 스트림.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaStreamAudioSourceNode</dd>
          <br>
          <dt><i>getOutputTimestamp()</i></dt>
          <dd>컨텍스트에 대해 두 개의 연관된 오디오 스트림 포지션 값을 포함하고 있는 새로운 AudioTimeStamp 인스턴스를 반환합니다: contextTime 멤버는 (컨텍스트의 currentTime과 같은 단위와 기원에서) 오디오 출력 장치에 의해 현재 렌더링되고 있는 샘플 프레임의 시간 (예: 출력 오디오 스트림 포지션)  입니다; performanceTime 멤버는 (performance.now()와 같은 단위와 기원에서) 저장된 contextTime에 해당하는 샘플 프레임이 오디오 출력 장치에 의해 렌더링된 순간을 추산하는 시간을 포함합니다. (이는 [hr-time-3] 에서 기술됨).</dd>
          <br>
          <dd>만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, getOutputTimestamp 호출은 두 멤버가 모두 0을 포함하고 있는 AudioTimestamp 인스턴스를 반환합니다.</dd>
          <br>
          <dd>컨텍스트의 렌더링 그래프가 오디오 블럭의 처리를 시작한 이후에, 이것의 currentTime 어트리뷰트 값은 항상 getOutputTimestamp 메서드 호출로 인해 얻어진 contextTime 값을 넘어섭니다.</dd>
          <br>
          <pre>예제 4
getOutputTimestamp 메서드로부터 반환된 값은 컨텍스트의 시간 값보다 살짝 후의 퍼포먼스 시간 추산을 얻기 위해 사용될 수 있습니다:

function outputPerformanceTime(contextTime) {
  const timestamp = context.getOutputTimestamp();
  const elapsedTime = contextTime - timestamp.contextTime;
  return timestamp.performanceTime + elapsedTime * 1000;
}

상기의 예제에서 추산의 정확도는 인자 값이 얼마나 현재 출력 오디오 스트림 포지션에 가까운지에 달려 있습니다: 주어진 contextTime이 timestamp.contextTime에 가까울수록, 얻어지는 추산의 정확도는 좋아집니다.
          </pre>
          <br>
          <div class="note">
            <p>노트: 컨텍스트의 currentTime과 getOutputTimestamp 메서드 호출로부터 얻어진 contextTime의 차이는 신용 가능한 출력 레이턴시 추산으로써 여겨질 수 없습니다. 왜냐하면 currentTime은 비균일한 시간 구간에서 증가될 수 있으므로, outputLatency 어트리뷰트가 대신 사용되어야 합니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioTimestamp</dd>
          <br>
          <dt><i>resume()</i></dt>
          <dd>AudioContext가 연기되었을 때 AudioContext의 currentTime의 진행을 재개합니다.</dd>
          <dd>resume이 호출되었을 때, 아래의 절차를 실행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>suspend()</i></dt>
          <dd>AudioContext의 currentTime의 진행을 연기하고, 이미 처리된 모든 현재의 컨텍스트 프로세싱 블럭들이 목적지에서 재생되는 것을 허용하고, 그리고 나서 시스템이 오디오 하드웨어에 대한 주장(claim)을 해방하도록 허용합니다. 이것을 일반적으로 어플리케이션이 AudioContext가 어떤 때는 필요가 없을 것이라는 것을 알고, 일시적으로 AudioContext에 연관된 시스템 자원을 해방하기를 소원할 때 유용합니다. 이 promise는 프레임 버퍼가 비었을 때 (프레임 버퍼가 하드웨어에서 손을 뗐을 때), 혹은 일시적으로 (다른 효과 없이) 만약 컨텍스트가 이미 연기되었을 경우 resolve합니다. 이 promise는 컨텍스트가 closed되었을 때 rejected됩니다.</dd>
          <br>
          <dd>suspend가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <h3>1.2.4. AudioContextOptions</h3>
        <p>AudioContextOptions 딕셔너리는 AudioContext에 대해 사용자가 정의한 옵션들을 명시하기 위해 사용됩니다.</p>
        <br>
        <pre>
dictionary AudioContextOptions {
  (AudioContextLatencyCategory or double) latencyHint = "interactive";
  float sampleRate;
};
        </pre>
        <br>
        <h4>1.2.4.1. 딕셔너리 AudioContextOptions 멤버</h4>
        <dl>
          <dt><i>latencyHint</i> / 자료형: (AudioContextLatencyCategory 또는 double) / 기본값: "interactive"</dt>
          <dd>재생의 유형을 명시하는데, 이는 오디오 출력 레이턴시와 파워 소모 사이의 균형(tradeoff)에 영향을 미칩니다.</dd>
          <br>
          <dd>latencyHint의 선호된 값은 AudioContextLatencyCategory로부터의 값입니다. 그러나, double 또한 레이턴시와 파워 소모를 균형잡기 위한 더 좋은 제어를 위한 레이턴시 (단위: 초) 를 위해 명시될 수 있습니다. 이 숫자를 적절히 해석하는 것은 브라우저의 재량에 있습니다. 사용되는 실제 레이턴시는 AudioContext의 baseLatenct 어트리뷰트에 의해 주어집니다.</dd>
          <br>
          <dt><i>sampleRate / 자료형: float</i></dt>
          <dd>생성될 AudioContext에 대해 이 값에 sampleRate를 설정합니다. 이 지원된 값들은 AudioBuffer의 샘플 레이트와 같습니다. 만약 명시된 샘플 레이트가 지원되지 않는다면 NotSupportedError 예외가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>만약 sampleRate가 명시되지 않았다면, 이 AudioContext의 출력 장치의 선호되는 샘플 레이트가 사용됩니다.</dd>
        </dl>
        <br>
        <h3>1.2.5. AudioTimestamp</h3>
        <pre>
dictionary AudioTimestamp {
  double contextTime;
  DOMHighResTimeStamp performanceTime;
};
        </pre>
        <br>
        <h4>1.2.5.1. 딕셔너리 AudioTimestamp 멤버</h4>
        <dl>
          <dt><i>contextTime</i> / 자료형: double</dt>
          <dd>BaseAudioContext의 currentTime의 시간 좌표계의 점(point)를 나타냅니다.</dd>
          <br>
          <dt><i>performanceTime</i> / 자료형: DOMHighResTimeStamp</dt>
          <dd>퍼포먼스 인터페이스 구현의 시간 좌표계의 점을 나타냅니다 ([hr-time-3]에서 기술됩니다).</dd>
        </dl>
        <br>
        <h2>1.3. OfflineAudioContext 인터페이스</h2>
        <p>OfflineAudioContext는 실시간보다 (잠재적으로) 렌더링/믹싱다운을 더 빠르게 하기 위한 BaseAudioContext의 특정한 유형입니다. 이것은 오디오 하드웨어로 렌더링하지 않지만, 대신 가능한 한 빨리 렌더링하는데, AudioBuffer로 렌더링된 결과와 함께 반환된 promise를 fulfill합니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioContext : BaseAudioContext {
  constructor(OfflineAudioContextOptions contextOptions);
  constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate);
  Promise<AudioBuffer> startRendering();
  Promise<undefined> resume();
  Promise<undefined> suspend(double suspendTime);
  readonly attribute unsigned long length;
  attribute EventHandler oncomplete;
};          
        </pre>
        <br>
        <h3>1.3.1. 생성자</h3>
        <dl>
          <dt><i>OfflineAudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단합니다.</dd>
          <br>
          <dd>c를 새로운 OfflineAudioContext 객체라고 합니다. c를 다음과 같이 초기화합니다:</dd>
          <ol>
            <li>c에 대해 [[control thread state]]를 "suspended"로 설정합니다.</li>
            <li>c에 대해 [[rendering thread state]]를 "suspended"로 설정합니다.</li>
            <li>AudioDestinationNode의 channelCount가 contextOptions.numberOfChannels로 설정된 채로 AudioDestinationNode를 생성합니다.</li>
          </ol>
          <br>
          <table>
            <caption>OfflineAudioContext.constructor(contextOptions) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>contextOptions</td>
              <td></td>
              <td></td>
              <td></td>
              <td>이 컨텍스트를 생성하는 데 필요한 초기 파라미터들.</td>
            </tr>
          </table>
          <br>
          <dt><i>OfflineAudioContext(numberOfChannels, length, sampleRate)</i></dt>
          <dd>OfflineAudioContext는 AudioContext.createBuffer와 같은 인자로 생성될 수 있습니다. 만약 인자 중 하나라도 음수거나, 0이거나, 명목상의 범위 밖이면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>OfflineAudioContext는 아래가 대신 호출된 것처럼 생성됩니다.</dd>
          <br>
          <pre>
new OfflineAudioContext({
  numberOfChannels: numberOfChannels,
  length: length,
  sampleRate: sampleRate
})
          </pre>
          <table>
            <caption>OfflineAudioContext.constructor(numberOfChannels, length, sampleRate) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 지원되는 채널의 수는 createBuffer()를 참고하세요.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼의 크기를 결정합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼 내의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 유효한 샘플 레이트에 대해서는 createBuffer()를 참고하세요.</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.3.2. 어트리뷰트</h3>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임 내의 버퍼의 크기. 이것은 생성자의 length 파라미터의 값과 같습니다.</dd>
          <br>
          <dt><i>oncomplete</i> / 자료형: EventHandler</dt>
          <dd>OfflineAudioCompletionEvent 유형의 EventHandler. 이것은 OfflineAudioContext에서 발생되는 마지막 이벤트입니다.</dd>
        </dl>
        <br>
        <h3>1.3.3. 메서드</h3>
        <dl>
          <dt><i>startRendering()</i></dt>
          <dd>주어진 현재 연결과 예정된 변화를 가지고, 오디오 렌더링을 시작합니다.</dd>
          <br>
          <dd>비록 렌더링된 오디오 데이터를 얻는 일차적인 방법은 이것의 promise 반환 값을 통해서이지만, 이 인스턴스는 또한 레거시의 이유로 complete라는 이름의 이벤트를 발생시킬 것입니다.</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <br>
          <dt><i>resume()</i> / 현재 크롬 엔진에만 있음</dt>
          <dd>OfflineAudioContext의 currentTime의 진행을 OfflineAudioContext가 suspended되었을 때 재개시킵니다.</dd>
          <br>
          <dd>이 메서드가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <dt><i>suspend(suspendTime)</i></dt>
          <dd>특정한 시간에 오디오 컨텍스트에서의 시간 진행의 연기를 예정하고 promise를 반환합니다. 이거슨 일반적으로 OfflineAudioContext에서 오디오 그래프를 동기적으로(synchronously) 조작할 때 유용합니다.</dd>
          <br>
          <dd>연기의 최대 정밀도는 render quantum의 크기이고 명시된 연기 시간은 가장 가까운 render quantum 경계선(boundary) 까지에서 반올림될 것입니다. 이러한 이유로, 다수의 연기를 같은 quantized 프레임에 예정하는 것은 허용되지 않습니다. 또한, 정밀한 연기를 보장하기 위해 스케쥴링은 컨텍스트가 실행 중(running)이 아닌 동안에 이루어져야 합니다.</dd>
          <br>
          <table>
            <caption>OfflineAudioContext.suspend() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>suspendTime</td>
              <td>double</td>
              <td>x</td>
              <td>x</td>
              <td>
                명시된 시간에 렌더링의 연기를 예정하는데, 이는 quantized되고 render quantum 크기까지 반올림될 것입니다. 만약 quantized 프레임 수가
                <ol>
                  <li>음수거나</li>
                  <li>current time보다 작거나 혹은 같거나</li>
                  <li>전체 render duration보다 크거나 혹은 같거나</li>
                  <li>같은 시간에 또 다른 suspend가 예정되어 있다면</li>
                </ol>
                promise는 InvalidStateError와 함께 rejected됩니다.
              </td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
        </dl>
        <br>
        <h3>1.3.4. OfflineAudioContextOptions</h3>
        <p>이것은 OfflineAudioContext를 생성하는 데 쓰이는 옵션들을 명시합니다.</p>
        <pre>
dictionary OfflineAudioContextOptions {
  unsigned long numberOfChannels = 1;
  required unsigned long length;
  required float sampleRate;
};
        </pre>
        <h4>1.3.4.1. 딕셔너리 OfflineAudioContextOptions 멤버</h4>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long</dt>
          <dd>샘플 프레임에서의 렌더링된 AudioBuffer의 길이</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 기본값: 1</dt>
          <dd>이 OfflineAudioContext의 채널의 수.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float</dt>
          <dd>이 OfflineAudioContext의 샘플 레이트</dd>
        </dl>
        <br>
        <h3>1.3.5. OfflineAudioCompletionEvent 인터페이스</h3>
        <p>이것은 레거시적 이유로 OfflineAudioContext에 전파되는 Event 객체입니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioCompletionEvent : Event {
  constructor (DOMString type, OfflineAudioCompletionEventInit eventInitDict);
  readonly attribute AudioBuffer renderedBuffer;
};
        </pre>
        <h4>1.3.5.1. 어트리뷰트</h4>
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer / 읽기 전용</i></dt>
          <dd>렌더링된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
        </dl>
        <br>
        <h4>1.3.5.2. OfflineAudioCompletionEventInit</h4>
        <br>
        <pre>
dictionary OfflineAudioCompletionEventInit : EventInit {
  required AudioBuffer renderedBuffer;
};
        </pre>
        <h5>1.3.5.2.1. 딕셔너리 OfflineAudioCompletionEventInit 멤버</h5>        
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer</i></dt>
          <dd>이 이벤트의 renderedBuffer 어트리뷰트에 할당될 값</dd>
        </dl>
        <br>
        <h2>1.4. AudioBuffer 인터페이스</h2>
        <p>이 인터페이스는 메모리에 상주하는 오디오 에셋을 나타냅니다. 이것은 하나 이상의 채널을 포함할 수 있는데 각 채널은 [-1, 1]의 명목상의 범위를 가지는 32비트 부동 소수점 선형 PCM 값을 가지고 있는 것으로 보이나 이 값들은 이 범위에 국한되지 않습니다. 통상적으로, PCM 데이터의 길이는 꽤 짧을 것이 기대됩니다 (보통 1분보다 짧은 정도). 음악 사운드트랙같은 더 긴 소리들에 대해서는, 스트리밍은 audio 요소와 MediaElementAudioSourceNode와 함께 사용되어야 합니다.</p>
        <br>
        <p>AudioBuffer는 하나 이상의 AudioContext에 의해 사용될 수 있고, OfflineAudioContext와 AudioContext 사이에서 공유될 수 있습니다.</p>
        <br>
        <p>AudioBuffer는 4개의 내부 슬롯을 갖습니다.</p>
        <dl>
          <dt><i>[[number of channels]]</i></dt>
          <dd>이 AudioBuffer의 오디오 채널의 수. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[length]]</i></dt>
          <dd>이 AudioBuffer의 각 채널의 길이. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[sample rate]]</i></dt>
          <dd>이 AudioBuffer의 샘플 레이트 (단위: Hz). / 자료형: float</dd>
          <br>
          <dt><i>[[internal data]]</i></dt>
          <dd>오디오 샘플 데이터를 가지고 있는 데이터 블럭.</dd>
        </dl>
        <br>
        <pre>
[Exposed=Window]
interface AudioBuffer {
  constructor (AudioBufferOptions options);
  readonly attribute float sampleRate;
  readonly attribute unsigned long length;
  readonly attribute double duration;
  readonly attribute unsigned long numberOfChannels;
  Float32Array getChannelData (unsigned long channel);
  undefined copyFromChannel (Float32Array destination,
                             unsigned long channelNumber,
                             optional unsigned long bufferOffset = 0);
  undefined copyToChannel (Float32Array source,
                           unsigned long channelNumber,
                           optional unsigned long bufferOffset = 0);
};
        </pre>
        <br>
        <h3>1.4.1. 생성자</h3>
        <dl>
          <dt><i>AudioBuffer(options)</i></dt>
          <dd>추후 번역</dd>
          <br>
          <table>
            <caption>AudioBuffer.constructor() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>options</td>
              <td>AudioBufferOptions</td>
              <td>x</td>
              <td>x</td>
              <td>이 AudioBuffer의 프로퍼티를 결정하는 AudioBufferOptions</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.4.2. 어트리뷰트</h3>
        <dl>
          <dt><i>duration</i> / 자료형: double / 읽기 전용</dt>
          <dd>PCM 오디오 데이터의 지속 기간 (단위: 초)</dd>
          <br>
          <dd>이것은 [[length]]와 [[sample rate]] 사이에서 나눗셈을 수행함으로써 AudioBuffer의 [[sample rate]]와 [[length]]로부터 계산됩니다.</dd>
          <br>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임의 PCM 오디오 데이터의 길이. 이것은 반드시 [[length]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>이산 오디오 채널의 수. 이것은 반드시 [[number of channels]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>초당 샘플 내의 PCM 오디오 데이터의 샘플 레이트. 이것은 반드시 [[sample rate]]의 값을 반환해야만 합니다.</dd>
        </dl>
        <br>
        <h3>1.4.3. 메서드</h3>
        <p>추후 번역</p>
        <br>
        <h3>1.4.4. AudioBufferOptions</h3>
        <p>추후 번역</p>
        <br>
        <h4>1.4.4.1. 딕셔너리 AudioBufferOptions 멤버</h4>
        <p>추후 번역</p>
        <br>
        <h2>1.5. AudioNode 인터페이스</h2>
        <p>AudioNode는 AudioContext의 구성 요소입니다. 이 인터페이스는 오디오 소스, 오디오 목적지, 중간 프로세싱 모듈을 나타냅니다. 이 모듈들은 오디오를 렌더링하여 오디오 하드웨어로 보내는 프로세싱 그래프를 형성하기 위해 함께 연결될 수 있습니다. 각 노드들은 입력 그리고/또는 출력을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력이 있습니다. 필터와 같은 대부분의 프로세싱 노드들은 하나의 입력과 하나의 출력을 갖습니다. 각 유형의 AudioNode는 어떻게 이것이 오디오를 처리하거나 합성하는지에 대한 세부 사항이 다릅니다. 하지만, 일반적으로, AudioNode는 (입력이 있다면) 입력을 처리하고, (출력이 있다면) 출력으로 오디오를 생성합니다.</p>
        <br>

        <h2>1.6. AudioParam 인터페이스</h2>
        <p>AudioParam은 0개 이상의 <strong>자동화 이벤트</strong>를 유지합니다. 각 자동화 이벤트는, AudioContext의 currentTime 어트리뷰트의 시간 좌표계의 <strong>자동화 이벤트 시간</strong>에 관련하여, 특정한 시간 범위에 대한 파라미터의 값 변화를 명시합니다. 자동화 이벤트의 리스트는 자동화 이벤트 시간의 오름차순으로 유지됩니다.</p>
        <p>주어진 자동화 이벤트의 행동은 이 이벤트의 자동화 이벤트 시간과 리스트의 인접한 이벤트의 자동화 이벤트 뿐만이 아니라, AudioContext의 현재 시간의 함수입니다. 다음의 <strong>자동화 메서드</strong>들은 이벤트 리스트에 메서드에 따른 새로운 이벤트를 추가함으로써 이벤트 리스트를 변경시킵니다.</p>
        <ul>
          <li>setValueAtTime() - setValue</li>
          <li>linearRampToValueAtTime() - LinearRampToValue</li>
          <li>exponentialRampToValueAtTime() - exponentialRampToValue</li>
          <li>setTargetAtTime() - setTarget</li>
          <li>setValueCurveAtTime() - setValueCurve</li>
        </ul>
        <p>다음의 규칙이 이 메서드들을 호출할 때 적용될 것입니다:</p>
        <ul>
          <li>자동화 이벤트 시간은 일반적인 샘플 레이트에 관하여 양자화되지 않습니다. 곡선과 경사를 결정하는 공식은 이벤트를 예정할 때 주어진 정확한 수치적 시간에 적용됩니다.</li>
          <li>만약 이 이벤트들 중 하나가 이미 하나 이상의 이벤트가 있는 시간에 추가되었다면, 이 이벤트는 그 이벤트들 이후에 리스트에 배치될 것이지만, 시간이 그 이벤트 이후에 있는 이벤트 전입니다.</li>
          <li>만약 setValueCurveAtTime()이 시간 T와 기간 D로 호출되고 T보다 엄격하게 큰 시간을 가지지만, T+D보다는 엄격히 작은 어떠한 이벤트가 있다면, NotSupportedError 예외가 반드시 발생되어야 합니다. 요컨대, 다른 이벤트를 포함하고 있는 시간 기간 동안에 value curve를 예정하는 것은 괜찮지 않지만, 다른 이벤트의 시간에 정확히 value curve를 예정하는 것은 괜찮습니다.</li>
          <li>유사하게 만약 [T, T+D)에 포함된 시간에 자동화 메서드가 호출되었다면 NotSupportedError가 반드시 발생되어야만 합니다 (T는 curve의 시간이고 D는 기간임).</li>
        </ul>
        <br>
        <div class="note">
          <p>노트: AudioParam 어트리뷰트는 읽기 전용입니다. 단, value 어트리뷰트는 예외입니다.</p>
        </div>
    </div>
    </main>
  </body>
</html>
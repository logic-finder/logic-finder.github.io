<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Web Audio API Spec Korean Translation</title>
    <link rel="stylesheet" href="index.css">
  </head>
  <body>
    <header>
      <p><a href="../index.html">return to home</a></p>
      <br>
      <p id="title">W3C Web Audio API Specification Korean Translation</p>
      <p class="description">It would be appreciated if you note that this translation is <strong>unofficial</strong>; it is just for the pure sake of my studying Web Audio API.</p>
      <p class="description">이 번역이 <strong>비공식</strong>임에 주목해주신다면 감사하겠습니다. 이 번역은 단지 순수하게 제가 Web Audio API를 공부하기 위한 것입니다.</p>
      <a href="https://www.w3.org/TR/webaudio/">link to the original document</a>
      <p>notice: currently on work: 21.11.18 ~ </p>
    </header>
    <hr>
    <main>
      <div id="frame">
        <h1>개요</h1>
        <p>이 명세는 웹 애플리케이션에서 오디오를 프로세싱하고 합성하기 위한 고수준의 WEB API를 기술합니다. 이 API의 주요한 패러다임은 오디오 라우팅 그래프에 관한 것인데, 오디오 라우팅 그래프란 다수의 AudioNode 객체들이 함께 연결되어 전반적인 오디오 렌더링을 정의하는 것입니다. 실제적인 프로세싱은 근본적인 구현 (보통 최적화된 Assembly / C / C++ 코드) 에서 주로 일어나지만, 직접적인 스크립트 프로세싱과 합성 또한 지원됩니다.</p>
        <p>아래의 '소개' 절은 이 명세의 동기를 다룹니다.</p>
        <p>이 API는 웹 플랫폼상의 다른 API 및 요소와 함께 사용되도록 설계되었습니다. 주요한 것으로는 XMLHttpRequest [XHR] 이 있습니다 (responseType과 response 어트리뷰트를 사용). 게임과 상호작용이 일어나는 어플리케이션에 대해서는, canvas 2D [2dcontext] 와 WebGL [WEBGL] 3D 그래픽 API와 함께 사용되는 것이 기대됩니다.</p>
        <h1>이 문서의 상태</h1>
        <p>생략</p>
        <h1>소개</h1>
        <p>웹에서의 오디오는 이 시점까지 상당히 원시적이었고 아주 최근까지는 Flash나 QuickTime과 같은 플러그인을 통해 전송되었습니다. HTML5에서의 audio 요소의 소개는 아주 중요했는데, 이는 기본적인 스트리밍 오디오 재생을 가능케 했습니다. 그러나, 더욱 복잡한 오디오 애플리케이션을 다루기에는 충분히 강력하지 못했습니다. 정교한 웹 기반의 게임과 상호작용이 일어나는 애플리케이션에 대해서, 또 다른 해결책이 요구되었습니다. 현대의 데스크톱 오디오 프로덕션 애플리케이션에서 발견되는 몇몇 믹싱, 프로세싱, 필터링 태스크에 더불어 현대 게임의 오디오 엔진에서 발견되는 능력들을 포함하는 것이 이 명세의 목표입니다.</p>
        <p>Web Audio API는 사용 경우 <a href="https://www.w3.org/TR/webaudio-usecases/" target="_blank">[webaudio-usecases]</a> 의 넓은 다양성을 염두에 두고 설계되었습니다. 이상적으로, 이 API는 스크립트를 통해 제어되고 브라우저에서 실행되는 최적화된 C++ 엔진으로 적절하게 구현될 수 있는 모든 사용 경우를 지원할 수 있어야 합니다. 그렇긴 하지만, 현대의 데스크톱 오디오 소프트웨어는 아주 발전된 능력을 가지고 있어서, 그것들 중 몇몇은 이 시스템으로 개발하기 어렵거나 불가능할지도 모릅니다. Apple의 Logic Audio가 그런 애플리케이션 중 하나인데, 이 애플리케이션은 외부 MIDI 컨트롤러, 임의 플러그인 오디오 이펙트, 신디사이저, 대단히 최적화된 직접 녹음형(direct-to-disk) 오디오 파일 읽기/쓰기, 단단히 통합된 time-stretching 등등을 지원합니다. 그럼에도 불구하고, 이 제안된 시스템은 적절히 복잡한 게임과 음악적인 애플리케이션을 포함하는 상호작용이 일어나는 애플리케이션의 넓은 범위를 꽤 지원할 수 있을 것입니다. 그리고 이 API는 WebGL에 의해 제공되는 아주 발전된 그래픽 기능의 아주 훌륭한 보완물이 될 수 있습니다. 이 API는 더욱 발전된 능력이 추후에 더해질 수 있도록 설계되었습니다.</p>
        <h2>기능</h2>
        <p>Web Audio API는 아래의 주요한 기능들을 지원합니다.</p>
        <ul>
          <li>간단하거나 복잡한 믹싱/이펙트 아키텍처를 위한 모듈러 라우팅</li>
          <li>높은 dynamic 범위. 내부 프로세싱에서 32비트 float을 사용.</li>
          <li>드럼 머신과 시퀸서와 같은 아주 높은 정도의 리듬 정밀도를 요구하는 음악 애플리케이션을 위한 낮은 레이턴시를 가지는 sample-accurate scheduled 사운드 재생. 이 기능은 또한 이펙트의 동적 생성의 가능성을 포함합니다.</li>
          <li>엔벨로프, 페이드인/페이드아웃, granular 이펙트, 필터 sweep, LFO 등을 위한 오디오 파라미터의 automation</li>
          <li>오디오 스트림에서의 유연한 채널 핸들링. 채널이 나눠지거나 합쳐질 수 있게 함.</li>
          <li>&lt;audio&gt; 또는 &lt;video&gt; 미디어 요소로부터의 오디오 소스 프로세싱
            <ul>
              <li>MediaStreamTrackAudioSourceNode와 [webrtc]를 사용한 원격 peer로부터 받은 오디오 프로세싱</li>
              <li>MediaStreamAudioDestinationNode와 [webrtc]를 사용해 원격 peer로 생성되거나 프로세싱된 오디오 스트림 전송</li>
            </ul>
          </li>
          <li>스크립트를 사용한 직접적 오디오 스트림 합성과 프로세싱</li>
          <li>3D 게임과 immersive한 환경의 넓은 범위를 지원하는 공간화된(spatialized) 오디오
            <ul>
              <li>패닝 모델: equalpower, HRTF, pass-through</li>
              <li>거리 감쇠(attenuation)</li>
              <li>sound cone</li>
              <li>방해/폐쇄 (obstuction/occlusion)</li>
              <li>음원(source)/청자(listener) 기반</li>
            </ul>
          </li>
          <li>선형 이펙트의 넓은 범위를, 특히 아주 높은 품질의 room 이펙트를 위한 convolution 엔진. 아래는 가능한 효과의 몇 가지 예입니다.
            <ul>
              <li>작은/큰 방(room)</li>
              <li>대성당(cathedral)</li>
              <li>콘서트 홀</li>
              <li>동굴(cave)</li>
              <li>터널(tunnel)</li>
              <li>복도(hallway)</li>
              <li>숲(forest)</li>
              <li>원형 극장(amphitheater)</li>
              <li>출입구를 거치는 멀리 있는 방으로부터의 소리</li>
              <li>extreme 필터</li>
              <li>기묘한 backward 이펙트</li>
              <li>extreme 빗(comb) 필터 이펙트</li>
            </ul>
          </li>
          <li>믹스의 전반적인 제어와 sweetening을 위한 dynamics compression</li>
          <li>효율적인 실시간 시간 영역(time-domain) 그리고 주파수 영역(frequency-domain) 분석 / 음악 시각화 지원</li>
          <li>lowpass, highpass, 그리고 다른 일반적인 필터를 위한 효율적인 biquad 필터</li>
          <li>distortion을 위한 waveshaping 효과와 다른 비선형적 효과</li>
          <li>오실레이터(oscilator)</li>
        </ul>
        <h3>모듈러 라우팅</h3>
        <p>모듈러 라우팅은 다른 AudioNode 객체 사이의 임의적인 연결을 가능케 합니다. 각 노드는 입력과 출력 양 쪽 모두 또는 어느 한 쪽을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력만을 가지고 있습니다. 목적지(destination) 노드는 하나의 입력을 가지고 출력이 없는 노드입니다. 필터와 같은 다른 노드들은 소스 노드와 목적지 노드 사이에 놓일 수 있습니다. 개발자는 두 객체가 함께 연결될 때의 저수준의 스트림 포맷 디테일에 대해 걱정할 필요가 없습니다. 옳은 일이 단지 일어나니까요. 예를 들자면, 만약 모노 오디오 스트림이 스테레오 입력에 연결되었다면 API는 단지 좌측 채널과 우측 채널을 적절히 믹스할 것입니다.</p>
        <p>가장 단순한 경우, 하나의 소스가 바로 출력으로 route될 수 있습니다. 모든 라우팅은 하나의 AudioDestinationNode를 포함하고 있는 AudioContext내에서 발생합니다.</p>
        <figure>
          <img src="img/modular-routing1.png" width="305" height="128">
          <figcaption><strong>도식 1</strong> 모듈러 라우팅의 단순한 예제</figcaption>
        </figure>
        <p>위 그림은 이 단순한 라우팅을 보여줍니다. 아래는 하나의 사운드를 재생하는 간단한 예제입니다.</p>
        <pre>예제 1
const context = new AudioContext();

function playSound() {
  const source = context.createBufferSource();
  source.buffer = dogBarkingBuffer;
  source.connect(context.destination);
  source.start(0);
}
        </pre>
        <p>아래는 세 개의 소스와 최종 출력 스테이지에서 dynamics compressor로 보내지는 convolution reverb를 보여주는 더욱 복잡한 예시입니다.</p>
        <figure>
          <img src="img/modular-routing2.png" width="561" height="411">
          <figcaption><strong>도식 2</strong> 모듈러 라우팅의 더욱 복잡한 예제</figcaption>
        </figure>
        <pre>예제 2
let context;
let compressor;
let reverb;
let source1, source2, source3;
let lowpassFilter;
let waveShaper;
let panner;
let dry1, dry2, dry3;
let wet1, wet2, wet3;
let mainDry;
let mainWet;
function setupRoutingGraph () {
  context = new AudioContext();
  // 이펙트 노드를 생성합니다.
  lowpassFilter = context.createBiquadFilter();
  waveShaper = context.createWaveShaper();
  panner = context.createPanner();
  compressor = context.createDynamicsCompressor();
  reverb = context.createConvolver();
  // main wet과 dry를 생성합니다.
  mainDry = context.createGain();
  mainWet = context.createGain();
  // 마지막 compressor를 마지막 목적지에 연결합니다.
  compressor.connect(context.destination);
  // main dry와 wet을 compressor에 연결합니다.
  mainDry.connect(compressor);
  mainWet.connect(compressor);
  // reverb를 main wet에 연결합니다.
  reverb.connect(mainWet);
  // 몇 가지 소스를 생성합니다.
  source1 = context.createBufferSource();
  source2 = context.createBufferSource();
  source3 = context.createOscillator();
  source1.buffer = manTalkingBuffer;
  source2.buffer = footstepsBuffer;
  source3.frequency.value = 440;
  // source1을 연결합니다.
  dry1 = context.createGain();
  wet1 = context.createGain();
  source1.connect(lowpassFilter);
  lowpassFilter.connect(dry1);
  lowpassFilter.connect(wet1);
  dry1.connect(mainDry);
  wet1.connect(reverb);
  // source2를 연결합니다.
  dry2 = context.createGain();
  wet2 = context.createGain();
  source2.connect(waveShaper);
  waveShaper.connect(dry2);
  waveShaper.connect(wet2);
  dry2.connect(mainDry);
  wet2.connect(reverb);
  // source3을 연결합니다.
  dry3 = context.createGain();
  wet3 = context.createGain();
  source3.connect(panner);
  panner.connect(dry3);
  panner.connect(wet3);
  dry3.connect(mainDry);
  wet3.connect(reverb);
  // 소스들을 지금 시작시킵니다.
  source1.start(0);
  source2.start(0);
  source3.start(0);
}
        </pre>
        <p>모듈러 라우팅은 또한 AudioNode들의 출력이 다른 AudioNode의 행동을 제어하는 AudioParam 파라미터에 route될 수 있게 합니다. 아래의 시나리오에서, 노드의 출력은 입력 시그널보다는 조절 시그널로서 행동합니다.</p>
        <figure>
          <img src="img/modular-routing3.png" width="346" height="337">
          <figcaption><strong>도식 3</strong> 하나의 오실레이터가 다른 오실레이터의 주파수를 조절하는 모습을 보여주는 모듈러 라우팅 시나리오</figcaption>
        </figure>
        <pre>예제 3
function setupRoutingGraph() {
  const context = new AudioContext();
  // 조절 시그널을 공급하는 낮은 주파수의 오실레이터를 생성합니다
  const lfo = context.createOscillator();
  lfo.frequency.value = 1.0;
  // 조절될 높은 주파수의 오실레이터를 생성합니다
  const hfo = context.createOscillator();
  hfo.frequency.value = 440.0;
  // gain 노드를 생성하는데 이 노드의 gain은 조절 시그널의 진폭을 결정합니다
  const modulationGain = context.createGain();
  modulationGain.gain.value = 50;
  // 그래프를 설정하고 오실레이터들을 시작시킵니다
  lfo.connect(modulationGain);
  modulationGain.connect(hfo.detune);
  hfo.connect(context.destination);
  hfo.start(0);
  lfo.start(0);
}
        </pre>
        <h2>API 개요</h2>
        <p>정의되는 인터페이스들은 아래와 같습니다.</p>
        <ul>
          <li>AudioContext 인터페이스. 이 인터페이스는 AudioNode간의 연결을 표현하는 오디오 시그널 그래프를 포함합니다.</li>
          <li>AudioNode 인터페이스. 이 인터페이스는 오디오 소스, 오디오 출력, 중간 프로세싱 모듈을 표현합니다. AudioNode는 모듈러 방식으로 동적으로 연결될 수 있습니다. AudioNode는 AudioContext의 컨텍스트 내에서 존재합니다.</li>
          <li>AnalyserNode 인터페이스. 이는 음악 시각화나 다른 시각화 애플리케이션에서의 사용을 위한 AudioNode입니다.</li>
          <li>AudioBuffer 인터페이스. 이는 메모리에 상주하는(memory-resident) 오디오 에셋으로 작업하기 위한 것입니다. 이 오디오 에셋은 한 번의 소리, 혹은 더 긴 오디오 클립을 표현할 수 있습니다.</li>
          <li>AudioBufferSourceNode 인터페이스. 이는 AudioBuffer에서 오디오를 생성하는 AudioNode입니다.</li>
          <li>AudioDestinationNode 인터페이스. 모든 렌더링된 오디오에 대한 마지막 목적지를 나타내는 AudioNode의 서브클래스입니다.</li>
          <li>AudioParam 인터페이스. 볼륨과 같은 AudioNode 기능의 개별적 측면을 제어합니다.</li>
          <li>AudioListener 인터페이스. 공간화를 위해 PannerNode와 함께 사용됩니다.</li>
          <li>AudioWorklet 인터페이스. 스크립트를 사용해 직접 오디오를 프로세싱할 수 있는 사용자 정의 노드를 생성하기 위한 팩토리를 나타냅니다.</li>
          <li>AudioWorkletGlobalScope 인터페이스. AudioWorkletProcessor의 프로세싱 스크립트가 실행되는 컨텍스트입니다.</li>
          <li>AudioWorkletNode 인터페이스. AudioWorkletProcessor 내에서 프로세싱되는 노드를 나타내는 AudioNode입니다.</li>
          <li>AudioWorkletProcessor 인터페이스. 오디오 worker 내부의 단일 노드 인스턴스를 나타냅니다.</li>
          <li>BiquadFilterNode 인터페이스. 다음과 같은 일반적인 low-order 필터들에 대한 AudioNode입니다.
            <ul>
              <li>Low Pass</li>
              <li>High Pass</li>
              <li>Band Pass</li>
              <li>Low Shelf</li>
              <li>High Shelf</li>
              <li>Peaking</li>
              <li>Notch</li>
              <li>Allpass</li>
            </ul>
          </li>
          <li>ChannelMergerNode 인터페이스. 다수의 오디오 스트림으로부터의 채널을 하나의 오디오 스트림으로 결합하기 위한 AudioNode.</li>
          <li>ChannelSplitterNode 인터페이스. 라우팅 그래프에서 오디오 스트림의 개개의 채널에 접근하기 위한 AudioNode.</li>
          <li>ConstantSourceNode 인터페이스. AudioParam이 명목상의 constant 출력값의 automation을 허용하게 하며, 저 값을 생성하기 위한 AudioNode.</li>
          <li>ConvolverNode 인터페이스. (콘서트 홀의 소리와 같이) 실시간 선형 이펙트를 적용하기 위한 AudioNode.</li>
          <li>DelayNode 인터페이스. 동적으로 조정 가능한 가변적인 딜레이를 적용하는 AudioNode.</li>
          <li>DynamicsCompressorNode 인터페이스. dynamics compression을 위한 AudioNode.</li>
          <li>GainNode 인터페이스. 명시적인 gain 제어를 위한 AudioNode.</li>
          <li>IIRFilterNode 인터페이스. 일반적인 IIR 필터를 위한 AudioNode.</li>
          <li>MediaElementAudioSourceNode 인터페이스. &lt;audio&gt;, &lt;video&gt;, 혹은 그 밖의 미디어 요소로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamTrackAudioSourceNode 인터페이스. MediaStreamTrack으로부터의 오디오 소스인 AudioNode.</li>
          <li>MediaStreamAudioDestinationNode 인터페이스. 원격 peer에게 전송되는 MediaStream으로의 오디오 목적지인 AudioNode.</li>
          <li>PannerNode 인터페이스. 3D 공간에서의 공간화/포지셔닝을 위한 AudioNode.</li>
          <li>PeriodicWave 인터페이스. OscillatorNode에 의한 사용을 위한 사용자 정의 주기 파형을 명시하기 위한 인터페이스입니다.</li>
          <li>OscillatorNode 인터페이스. 주기 파형을 생성하기 위한 AudioNode.</li>
          <li>StereoPannerNode 인터페이스. 스테레오 스트림에서 오디오 입력의 equal-power 포지셔닝을 위한 AudioNode.</li>
          <li>WaveShaperNode 인터페이스. distortion과 그 밖의 다른 subtle warming 효과를 위한 비선형적 waveshaping 효과를 적용하는 AudioNode.</li>
        </ul>
        <p>Web Audio API에서 폐기(deprecated)되었지만 이것들의 대체물의 구현 경험이 있을 때 까지는 아직 제거되지 않을 몇몇 기능들이 있습니다.</p>
        <ul>
          <li>ScriptProcessorNode 인터페이스. 스크립트를 사용해 직접적으로 오디오를 생성하거나 프로세싱하기 위한 AudioNode.</li>
          <li>AudioProcessingEvent 인터페이스. ScriptProcessorNode 객체와 함께 사용되는 이벤트 유형입니다.</li>
        </ul>
        <h1>1. Audio API</h1>
        <h2>1.1. BaseAudioContext 인터페이스</h2>
        <p>BaseAudioContext 인터페이스는 AudioNode 객체들의 집합과 이 객체들의 연결을 나타냅니다. 이 인터페이스는 AudioDestinationNode로 향하는 임의적인 신호 라우팅을 고려합니다. 노드들은 이 컨텍스트에서 생성되고 그리고 나서 함께 연결됩니다.</p>
        <p>BaseAudioContext는 직접적으로 인스턴스화되지 않지만, 대신 실체가 있는 인터페이스인 AudioContext (실시간 렌더링용) 와 OfflineAudioContext (오프라인 렌더링용) 에 의해 상속됩니다.</p>
        <p>BaseAudioContext는 내부 슬롯 [[pending promises]] 를 가지고 생성되는데, 이 내부 슬롯은 초기적으로는 빈 promise의 정렬된 리스트입니다.</p>
        <p>각 BaseAudioContext는 고유의 미디어 요소 이벤트 태스크 소스를 가지고 있습니다. 추가적으로, BaseAudioContext는 두 개의 private 슬롯 [[rendering thread state]] 와 [[control thread state]]를 가지고 있는데, 이 슬롯들은 AudioContextState로부터 값을 취하며, 둘 다 초기적으로 "suspended"로 설정되어 있습니다.</p>
        <pre>
enum AudioContextState {
  "suspended",
  "running",
  "closed"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>suspended</i>"</td>
              <td>이 컨텍스트는 현재 연기되었습니다 (context time은 진행되지 않으며, audio 하드웨어는 전원이 차단되었거나/released 되었을 수 있습니다).</td>
            </tr>
            <tr>
              <td>"<i>running</i>"</td>
              <td>오디오가 처리되고 있습니다.</td>
            </tr>
            <tr>
              <td>"<i>closed</i>"</td>
              <td>이 컨텍스트는 released되었고, 더 이상 오디오를 처리하기 위해 사용될 수 없습니다. 모든 시스템 오디오 자원이 released되었습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
callback DecodeErrorCallback = undefined (DOMException error);

callback DecodeSuccessCallback = undefined (AudioBuffer decodedData);

[Exposed=Window]
interface BaseAudioContext : EventTarget {
  readonly attribute AudioDestinationNode destination;
  readonly attribute float sampleRate;
  readonly attribute double currentTime;
  readonly attribute AudioListener listener;
  readonly attribute AudioContextState state;
  [SameObject, SecureContext]
  readonly attribute AudioWorklet audioWorklet;
  attribute EventHandler onstatechange;

  AnalyserNode createAnalyser ();
  BiquadFilterNode createBiquadFilter ();
  AudioBuffer createBuffer (unsigned long numberOfChannels,
                            unsigned long length,
                            float sampleRate);
  AudioBufferSourceNode createBufferSource ();
  ChannelMergerNode createChannelMerger (optional unsigned long numberOfInputs = 6);
  ChannelSplitterNode createChannelSplitter (
    optional unsigned long numberOfOutputs = 6);
  ConstantSourceNode createConstantSource ();
  ConvolverNode createConvolver ();
  DelayNode createDelay (optional double maxDelayTime = 1.0);
  DynamicsCompressorNode createDynamicsCompressor ();
  GainNode createGain ();
  IIRFilterNode createIIRFilter (sequence<double> feedforward,
                                 sequence<double> feedback);
  OscillatorNode createOscillator ();
  PannerNode createPanner ();
  PeriodicWave createPeriodicWave (sequence<float> real,
                                   sequence<float> imag,
                                   optional PeriodicWaveConstraints constraints = {});
  ScriptProcessorNode createScriptProcessor(
    optional unsigned long bufferSize = 0,
    optional unsigned long numberOfInputChannels = 2,
    optional unsigned long numberOfOutputChannels = 2);
  StereoPannerNode createStereoPanner ();
  WaveShaperNode createWaveShaper ();

  Promise<AudioBuffer> decodeAudioData (
    ArrayBuffer audioData,
    optional DecodeSuccessCallback? successCallback,
    optional DecodeErrorCallback? errorCallback);
};          
        </pre>
        <h3>1.1.1. 어트리뷰트</h3>
        <dl>
          <dt><i>audioWorklet</i> / 자료형: AudioWorklet / 읽기 전용</dt>
          <dd>Worklet 객체로의 접근을 가능케 하는데, Worklet 객체는 [HTML] 과 AudioWorklet에 의해 정의된 알고리즘을 통해 AudioWorkletProcessor 클래스 정의를 포함하고 있는 스크립트를 import할 수 있습니다.</dd>
          <br>
          <dt></i>currentTime</i> / 자료형: double / 읽기 전용</dt>
          <dd>currentTime은 컨텍스트의 렌더링 그래프에 의해 가장 최근에 처리된 오디오 블럭의 마지막 sample frame 바로 다음의 샘플 프레임의 시간 (초) 입니다. 만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, currentTime은 0의 값을 갖습니다.</dd>
          <br>
          <dd>currentTime의 시간 좌표계에서, 0의 값은 그래프에 의해 처리되는 첫번째 블럭의 첫번째 샘플 프레임과 일치합니다. 이 계(system)에서 경과된 시간은 BaseAudioContext에 의해 생성된 오디오 스트림에서의 경과된 시간과 일치하는데, 이 시간은 이 계의 다른 시계와 동기화되지 않았을 수 있습니다. (OfflineAudioContext의 경우, 스트림이 어떠한 장치에서도 현재 재생되고 있지 않으므로, 실제 시간에 대한 근사치조차도 존재하지 않습니다.</dd>
          <br>
          <dd>Web Audio API에서의 모든 예정된 시간들은 currentTime의 값에 관련이 있습니다.</dd>
          <br>
          <dd>BaseAudioContext가 "running" 상태일 때, 이 어트리뷰트의 값은, 하나의 render quantum에 일치하며, 단조적으로 증가하고 획일적인 증가 속에서 렌더링 스레드에 의해 갱신됩니다. 따라서, 실행 중인 컨텍스트의 경우, 이 계가 오디오 블럭을 처리해 가는 동안 currentTime은 꾸준히 증가하고, 항상 처리될 다음 오디오 블럭의 시작 시간을 나타냅니다. 현재 상태에서 예정된 어떠한 변화가 일어날지도 모를 때 이것은 또한 가능한 가장 빠른 시간입니다.</dd>
          <dd>currentTime은 반환되기 전에 반드시 control thread에서 atomically하게 read되어야만 합니다.</dd>
          <br>
          <dt><i>destination</i> / 자료형: AudioDestinationNode / 읽기 전용</dt>
          <dd>모든 오디오의 최종 목적지를 나타내는 하나의 입력을 가진 AudioDestinationNode. 보통 이것은 실제 오디오 하드웨어를 나타냅니다. 오디오를 현재 렌더링하고 있는 모든 AudioNode는 직접적으로 혹은 간접적으로 목적지에 연결됩니다.</dd>
          <br>
          <dt><i>listener</i> / 자료형: AudioListener / 읽기 전용</dt>
          <dd>3D 공간화에 사용되는 AudioListener</dd>
          <br>
          <dt><i>onstatechange</i> / 자료형: EventHandler</dt>
          <dd>AudioContext의 상태가 변경되었을 때 BaseAudioContext에 전파되는 이벤트에 대한 EventHandler를 설정하기 위해 사용되는 프로퍼티 (예: 해당하는 promise가 resolve되었을 때). Event 유형의 이벤트가 이벤트 핸들러에 전파될 것인데, 이는 AudioContext의 상태를 직접적으로 질의할 수 있습니다. 새롭게 생성된 AudioContext는 항상 suspended 상태에서 시작하고, 언제든지 다른 상태로 상태가 변경될 때 상태 변화 이벤트가 발생될 것입니다. 이 이벤트는 oncomplete 이벤트가 발생되기 이전에 발생됩니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>BaseAudioContext가 오디오를 다루는 (초당 샘플 프레임에서의) 샘플 레이트. 이것은 컨텍스트의 모든 AudioNode가 이 rate에서 실행될 것으로 추정합니다. 이 추정을 내림에 있어, 샘플 레이트 전환기 혹은 "varispeed" 프로세서는 실시간 프로세싱이 지원되지 않습니다. <strong><i>나이퀴스트 주파수</i></strong>는 이 샘플 레이트 값의 절반입니다.</dd>
          <br>
          <dt><i>state</i> / 자료형: AudioContextState / 읽기 전용</dt>
          <dd>BaseAudioContext의 현재 상태를 기술합니다. 이 어트리뷰트를 get하는 것은 [[control thread state]] 슬롯의 내용을 반환합니다.</dd>
        </dl>
        <br>
        <h3>1.1.2. 메서드</h3>
        <dl>
          <dt><i>createAnalyser()</i></dt>
          <dd>AnalyserNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AnalyserNode</dd>
          <br>
          <dt><i>createBiquadFilter()</i></dt>
          <dd>몇몇 일반적인 필터 유형 중 하나로써 설정될 수 있는 second order 필터를 나타내는 BiquadFilterNode의 팩토리 메서드.</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: BiquadFilterNode</dd>
          <br>
          <dt><i>createBuffer(numOfChannels, length, sampleRate)</i></dt>
          <dd>주어진 크기의 AudioBuffer를 생성합니다. 버퍼 내의 오디오 데이터는 0으로 초기화될 것입니다 (silent). 만약 인자가 어느 하나라도 음수거나, 0이거나, 명목상의 범위 바깥에 있다면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createBuffer() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 구현은 반드시 적어도 32개의 채널을 지원해야 합니다.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼 크기를 결정합니다. 이것은 반드시 적어도 1이어야 합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 구현은 반드시 적어도 범위 8000에서 96000까지의 샘플 레이트를 지원해야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: AudioBuffer</dd>
          <br>
          <dt><i>createBufferSource()</i></dt>
          <dd>AudioBufferSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioBufferSourceNode</dd>
          <br>
          <dt><i>createChannelMerger(numberOfInputs)</i></dt>
          <dd>channel merger를 나타내는 ChannelMergerNode의 팩토리 메서드. 만약 numberOfInputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelMerger(numberOfInputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfInputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>입력의 수를 결정합니다. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelMergerNode</dd>
          <br>
          <dt><i>channelSplitterNode(numberOfOutputs)</i></dt>
          <dd>channel splitter를 나타내는 ChannelSplitterNode의 팩토리 메서드. 만약 numberOfOutputs가 1보다 작거나 지원되는 채널의 수보다 크다면 IndexSizeError 예외가 반드시 발생되어야만 합니다.</dd>
          <table>
            <caption>BaseAudioContext.createChannelSplitter(numberOfOutputs) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfOutputs</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>v</td>
              <td>출력의 수. 32까지의 값이 반드시 지원되어야만 합니다. 만약 명시되지 않았다면, 6이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: ChannelSplitterNode</dd>
          <br>
          <dt><i>createConstantSource()</i></dt>
          <dd>ConstantSourceNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConstantSourceNode</dd>
          <br>
          <dt><i>createConvolver()</i></dt>
          <dd>ConvolverNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: ConvolverNode</dd>
          <br>
          <dt><i>createDelay(maxDelayTime)</i></dt>
          <dd>DelayNode의 팩토리 메서드. 초기 기본 delay time은 0초입니다.</dd>
          <table>
            <caption>BaseAudioContext.createDelay(maxDelayTime) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>maxDelayTime</td>
              <td>double</td>
              <td>x</td>
              <td>v</td>
              <td>지연선(delay line)을 고려하는 최대 delay time (초) 를 명시합니다. 만약 명시되었다면, 이 값은 반드시 0보다 크고 3분보다 작아야 하며 그렇지 않다면 NotSupportedError 예외가 반드시 발생되어야 합니다. 만약 명시되지 않았다면, 1이 사용될 것입니다.</td>
            </tr>
          </table>
          <dd>반환 유형: DelayNode</dd>
          <br>
          <dt><i>createDynamicsCompressor()</i></dt>
          <dd>DynamicsCompressorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: DynamicsCompressorNode</dd>
          <br>
          <dt><i>createGain()</i></dt>
          <dd>GainNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: GainNode</dd>
          <br>
          <dt><i>createIIRFilter(feedforward, feedback)</i></dt>
          <table>
            <caption>BaseAudioContext.createIIRFilter(feedforward, feedback) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>feedforward</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedforward (분자) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 모든 값들이 0이라면, InvalidStateError가 반드시 발생되어야 합니다. 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
            <tr>
              <td>feedback</td>
              <td>sequence&lt;double&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>IIR 필터의 transfer 함수의 feedback (분모) 계수의 배열입니다. 이 배열의 최대 길이는 20입니다. 만약 이 배열의 첫번째 요소가 0이라면, InvalidStateError가 반드시 발생되어야 합니다. 만약 배열의 길이가 0이거나 20보다 크다면 NotSupportedError가 반드시 발생되어야 합니다.</td>
            </tr>
          </table>
          <dd>반환 유형: IIRFilterNode</dd>
          <br>
          <dt><i>createOscillator()</i></dt>
          <dd>OscillatorNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: OscillatorNode</dd>
          <br>
          <dt><i>createPanner()</i></dt>
          <dd>PannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: PannerNode</dd>
          <br>
          <dt><i>createPeriodicWave(real, imag, constraints)</i></dt>
          <dd>PeriodicWave를 생성하기 위한 팩토리 메서드</dd>
          <dd>이 메서드를 호출할 때, 다음의 단계를 실행합니다:</dd>
          <ol>
            <li>만약 real과 imag가 같은 길이가 아니면, IndexSizeError가 반드시 발생되어야 합니다.</li>
            <li>o가 PeriodicWaveOptions 유형의 새로운 객체이게 합니다.</li>
            <li>이 팩토리 메서드에 전달된 real과 imag 파라미터를 각각 o에 같은 이름의 어트리뷰트로 설정합니다.</li>
            <li>o에 disableNormalization 어트리뷰트를 이 팩토리 메서드에 전달된 constraints 어트리뷰트의 disableNormalization 어트리뷰트의 값으로 설정합니다.</li>
            <li>BaseAudioContext에 이 팩토리 메서드가 첫번째 인자로서 호출되었음을 전달하며, 새로운 PeriodicWave p와, o를 생성합니다.</li>
            <li>p를 반환합니다.</li>
          </ol>
          <table>
            <caption>BaseAudioContext.createPeriodicWave() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>real</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>코사인 파라미터의 sequence. 더욱 자세한 설명은 real 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>imag</td>
              <td>seqeunce&lt;float&gt;</td>
              <td>x</td>
              <td>x</td>
              <td>사인 파라미터의 sequence. 더욱 자세한 설명은 imag 생성자 인자를 참조하십시오.</td>
            </tr>
            <tr>
              <td>constraints</td>
              <td>PeriodicWaveConstraints</td>
              <td>x</td>
              <td>v</td>
              <td>만약 주어지지 않았다면, 파형은 정규화(normalize)됩니다. 그렇지 않으면, 파형은 constraints에 의해 주어진 값에 따라 정규화됩니다.</td>
            </tr>
          </table>
          <dd>반환 유형: PeriodicWave</dd>
          <br>
          <dt><i>createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels)</i></dt>
          <dd>ScriptProcessorNode의 팩토리 메서드. 이 메서드는 폐기되었습니다. 왜냐하면 이 메서드는 AudioWorkletNode에 의해 대체되는 것이 의도되었기 때문입니다.</dd>
          <dd>추후 번역</dd>
          <br>
          <dt><i>createStereoPanner()</i></dt>
          <dd>StereoPannerNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: StereoPannerNode</dd>
          <br>
          <dt><i>createWaveShaper()</i></dt>
          <dd>비선형 왜곡을 나타내는 WaveShaperNode의 팩토리 메서드</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: WaveShaperNode</dd>
          <br>
          <dt><i>decodeAudioData(audioData, successCallback, errorCallback)</i></dt>
          <dd>ArrayBuffer에 포함된 오디오 파일 데이터를 비동기적으로 디코드합니다.</dd>
          <dd>추후 번역</dd>
          <br>
          <h3>1.1.3. 콜백 DecodeSuccessCallback() 파라미터</h3>
          <dl>
            <dt>decodedData / 유형: AudioBuffer</dt>
            <dd>디코드된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
          </dl>
          <br>
          <h3>1.1.4. 콜백 DecodeErrorCallback() 파라미터</h3>
          <dl>
            <dt>error / 유형: DOMException</dt>
            <dd>디코드 도중에 발생한 에러</dd>
          </dl>
        </dl>
        <br>
        <h3>1.1.5. 생애 주기</h3>
        <p>한 번 생성되고 나면, AudioContext는 자신이 재생할 소리가 더 이상 남아있지 않거나, 페이지가 꺼질 때까지 계속 소리를 재생합니다.</p>
        <br>
        <h3>1.1.6. 자기 성찰(introspection) 또는 직렬화 primitives(기본 요소?)의 부족</h3>
        <p>Web Audio API는 오디오 소스 스케쥴링에 사용하고 나서 잊는(fire-and-forget) 접근 방식을 취합니다. 즉, 소스 노드는 AudioContext의 생애 주기 동안 각 노트에 대해 생성되고, 절대로 명시적으로 그래프에서 제거되지 않습니다. 이것은 직렬화 API와 호환되지 않습니다. 왜냐하면 직렬화될 수 있는 노드들의 안정된 집합이 존재하지 않기 때문입니다.</p>
        <p>게다가, 자기 성찰 API를 가지는 것은 content 스크립트가 가비지 컬렉션을 관찰할 수 있게 허용할 것입니다.</p>
        <h3>1.1.7. BaseAudioContext 서브클래스와 연관된 시스템 자원</h3>
        <p>서브클래스 AudioContext와 OfflineAudioContext는 비용이 많이 드는(expensive) 객체로 여겨져야 합니다. 이 객체들은 생성하는 것은 높은 우선도의 스레드를 생성하는 것이나, 낮은 레이턴시의 시스템 오디오 스트림을 사용하는 것을 포함할 수도 있는데, 양측 다 에너지 소모에 영향을 가집니다. 문서에서 하나 이상의 AudioContext를 생성하는 것은 보통 필요하지 않습니다.</p>
        <p>BaseAudioContext 서브클래스를 구성하거나 재개하는 것은 그 컨텍스트에 대해 시스템 자원을 취득하는 것을 포함합니다. AudioContext에 대해, 이것은 또한 시스템 오디오 스트림의 생성을 요구합니다. 이 연산들은 컨텍스트가 컨텍스트의 연관된 오디오 그래프로부터 출력을 생성하는 것을 시작할 때 반환됩니다.</p>
        <p>추가적으로, 유저 에이전트는 구현이 정의된(implementation-defined) AudioContext의 최대 수를 가질 수 있는데, 이는 새로운 AudioContext를 생성하려는 시도가 실패한 이후에, NotSupportedError를 발생시킵니다.</p>
        <p>suspend와 close는 작성자로 하여금 시스템 자원을 해방하게 할 수 있는데, 이는 스레드, 프로세스, 오디오 스트림을 포함합니다. BaseAudioContext를 연기(suspend)하는 것은 BaseAudioContext의 일부 자원을 해방하는 구현을 허용하고, BaseAudioContext가 이후에 resume을 호출함으로써 작동을 계속하도록 허락합니다. AudioContext를 닫는(close)것은 AudioContext의 모든 자원을 해방하는 구현을 허락하는데, 이는 AudioContext가 사용될 수 없거나 다시 재개될 수 없는 이후입니다.</p>
        <div class="note">
          <p>노트: 예를 들어, 이것은 audio callback이 정기적으로 발생되는 것을 기다리거나, 하드웨어가 프로세싱을 위해 준비되기를 기다리는 것을 포함할 수 있습니다.</p>
        </div>
        <br>
        <h2>1.2. AudioContext 인터페이스</h2>
        <p>이 인터페이스는 유저에게 향하는 신호를 생산하는 실시간 출력 장치로 라우팅되는 AudioDestinationNode를 가지고 있는 오디오 그래프를 나타냅니다. 대부분의 사용 경우에, 오직 하나의 AudioContext가 문서당 사용됩니다.</p>
        <pre>
enum AudioContextLatencyCategory {
  "balanced",
  "interactive",
  "playback"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>balanced</i>"</td>
              <td>오디오 출력 레이턴시와 파워 소모의 균형을 유지합니다.</td>
            </tr>
            <tr>
              <td>"<i>interactive</i>"</td>
              <td>glitching이 없는 가능한 가장 낮은 오디오 출력 레이턴시를 제공합니다. 이것이 기본값입니다.</td>
            </tr>
            <tr>
              <td>"<i>playback</i>"</td>
              <td>오디오 출력 레이턴시에 대한 중단 없이 일관된 재생을 우선적으로 처리합니다. 파워 소모가 가장 적습니다.</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface AudioContext : BaseAudioContext {
  constructor (optional AudioContextOptions contextOptions = {});
  readonly attribute double baseLatency;
  readonly attribute double outputLatency;
  AudioTimestamp getOutputTimestamp ();
  Promise<undefined> resume ();
  Promise<undefined> suspend ();
  Promise<undefined> close ();
  MediaElementAudioSourceNode createMediaElementSource (HTMLMediaElement mediaElement);
  MediaStreamAudioSourceNode createMediaStreamSource (MediaStream mediaStream);
  MediaStreamTrackAudioSourceNode createMediaStreamTrackSource (
    MediaStreamTrack mediaStreamTrack);
  MediaStreamAudioDestinationNode createMediaStreamDestination ();
};
        </pre>
        <p>AudioContext는 user agent가 컨텍스트 상태가 "suspended"에서 "running"으로 전이되는 것을 허용한 경우 <strong>시작되도록 허용</strong>됩니다. user agent는 이 초기 전이를 비허용할 수 있으며 이것을 허용하기 위해서는 AudioContext'의 관련된 전역 객체가 sticky activation을 가지고 있어야만 합니다.</p>
        <p>AudioContext는 내부 슬롯을 가지고 있습니다:</p>
        <dl>
          <dt><i>[[suspended by user]]</i></dt>
          <dd>context가 사용자 코드에 의해 suspended되었는지를 나타내는 boolean flag. 초기값은 false.</dd>
        </dl>
        <br>
        <h3>1.2.1. 생성자</h3>
        <dl>
          <dt><i>AudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단시킵니다.</dd>
          <br>
          <dd>AudioContext를 생성할 때, 아래의 과정들을 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <div class="note">
          <p>노트: 프로그래밍적으로 작성자에게 AudioContext의 생성이 실패했다는 것을 알리는 것이 불행하게도 가능하지 않습니다. 유저 에이전트는 만약 작성자가 로깅 메커니즘에 접근권한을 가지고 있다면 정보 메시지를 로그할 것이 장려됩니다 (예: 개발자 도구 콘솔).</p>
        </div>
        <br>
        <table>
          <caption>AudioContext.constructor(contextOptions) 메서드의 인자</caption>
          <tr>
            <td>매개변수</td>
            <td>자료형</td>
            <td>nullable</td>
            <td>optional</td>
            <td>설명</td>
          </tr>
          <tr>
            <td>contextOptions</td>
            <td>AudioContextOptions</td>
            <td>x</td>
            <td>v</td>
            <td>AudioContext가 어떻게 생성되어야 하는지를 제어하는 유저가 명시한 옵션</td>
          </tr>
        </table>
        <br>
        <h3>1.2.2. 어트리뷰트</h3>
        <dl>
          <dt><i>baseLatency</i> / 자료형: double / 읽기 전용</dt>
          <dd>baseLatency는 AudioDestinationNode에서 audio subsystem으로 오디오를 전달하는 AudioContext에 의해 발생되는 프로세싱 레이턴시의 초를 나타냅니다. 이것은 AudioDestinationNode의 출력과 오디오 하드웨어 사이의 다른 프로세싱에 의해 유발될 수 있는 다른 추가적인 레이턴시를 포함하지 않으며 분명히 오디오 그래프 그 자체에 의해 발생되는 모든 레이턴시를 포함하지 않습니다.</dd>
          <br>
          <dd>예를 들어, 만약 오디오 컨텍스트가 44.1 kHz로 실행되고 AudioDestinationNode가 내부적으로 double buffering을 구현하고 각 render quantum마다 오디오를 처리하고 출력할 수 있다면, 이 경우 프로세싱 레이턴시는 (2 * 128) / 44100 = 5.805ms (근사치) 입니다.</dd>
          <br>
          <dt><i>outputLatency</i> / 자료형: double / 읽기 전용 / 현재 파이어폭스 엔진에만 있음</dt>
          <dd>오디오 출력 레이턴시의 추정 (단위: 초). 예: UA가 호스트 시스템에게 버퍼를 재생하는 것과 버퍼 내의 첫번째 샘플이 실제로 오디오 출력 장치에 의해 처리되는 시간을 요구하는 것. 음향 시그널을 생산하는 스피커와 헤드폰과 같은 장치에 대해서, 이 후자의 시간은 샘플의 소리가 생산될 때의 시간을 나타냅니다.</dd>
          <br>
          <dd>outputLatency 어트리뷰트 값은 플랫폼과 연결된 하드웨어 오디오 출력 장치에 달려 있습니다. outputLatency 어트리뷰트 값은 연결된 오디오 출력 장치가 동일한 이상 컨텍스트의 생명 주기에 대해 변하지 않습니다. 만약 오디오 출력 장치가 변경되었다면 outputLatency 어트리뷰트 값은 그에 맞추어 갱신될 것입니다.</dd>
        </dl>
        <br>
        <h3>1.2.3. 메서드</h3>
        <dl>
          <dt><i>close()</i></dt>
          <dd>사용되고 있던 시스템 자원을 해방하며 AudioContext를 닫습니다. 이것은 자동적으로 AudioContext가 생성한 모든 객체를를 해방하지 않을 것이지만, AudioContext의 currentTime의 진행을 유예할 것이고, 오디오 데이터를 프로세싱하는 것을 멈춥니다.</dd>
          <br>
          <dd>close가 호출되었을 때, 아래의 단계를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <br>
          <div class="note">
            <p>AudioContext가 closed되었을 때, 구현은 suspending되었을 때보다 더 공격적으로 자원을 해방하는 것을 선택할 수 있습니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>createMediaElementSource(mediaElement)</i></dt>
          <dd>주어진 HTMLMediaElement로 MediaElementAudioSourceNode를 생성합니다. 이 메서드를 호출한 결과로서, HTMLMediaElement로부터의 오디오 재생이 AudioContext의 프로세싱 그래프 내로 재라우팅(re-route)될 것입니다.</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaElementSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaElement</td>
              <td>HTMLMediaElement</td>
              <td>x</td>
              <td>x</td>
              <td>재라우팅될 미디어 요소</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaElementAudioSourceNode</dd>
          <br>
          <dt><i>createMediaStreamDestination</i></dt>
          <dd>MediaStreamAudioDestinationNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: MediaStreamAudiodestinationNode</dd>
          <br>
          <dt><i>createMediaStreamSource(mediaStream)</i></dt>
          <dd>MediaStreamAudioSourceNode를 생성합니다</dd>
          <dd>매개변수 없음</dd>
          <br>
          <table>
            <caption>AudioContext.createMediaStreamSource() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>mediaStream</td>
              <td>MediaStream</td>
              <td>x</td>
              <td>x</td>
              <td>소스(source)로서 작동할 미디어 스트림.</td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: MediaStreamAudioSourceNode</dd>
          <br>
          <dt><i>getOutputTimestamp()</i></dt>
          <dd>컨텍스트에 대해 두 개의 연관된 오디오 스트림 포지션 값을 포함하고 있는 새로운 AudioTimeStamp 인스턴스를 반환합니다: contextTime 멤버는 (컨텍스트의 currentTime과 같은 단위와 기원에서) 오디오 출력 장치에 의해 현재 렌더링되고 있는 샘플 프레임의 시간 (예: 출력 오디오 스트림 포지션)  입니다; performanceTime 멤버는 (performance.now()와 같은 단위와 기원에서) 저장된 contextTime에 해당하는 샘플 프레임이 오디오 출력 장치에 의해 렌더링된 순간을 추산하는 시간을 포함합니다. (이는 [hr-time-3] 에서 기술됨).</dd>
          <br>
          <dd>만약 컨텍스트의 렌더링 그래프가 아직 오디오 블럭을 처리하지 않았다면, getOutputTimestamp 호출은 두 멤버가 모두 0을 포함하고 있는 AudioTimestamp 인스턴스를 반환합니다.</dd>
          <br>
          <dd>컨텍스트의 렌더링 그래프가 오디오 블럭의 처리를 시작한 이후에, 이것의 currentTime 어트리뷰트 값은 항상 getOutputTimestamp 메서드 호출로 인해 얻어진 contextTime 값을 넘어섭니다.</dd>
          <br>
          <pre>예제 4
getOutputTimestamp 메서드로부터 반환된 값은 컨텍스트의 시간 값보다 살짝 후의 퍼포먼스 시간 추산을 얻기 위해 사용될 수 있습니다:

function outputPerformanceTime(contextTime) {
  const timestamp = context.getOutputTimestamp();
  const elapsedTime = contextTime - timestamp.contextTime;
  return timestamp.performanceTime + elapsedTime * 1000;
}

상기의 예제에서 추산의 정확도는 인자 값이 얼마나 현재 출력 오디오 스트림 포지션에 가까운지에 달려 있습니다: 주어진 contextTime이 timestamp.contextTime에 가까울수록, 얻어지는 추산의 정확도는 좋아집니다.
          </pre>
          <br>
          <div class="note">
            <p>노트: 컨텍스트의 currentTime과 getOutputTimestamp 메서드 호출로부터 얻어진 contextTime의 차이는 신용 가능한 출력 레이턴시 추산으로써 여겨질 수 없습니다. 왜냐하면 currentTime은 비균일한 시간 구간에서 증가될 수 있으므로, outputLatency 어트리뷰트가 대신 사용되어야 합니다.</p>
          </div>
          <br>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: AudioTimestamp</dd>
          <br>
          <dt><i>resume()</i></dt>
          <dd>AudioContext가 연기되었을 때 AudioContext의 currentTime의 진행을 재개합니다.</dd>
          <dd>resume이 호출되었을 때, 아래의 절차를 실행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
          <br>
          <dt><i>suspend()</i></dt>
          <dd>AudioContext의 currentTime의 진행을 연기하고, 이미 처리된 모든 현재의 컨텍스트 프로세싱 블럭들이 목적지에서 재생되는 것을 허용하고, 그리고 나서 시스템이 오디오 하드웨어에 대한 주장(claim)을 해방하도록 허용합니다. 이것을 일반적으로 어플리케이션이 AudioContext가 어떤 때는 필요가 없을 것이라는 것을 알고, 일시적으로 AudioContext에 연관된 시스템 자원을 해방하기를 소원할 때 유용합니다. 이 promise는 프레임 버퍼가 비었을 때 (프레임 버퍼가 하드웨어에서 손을 뗐을 때), 혹은 일시적으로 (다른 효과 없이) 만약 컨텍스트가 이미 연기되었을 경우 resolve합니다. 이 promise는 컨텍스트가 closed되었을 때 rejected됩니다.</dd>
          <br>
          <dd>suspend가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
        </dl>
        <br>
        <h3>1.2.4. AudioContextOptions</h3>
        <p>AudioContextOptions 딕셔너리는 AudioContext에 대해 사용자가 정의한 옵션들을 명시하기 위해 사용됩니다.</p>
        <br>
        <pre>
dictionary AudioContextOptions {
  (AudioContextLatencyCategory or double) latencyHint = "interactive";
  float sampleRate;
};
        </pre>
        <br>
        <h4>1.2.4.1. 딕셔너리 AudioContextOptions 멤버</h4>
        <dl>
          <dt><i>latencyHint</i> / 자료형: (AudioContextLatencyCategory 또는 double) / 기본값: "interactive"</dt>
          <dd>재생의 유형을 명시하는데, 이는 오디오 출력 레이턴시와 파워 소모 사이의 균형(tradeoff)에 영향을 미칩니다.</dd>
          <br>
          <dd>latencyHint의 선호된 값은 AudioContextLatencyCategory로부터의 값입니다. 그러나, double 또한 레이턴시와 파워 소모를 균형잡기 위한 더 좋은 제어를 위한 레이턴시 (단위: 초) 를 위해 명시될 수 있습니다. 이 숫자를 적절히 해석하는 것은 브라우저의 재량에 있습니다. 사용되는 실제 레이턴시는 AudioContext의 baseLatenct 어트리뷰트에 의해 주어집니다.</dd>
          <br>
          <dt><i>sampleRate / 자료형: float</i></dt>
          <dd>생성될 AudioContext에 대해 이 값에 sampleRate를 설정합니다. 이 지원된 값들은 AudioBuffer의 샘플 레이트와 같습니다. 만약 명시된 샘플 레이트가 지원되지 않는다면 NotSupportedError 예외가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>만약 sampleRate가 명시되지 않았다면, 이 AudioContext의 출력 장치의 선호되는 샘플 레이트가 사용됩니다.</dd>
        </dl>
        <br>
        <h3>1.2.5. AudioTimestamp</h3>
        <pre>
dictionary AudioTimestamp {
  double contextTime;
  DOMHighResTimeStamp performanceTime;
};
        </pre>
        <br>
        <h4>1.2.5.1. 딕셔너리 AudioTimestamp 멤버</h4>
        <dl>
          <dt><i>contextTime</i> / 자료형: double</dt>
          <dd>BaseAudioContext의 currentTime의 시간 좌표계의 점(point)를 나타냅니다.</dd>
          <br>
          <dt><i>performanceTime</i> / 자료형: DOMHighResTimeStamp</dt>
          <dd>퍼포먼스 인터페이스 구현의 시간 좌표계의 점을 나타냅니다 ([hr-time-3]에서 기술됩니다).</dd>
        </dl>
        <br>
        <h2>1.3. OfflineAudioContext 인터페이스</h2>
        <p>OfflineAudioContext는 실시간보다 (잠재적으로) 렌더링/믹싱다운을 더 빠르게 하기 위한 BaseAudioContext의 특정한 유형입니다. 이것은 오디오 하드웨어로 렌더링하지 않지만, 대신 가능한 한 빨리 렌더링하는데, AudioBuffer로 렌더링된 결과와 함께 반환된 promise를 fulfill합니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioContext : BaseAudioContext {
  constructor(OfflineAudioContextOptions contextOptions);
  constructor(unsigned long numberOfChannels, unsigned long length, float sampleRate);
  Promise<AudioBuffer> startRendering();
  Promise<undefined> resume();
  Promise<undefined> suspend(double suspendTime);
  readonly attribute unsigned long length;
  attribute EventHandler oncomplete;
};          
        </pre>
        <br>
        <h3>1.3.1. 생성자</h3>
        <dl>
          <dt><i>OfflineAudioContext(contextOptions)</i></dt>
          <dd>만약 current settings object의 responsible document가 완전히 active하지 않다면, InvalidStateError를 발생시키고 아래의 과정들을 중단합니다.</dd>
          <br>
          <dd>c를 새로운 OfflineAudioContext 객체라고 합니다. c를 다음과 같이 초기화합니다:</dd>
          <ol>
            <li>c에 대해 [[control thread state]]를 "suspended"로 설정합니다.</li>
            <li>c에 대해 [[rendering thread state]]를 "suspended"로 설정합니다.</li>
            <li>AudioDestinationNode의 channelCount가 contextOptions.numberOfChannels로 설정된 채로 AudioDestinationNode를 생성합니다.</li>
          </ol>
          <br>
          <table>
            <caption>OfflineAudioContext.constructor(contextOptions) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>contextOptions</td>
              <td></td>
              <td></td>
              <td></td>
              <td>이 컨텍스트를 생성하는 데 필요한 초기 파라미터들.</td>
            </tr>
          </table>
          <br>
          <dt><i>OfflineAudioContext(numberOfChannels, length, sampleRate)</i></dt>
          <dd>OfflineAudioContext는 AudioContext.createBuffer와 같은 인자로 생성될 수 있습니다. 만약 인자 중 하나라도 음수거나, 0이거나, 명목상의 범위 밖이면 NotSupportedError가 반드시 발생되어야 합니다.</dd>
          <br>
          <dd>OfflineAudioContext는 아래가 대신 호출된 것처럼 생성됩니다.</dd>
          <br>
          <pre>
new OfflineAudioContext({
  numberOfChannels: numberOfChannels,
  length: length,
  sampleRate: sampleRate
})
          </pre>
          <table>
            <caption>OfflineAudioContext.constructor(numberOfChannels, length, sampleRate) 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>numberOfChannels</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>버퍼가 얼마나 많은 채널을 가질 지 결정합니다. 지원되는 채널의 수는 createBuffer()를 참고하세요.</td>
            </tr>
            <tr>
              <td>length</td>
              <td>unsigned long</td>
              <td>x</td>
              <td>x</td>
              <td>샘플 프레임 내의 버퍼의 크기를 결정합니다.</td>
            </tr>
            <tr>
              <td>sampleRate</td>
              <td>float</td>
              <td>x</td>
              <td>x</td>
              <td>초당 샘플 프레임 내의 버퍼 내의 선형 PCM 오디오 데이터의 샘플 레이트를 기술합니다. 유효한 샘플 레이트에 대해서는 createBuffer()를 참고하세요.</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.3.2. 어트리뷰트</h3>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임 내의 버퍼의 크기. 이것은 생성자의 length 파라미터의 값과 같습니다.</dd>
          <br>
          <dt><i>oncomplete</i> / 자료형: EventHandler</dt>
          <dd>OfflineAudioCompletionEvent 유형의 EventHandler. 이것은 OfflineAudioContext에서 발생되는 마지막 이벤트입니다.</dd>
        </dl>
        <br>
        <h3>1.3.3. 메서드</h3>
        <dl>
          <dt><i>startRendering()</i></dt>
          <dd>주어진 현재 연결과 예정된 변화를 가지고, 오디오 렌더링을 시작합니다.</dd>
          <br>
          <dd>비록 렌더링된 오디오 데이터를 얻는 일차적인 방법은 이것의 promise 반환 값을 통해서이지만, 이 인스턴스는 또한 레거시의 이유로 complete라는 이름의 이벤트를 발생시킬 것입니다.</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <br>
          <dt><i>resume()</i> / 현재 크롬 엔진에만 있음</dt>
          <dd>OfflineAudioContext의 currentTime의 진행을 OfflineAudioContext가 suspended되었을 때 재개시킵니다.</dd>
          <br>
          <dd>이 메서드가 호출되었을 때, 아래의 절차를 수행합니다:</dd>
          <dd>추후 번역</dd>
          <dd>매개변수 없음</dd>
          <dd>반환 유형: Promise&lt;AudioBuffer&gt;</dd>
          <dt><i>suspend(suspendTime)</i></dt>
          <dd>특정한 시간에 오디오 컨텍스트에서의 시간 진행의 연기를 예정하고 promise를 반환합니다. 이거슨 일반적으로 OfflineAudioContext에서 오디오 그래프를 동기적으로(synchronously) 조작할 때 유용합니다.</dd>
          <br>
          <dd>연기의 최대 정밀도는 render quantum의 크기이고 명시된 연기 시간은 가장 가까운 render quantum 경계선(boundary) 까지에서 반올림될 것입니다. 이러한 이유로, 다수의 연기를 같은 quantized 프레임에 예정하는 것은 허용되지 않습니다. 또한, 정밀한 연기를 보장하기 위해 스케쥴링은 컨텍스트가 실행 중(running)이 아닌 동안에 이루어져야 합니다.</dd>
          <br>
          <table>
            <caption>OfflineAudioContext.suspend() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>suspendTime</td>
              <td>double</td>
              <td>x</td>
              <td>x</td>
              <td>
                명시된 시간에 렌더링의 연기를 예정하는데, 이는 quantized되고 render quantum 크기까지 반올림될 것입니다. 만약 quantized 프레임 수가
                <ol>
                  <li>음수거나</li>
                  <li>current time보다 작거나 혹은 같거나</li>
                  <li>전체 render duration보다 크거나 혹은 같거나</li>
                  <li>같은 시간에 또 다른 suspend가 예정되어 있다면</li>
                </ol>
                promise는 InvalidStateError와 함께 rejected됩니다.
              </td>
            </tr>
          </table>
          <br>
          <dd>반환 유형: Promise&lt;undefined&gt;</dd>
        </dl>
        <br>
        <h3>1.3.4. OfflineAudioContextOptions</h3>
        <p>이것은 OfflineAudioContext를 생성하는 데 쓰이는 옵션들을 명시합니다.</p>
        <pre>
dictionary OfflineAudioContextOptions {
  unsigned long numberOfChannels = 1;
  required unsigned long length;
  required float sampleRate;
};
        </pre>
        <h4>1.3.4.1. 딕셔너리 OfflineAudioContextOptions 멤버</h4>
        <dl>
          <dt><i>length</i> / 자료형: unsigned long</dt>
          <dd>샘플 프레임에서의 렌더링된 AudioBuffer의 길이</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 기본값: 1</dt>
          <dd>이 OfflineAudioContext의 채널의 수.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float</dt>
          <dd>이 OfflineAudioContext의 샘플 레이트</dd>
        </dl>
        <br>
        <h3>1.3.5. OfflineAudioCompletionEvent 인터페이스</h3>
        <p>이것은 레거시적 이유로 OfflineAudioContext에 전파되는 Event 객체입니다.</p>
        <pre>
[Exposed=Window]
interface OfflineAudioCompletionEvent : Event {
  constructor (DOMString type, OfflineAudioCompletionEventInit eventInitDict);
  readonly attribute AudioBuffer renderedBuffer;
};
        </pre>
        <h4>1.3.5.1. 어트리뷰트</h4>
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer / 읽기 전용</i></dt>
          <dd>렌더링된 오디오 데이터를 포함하고 있는 AudioBuffer</dd>
        </dl>
        <br>
        <h4>1.3.5.2. OfflineAudioCompletionEventInit</h4>
        <br>
        <pre>
dictionary OfflineAudioCompletionEventInit : EventInit {
  required AudioBuffer renderedBuffer;
};
        </pre>
        <h5>1.3.5.2.1. 딕셔너리 OfflineAudioCompletionEventInit 멤버</h5>        
        <dl>
          <dt><i>renderedBuffer / 자료형: AudioBuffer</i></dt>
          <dd>이 이벤트의 renderedBuffer 어트리뷰트에 할당될 값</dd>
        </dl>
        <br>
        <h2>1.4. AudioBuffer 인터페이스</h2>
        <p>이 인터페이스는 메모리에 상주하는 오디오 에셋을 나타냅니다. 이것은 하나 이상의 채널을 포함할 수 있는데 각 채널은 [-1, 1]의 명목상의 범위를 가지는 32비트 부동 소수점 선형 PCM 값을 가지고 있는 것으로 보이나 이 값들은 이 범위에 국한되지 않습니다. 통상적으로, PCM 데이터의 길이는 꽤 짧을 것이 기대됩니다 (보통 1분보다 짧은 정도). 음악 사운드트랙같은 더 긴 소리들에 대해서는, 스트리밍은 audio 요소와 MediaElementAudioSourceNode와 함께 사용되어야 합니다.</p>
        <br>
        <p>AudioBuffer는 하나 이상의 AudioContext에 의해 사용될 수 있고, OfflineAudioContext와 AudioContext 사이에서 공유될 수 있습니다.</p>
        <br>
        <p>AudioBuffer는 4개의 내부 슬롯을 갖습니다.</p>
        <dl>
          <dt><i>[[number of channels]]</i></dt>
          <dd>이 AudioBuffer의 오디오 채널의 수. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[length]]</i></dt>
          <dd>이 AudioBuffer의 각 채널의 길이. / 자료형: unsigned long</dd>
          <br>
          <dt><i>[[sample rate]]</i></dt>
          <dd>이 AudioBuffer의 샘플 레이트 (단위: Hz). / 자료형: float</dd>
          <br>
          <dt><i>[[internal data]]</i></dt>
          <dd>오디오 샘플 데이터를 가지고 있는 데이터 블럭.</dd>
        </dl>
        <br>
        <pre>
[Exposed=Window]
interface AudioBuffer {
  constructor (AudioBufferOptions options);
  readonly attribute float sampleRate;
  readonly attribute unsigned long length;
  readonly attribute double duration;
  readonly attribute unsigned long numberOfChannels;
  Float32Array getChannelData (unsigned long channel);
  undefined copyFromChannel (Float32Array destination,
                             unsigned long channelNumber,
                             optional unsigned long bufferOffset = 0);
  undefined copyToChannel (Float32Array source,
                           unsigned long channelNumber,
                           optional unsigned long bufferOffset = 0);
};
        </pre>
        <br>
        <h3>1.4.1. 생성자</h3>
        <dl>
          <dt><i>AudioBuffer(options)</i></dt>
          <dd>추후 번역</dd>
          <br>
          <table>
            <caption>AudioBuffer.constructor() 메서드의 인자</caption>
            <tr>
              <td>매개변수</td>
              <td>자료형</td>
              <td>nullable</td>
              <td>optional</td>
              <td>설명</td>
            </tr>
            <tr>
              <td>options</td>
              <td>AudioBufferOptions</td>
              <td>x</td>
              <td>x</td>
              <td>이 AudioBuffer의 프로퍼티를 결정하는 AudioBufferOptions</td>
            </tr>
          </table>
        </dl>
        <br>
        <h3>1.4.2. 어트리뷰트</h3>
        <dl>
          <dt><i>duration</i> / 자료형: double / 읽기 전용</dt>
          <dd>PCM 오디오 데이터의 지속 기간 (단위: 초)</dd>
          <br>
          <dd>이것은 [[length]]와 [[sample rate]] 사이에서 나눗셈을 수행함으로써 AudioBuffer의 [[sample rate]]와 [[length]]로부터 계산됩니다.</dd>
          <br>
          <dt><i>length</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>샘플 프레임의 PCM 오디오 데이터의 길이. 이것은 반드시 [[length]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>numberOfChannels</i> / 자료형: unsigned long / 읽기 전용</dt>
          <dd>이산 오디오 채널의 수. 이것은 반드시 [[number of channels]]의 값을 반환해야만 합니다.</dd>
          <br>
          <dt><i>sampleRate</i> / 자료형: float / 읽기 전용</dt>
          <dd>초당 샘플 내의 PCM 오디오 데이터의 샘플 레이트. 이것은 반드시 [[sample rate]]의 값을 반환해야만 합니다.</dd>
        </dl>
        <br>
        <h3>1.4.3. 메서드</h3>
        <p>추후 번역</p>
        <br>
        <h3>1.4.4. AudioBufferOptions</h3>
        <p>추후 번역</p>
        <br>
        <h4>1.4.4.1. 딕셔너리 AudioBufferOptions 멤버</h4>
        <p>추후 번역</p>
        <br>
        <h2>1.5. AudioNode 인터페이스</h2>
        <p>AudioNode는 AudioContext의 구성 요소입니다. 이 인터페이스는 오디오 소스, 오디오 목적지, 중간 프로세싱 모듈을 나타냅니다. 이 모듈들은 오디오를 렌더링하여 오디오 하드웨어로 보내는 프로세싱 그래프를 형성하기 위해 함께 연결될 수 있습니다. 각 노드들은 입력 그리고/또는 출력을 가질 수 있습니다. 소스 노드는 입력이 없고 하나의 출력이 있습니다. 필터와 같은 대부분의 프로세싱 노드들은 하나의 입력과 하나의 출력을 갖습니다. 각 유형의 AudioNode는 어떻게 이것이 오디오를 처리하거나 합성하는지에 대한 세부 사항이 다릅니다. 하지만, 일반적으로, AudioNode는 (입력이 있다면) 입력을 처리하고, (출력이 있다면) 출력으로 오디오를 생성합니다.</p>
        <br>
        <p>각 출력은 하나 이상의 채널을 가집니다. 채널의 정확한 수는 각 AudioNode의 세부 사항에 달려 있습니다.</p>
        <br>
        <p>출력은 하나 이상의 AudioNode 입력에 연결될 수 있으므로, 팬 아웃이 지원됩니다. 입력은 초기적으로 연결을 가지고 있지 않지만, 하나 이상의 AudioNode 출력으로부터 연결될 수 있으므로, 핸 인이 지원됩니다. connect() 메서드가 AudioNode의 출력을 다른 AudioNode의 입력에 연결하기 위해 호출되었을 때, 우리는 이것을 입력에의 연결이라고 부릅니다.</p>
        <br>
        <p>각 AudioNode 입력은 특정한 수의 채널을 주어진 시간에 가지고 있습니다. 이 숫자는 입력에 만들어진 연결(들)에 따라 바뀔 수 있습니다. 만약 입력이 연결을 가지고 있지 않다면 이것은 소리가 없는(silent) 하나의 채널을 가지고 있습니다.</p>
        <br>
        <p>각 입력에 대해, AudioNode는 그 입력으로 가는 모든 연결의 믹싱을 수행합니다. 규범적인 필요조건과 세부 사항에 대해서는 § 4 채널 업 믹싱과 다운 믹싱을 참고해 보세요.</p>
        <br>
        <p>입력의 프로세싱과 AudioNode의 내부 연산은 AudioContext 시간에 대하여 계속하여 발생하는데, 이는 노드가 연결된 출력을 가지고 있는지의 여부에 관계없으며, 이 출력이 궁극적으로 AudioContext의 AudioDestinationNode에 도달하는지의 여부에도 관계없습니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioNode : EventTarget {
  AudioNode connect (AudioNode destinationNode,
                     optional unsigned long output = 0,
                     optional unsigned long input = 0);
  undefined connect (AudioParam destinationParam, optional unsigned long output = 0);
  undefined disconnect ();
  undefined disconnect (unsigned long output);
  undefined disconnect (AudioNode destinationNode);
  undefined disconnect (AudioNode destinationNode, unsigned long output);
  undefined disconnect (AudioNode destinationNode,
                        unsigned long output,
                        unsigned long input);
  undefined disconnect (AudioParam destinationParam);
  undefined disconnect (AudioParam destinationParam, unsigned long output);
  readonly attribute BaseAudioContext context;
  readonly attribute unsigned long numberOfInputs;
  readonly attribute unsigned long numberOfOutputs;
  attribute unsigned long channelCount;
  attribute ChannelCountMode channelCountMode;
  attribute ChannelInterpretation channelInterpretation;
};
        </pre>
        <br>
        <h3>1.5.1. AudioNode 생성</h3>
        <p>AudioNode는 두 가지 방법으로 생성될 수 있는데 그 방법이란 특정한 인터페이스에 대한 생성자를 사용하는 것 혹은 BaseAudioContext 또는 AudioContext의 팩토리 메서드를 사용하는 것입니다.</p>
        <br>
        <p>AudioNode의 생성자의 첫번째 인자로써 전달되는 BaseAudioContext는 생성될 AudioNode의 연관된 BaseAudioContext라고 불려집니다. 유사하게, 팩토리 메서드를 사용할 때, AudioNode의 연관된 BaseAudioContext는 이 팩토리 메서드가 호출된 BaseAudioContext입니다.</p>
        <br>
        <p>추후 번역</p>
        <br>
        <p>팩토리 메서드의 연관된 인터페이스란 이 메서드로부터 반환된 객체의 인터페이스입니다. 인터페이스의 연관된 옵션 객체란 이 인터페이스의 생성자에 전달될 수 있는 옵션 객체입니다.</p>
        <br>
        <p>AudioNode는 [DOM]에 기술된 대로, EventTarget입니다. 이것이 의미하는 바는 다른 EventTarget들이 이벤트를 받아들이는 것과 같은 방식으로 AudioNode에 이벤트를 전파할 수 있다는 것입니다.</p>
        <br>
        <pre>
enum ChannelCountMode {
  "max",
  "clamped-max",
  "explicit"
};
        </pre>
        <br>
        <p>추후 번역</p>
        <br>
        <pre>
enum ChannelInterpretation {
  "speakers",
  "discrete"
};
        </pre>
        <br>
        <p>추후 번역</p>
        <br>
        <h3>1.5.2. AudioNode 꼬리 시간(tail-time)</h3>
        <p>AudioNode는 꼬리 시간을 가질 수 있습니다. 이는 AudioNode에 아무 소리도 입력되지 않았을 지라도(fed silence), 출력은 소리가 있을 수 있다(non-silent)는 것입니다.</p>
        <br>
        <p>만약 과거의 입력이 미래의 출력에 영향을 주는 것과 같은 내부 프로세싱 상태를 AudioNode가 가지고 있다면 AudioNode는 0이 아닌 꼬리 시간을 가집니다. AudioNode는 심지어 입력이 소리 있음에서 소리 없음으로 전이된 후일지라도 계산된 꼬리 시간동안 소리가 있는 출력을 생성하는 것을 계속할 수도 있습니다.</p>
        <br>
        <h3>1.5.3. AudioNode 생명 주기</h3>
        <p>만약 다음의 조건 중 어느 하나라도 부합한다면, AudioNode는 render quantum 도중에 활발히 처리 중(actively processing)일 수 있습니다.</p>
        <p>추후 번역</p>
        <p>활발히 처리 중이 아닌 AudioNode는 무음을 내는 채널 하나를 출력합니다.</p>
        <br>
        <h3>1.5.4. 어트리뷰트</h3>
        <p>추후 번역</p>
        <br>
        <h3>1.5.5. 메서드</h3>
        <p>추후 번역</p>
        <br>
        <h3>1.5.6. AudioNodeOptions</h3>
        <p>추후 번역</p>
        <br>
        <h4>1.5.6.1. 딕셔너리 AudioNodeOptions 멤버</h4>
        <p>추후 번역</p>
        <br>
        <h2>1.6. AudioParam 인터페이스</h2>
        <p>AudioParam은 AudioNode의 기능의 개별적 측면을 제어하는데, 예를 들자면 음량이 있습니다. 이 파라미터는 value 어트리뷰트를 사용하여 특정한 값으로 즉시 설정될 수 있습니다. 또는, 값은 아주 정밀한 시간에 (AudioContext의 currentTime 어트리뷰트의 좌표계에서) 발생하도록 예정될 수도 있는데, 엔벨로프, 볼륨 페이드, LFO, filter sweep, grain window 등이 그 예입니다. 이 방법으로, 임의적인 타임라인 기반의 자동화 곡선이 모든 AudioParam에 대해서 설정될 수 있습니다. 추가적으로, AudioNode의 출력으로부터의 오디오 신호는 AudioParam에 연결될 수 있고 고유 파라미터 값으로 평가됩니다.</p>
        <br>
        <p>몇몇 합성과 AudioNode 처리는 어트리뷰트의 값이 반드시 '오디오 샘플 당' 기반으로 고려되어져야만 하는 어트리뷰트로써 AudioParam을 가집니다. 다른 AudioParam에 대해서는, 샘플 정확성은 중요하지 않으며 값 변화는 더욱 조악하게 샘플될 수 있습니다. 개개의 AudioParam은 이것이 a-rate 파라미터인지 k-rate 파라미터인지를 명시할 것입니다. a-rate 파라미터란 이것의 값이 반드시 '오디오 샘플 당' 기반으로 고려되어져야 함을 의미합니다.</p>
        <br>
        <p>구현은 반드시 블럭 프로세싱을 사용해야 하며, 각 AudioNode는 하나의 render quantum을 처리합니다.</p>
        <br>
        <p>각 render quantum에 대해, <strong><i>k-rate</i></strong> 파라미터의 값은 반드시 첫 번째 샘플 프레임의 시간에 샘플되어야만 하고, 그 값은 반드시 전체 블럭에 대해 사용되어야만 합니다. <strong><i>a-rate</i></strong> 파라미터의 값은 반드시 블럭의 각 샘플 프레임에 대해 샘플되어야만 합니다. AudioParam에 따라, 이것의 rate는 automationRate 어트리뷰트를 "a-rate"나 "k-rate"로 설정함으로써 제어될 수 있습니다. 추가적인 세부 사항은 각 AudioParam의 설명을 참고해 보세요.</p>
        <br>
        <p>각 AudioParam은 이 파라미터의 <strong>단순한 명목상의 범위</strong>를 함께 형성하는 minValue와 maxValue 어트리뷰트를 포함하고 있습니다. 실제로는, 파라미터의 값은 [minValue, maxValue] 범위로 강제됩니다. 전체 사항에 대해서는 § 1.6.3. 값의 계산을 참고해 보세요.</p>
        <br>
        <p>많은 AudioParam의 경우 minValue와 maxValue는 최대로 가능한 범위로 설정되는 것이 의도됩니다. 이 경우, maxValue는 <strong><i>most-positive-single-float</i></strong> 값으로 설정되어야 하는데, 이 값은 3.4028235e38입니다. (그러나, 오직 IEEE-754 double precision float value만을 지원하는 JavaScript에서는, 이것은 반드시 3.4028234663852886e38로 작성되어야만 합니다.) 유사하게, minValue는 <strong><i>most-negative-single-float</i></strong>로 설정되어야 하고, 이 값은 -3.4028235e38입니다. (유사하게, 이것은 반드시 JavaScript에서 -3.4028234663852886e38로 작성되어야만 합니다.)</p>
        <br>
        <p>AudioParam은 0개 이상의 <strong>자동화 이벤트</strong>를 유지합니다. 각 자동화 이벤트는, AudioContext의 currentTime 어트리뷰트의 시간 좌표계의 <strong>자동화 이벤트 시간</strong>에 관련하여, 특정한 시간 범위에 대한 파라미터의 값 변화를 명시합니다. 자동화 이벤트의 리스트는 자동화 이벤트 시간의 오름차순으로 유지됩니다.</p>
        <p>주어진 자동화 이벤트의 행동은 이 이벤트의 자동화 이벤트 시간과 리스트의 인접한 이벤트의 자동화 이벤트 뿐만이 아니라, AudioContext의 현재 시간의 함수입니다. 다음의 <strong>자동화 메서드</strong>들은 이벤트 리스트에 메서드에 따른 새로운 이벤트를 추가함으로써 이벤트 리스트를 변경시킵니다.</p>
        <ul>
          <li>setValueAtTime() - setValue</li>
          <li>linearRampToValueAtTime() - LinearRampToValue</li>
          <li>exponentialRampToValueAtTime() - exponentialRampToValue</li>
          <li>setTargetAtTime() - setTarget</li>
          <li>setValueCurveAtTime() - setValueCurve</li>
        </ul>
        <br>
        <p>다음의 규칙이 이 메서드들을 호출할 때 적용될 것입니다:</p>
        <ul>
          <li>자동화 이벤트 시간은 일반적인 샘플 레이트에 관하여 양자화되지 않습니다. 곡선과 경사를 결정하는 공식은 이벤트를 예정할 때 주어진 정확한 수치적 시간에 적용됩니다.</li>
          <li>만약 이 이벤트들 중 하나가 이미 하나 이상의 이벤트가 있는 시간에 추가되었다면, 이 이벤트는 그 이벤트들 이후에 리스트에 배치될 것이지만, 시간이 그 이벤트 이후에 있는 이벤트 전입니다.</li>
          <li>만약 setValueCurveAtTime()이 시간 T와 기간 D로 호출되고 T보다 엄격하게 큰 시간을 가지지만, T+D보다는 엄격히 작은 어떠한 이벤트가 있다면, NotSupportedError 예외가 반드시 발생되어야 합니다. 요컨대, 다른 이벤트를 포함하고 있는 시간 기간 동안에 value curve를 예정하는 것은 괜찮지 않지만, 다른 이벤트의 시간에 정확히 value curve를 예정하는 것은 괜찮습니다.</li>
          <li>유사하게 만약 [T, T+D)에 포함된 시간에 자동화 메서드가 호출되었다면 NotSupportedError가 반드시 발생되어야만 합니다 (T는 curve의 시간이고 D는 기간임).</li>
        </ul>
        <br>
        <div class="note">
          <p>노트: AudioParam 어트리뷰트는 읽기 전용입니다. 단, value 어트리뷰트는 예외입니다.</p>
        </div>
        <br>
        <p>AudioParam의 automation rate는 다음의 값 중 하나와 함께 automationRate 어트리뷰트를 설정함으로써 선택될 수 있습니다. 그러나, 몇몇 AudioParam은 automation rate가 변경될 수 있는지에 대한 제약을 가지고 있습니다.</p>
        <br>
        <p>추후 번역</p>
        <br>
        <h3>1.6.1. 어트리뷰트</h3>
        <h3>1.6.2. 메서드</h3>
        <h3>1.6.3. 값의 계산</h3>
        <br>
        <h2>1.7. AudioScheduledSourceNode 인터페이스</h2>
        <p>이 인터페이스는 AudioBufferSourceNode, ConstantSourceNode, OscillatorNode와 같은 소스 노드의 공통적인 기능을 나타냅니다.</p>
        <br>
        <p>소스가 시작되기 전에 (start()를 호출함으로써) 소스 노드는 반드시 무음 (0) 을 출력해야 합니다. 소스가 정지된 후 (stop()을 호출함으로써), 소스는 반드시 무음을 출력해야 합니다 (0).</p>
        <br>
        <p>AudioScheduledSourceNode는 직접적으로 인스턴스화될 수 없지만, 대신 소스 노드에 대한 구체적인 인터페이스에 의해 확장됩니다.</p>
        <br>
        <p>AudioScheduledSourceNode는 이것의 연관된 BaseAudioContext의 currentTime이 AudioScheduledSourceNode가 시작되기로 설정된 시간보다 크거나 같고, 이것이 정지되기로 설정된 시간보다 작을 때 <strong>재생</strong>된다고 말해집니다.</p>
        <br>
        <p>AudioScheduledSourceNode는 내부 boolean 슬롯 [[source started]]와 함께 생성되는데, 이는 초기적으로 false로 설정되어 있습니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioScheduledSourceNode : AudioNode {
  attribute EventHandler onended;
  undefined start(optional double when = 0);
  undefined stop(optional double when = 0);
};
        </pre>
        <br>
        <h3>1.7.1. 어트리뷰트</h3>
        <h3>1.7.2. 메서드</h3>
        <br>
        <h2>1.8. AnalyserNode 인터페이스</h2>
        <p>이 인터페이스는 실시간 주파수 그리고 시간 영역 분석 정보를 제공 가능한 노드를 나타냅니다. 오디오 스트림은 입력에서 출력까지 처리되지 않은 채로 전달될 것입니다.</p>
        <br>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td>이 output은 연결되지 않은 채로 남아있을 수도 있습니다.</td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface AnalyserNode : AudioNode {
  constructor (BaseAudioContext context, optional AnalyserOptions options = {});
  undefined getFloatFrequencyData (Float32Array array);
  undefined getByteFrequencyData (Uint8Array array);
  undefined getFloatTimeDomainData (Float32Array array);
  undefined getByteTimeDomainData (Uint8Array array);
  attribute unsigned long fftSize;
  readonly attribute unsigned long frequencyBinCount;
  attribute double minDecibels;
  attribute double maxDecibels;
  attribute double smoothingTimeConstant;
};
        </pre>
        <br>
        <h3>1.8.1. 생성자</h3>
        <h3>1.8.2. 어트리뷰트</h3>
        <h3>1.8.3. 메서드</h3>
        <h3>1.8.4. AnalyserOptions</h3>
        <h4>1.8.4.1. 딕셔너리 AnalyserOptions 멤버</h4>
        <h3>1.8.5. 시간 영역 다운 믹싱</h3>
        <p>현재 시간 영역 데이터가 계산될 때, 입력 신호는 반드시 마치 channelCount가 1이고, channelCountMode가 "max"이고, channelInterpretation이 "speakers"인 것처럼 모노로 다운믹스되어야만 합니다. 이것은 AnalyserNode 자체의 설정과는 무관합니다. 가장 최신의 fftSize 프레임이 다운믹싱 연산에 사용됩니다.</p>
        <br>
        <h3>1.8.6. 시간에 따른 FFT windowing과 smoothing</h3>
        <p>현재 주파수 데이터가 계산될 때, 다음의 연산이 수행될 것입니다.</p>
        <br>
        <ol>
          <li>현재 시간 영역 데이터를 계산합니다.</li>
          <li>Blackman window를 시간 영역 입력 데이터에 적용합니다.</li>
          <li>푸리에 변환을 window된 시간 영역 입력 데이터에 적용하여 real과 imaginary 주파수 데이터를 얻습니다.</li>
          <li>시간에 따라 주파수 영역 데이터를 smooth합니다.</li>
          <li>dB로 변환합니다.</li>
        </ol>
        <br>
        <h2>1.9. AudioBufferSourceNode 인터페이스</h2>
        <p>이 인터페이스는 Audiobuffer 내의 in-memory 오디오 에셋으로부터의 오디오 소스를 나타냅니다. 이것은 높은 정도의 스케쥴링 유연성과 정확성을 요구하는 오디오 에셋 재생에 유용합니다. 만약 network- 또는 disk-backed 에셋의 sample-accurate 재생이 요구된다면, 구현자는 AudioWorkletNode를 사용하여 재생을 구현해야 합니다.</p>
        <br>
        <p>start() 메서드는 사운드 재생이 일어날 때를 예정하기 위해 사용됩니다. start() 메서드는 여러 번 호출될 수는 없을지도 모릅니다. 재생은 버퍼의 오디오 데이터가 완전히 재생되었을 때 (만약 loop 어트리뷰트가 false라면), 혹은 stop() 메서드가 호출되고 명시된 시간에 도달했을 때 자동적으로 정지할 것입니다. 자세한 사항은 start()와 stop() 설명을 참고해 보세요.</p>
        <br>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력의 채널의 수는 buffer 어트리뷰트에 할당된 AudioBuffer의 채널의 수와 동일하거나, 만약 buffer가 null이면 무음인 채널 하나입니다.</p>
        <br>
        <p>추가적으로, 만약 버퍼가 하나 이상의 채널을 가지고 있다면, AudioBufferSourceNode 출력은 다음의 조건 중 하나가 부합하는 시간 이후에 반드시 render quantum의 시작에 무음인 하나의 채널로 변경되어야만 합니다.</p>
        <br>
        <ul>
          <li>버퍼의 끝에 도달했습니다.</li>
          <li>지속 기간에 도달했습니다.</li>
          <li>정지 시간에 도달했습니다.</li>
        </ul>
        <br>
        <p>AudioBufferSourceNode의 <strong>playhead 포지션</strong>은 시간 오프셋 (초) 을 나타내는 어떠한 양으로써 정의되고, 이 오프셋은 버퍼의 첫번째 샘플 프레임의 시간 좌표계에 관련있습니다. 이러한 값은 노드의 playbackRate와 detune 파라미터와는 별도로 생각되어져야 할 것입니다. 일반적으로, playhead 포지션은 subsample-accurate할 수 있고 정확한 샘플 프레임 포지션을 나타낼 필요가 없습니다. 이것은 0과 버퍼의 지속 기간 사이에서 유효한 값을 추정할 수 있습니다.</p>
        <br>
        <p>playbackRate와 detune 어트리뷰트는 복합적인 파라미터를 형성합니다. 이것들은 함께 사용되어 <strong><i>computedPlaybackRate</i></strong> 값을 결정합니다.</p>
        <br>
        <p>computedPlaybackRate(t) = playbackRate(t) * pow(2, detune(t) / 1200)</p>
        <br>
        <p>이 복합적인 파라미터의 명목상의 범위는 (−∞,∞)입니다.</p>
        <br>
        <p>AudioBufferSourceNode는 내부 boolean 슬롯 [[buffer set]]과 함께 생성되는데, 이는 초기적으로 false로 설정됩니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioBufferSourceNode : AudioScheduledSourceNode {
  constructor (BaseAudioContext context,
               optional AudioBufferSourceOptions options = {});
  attribute AudioBuffer? buffer;
  readonly attribute AudioParam playbackRate;
  readonly attribute AudioParam detune;
  attribute boolean loop;
  attribute double loopStart;
  attribute double loopEnd;
  undefined start (optional double when = 0,
                   optional double offset,
                   optional double duration);
};
        </pre>
        <br>
        <h3>1.9.1. 생성자</h3>
        <h3>1.9.2. 어트리뷰트</h3>
        <h3>1.9.3. 메서드</h3>
        <h3>1.9.4. AudioBufferSourceOptions</h3>
        <h4>1.9.4.1. 딕셔너리 AudioBufferSourceOptions 멤버</h4>
        <h3>1.9.5. 반복</h3>
        <h3>1.9.6. AudioBuffer 내용의 재생</h3>
        <br>
        <h2>1.10. AudioDestinationNode 인터페이스</h2>
        <p>이것은 최종 오디오 목적지를 나타내는 AudioNode이며 유저가 최종적으로 듣게 될 것입니다. 이것은 종종 스피커에 연결되는 오디오 출력 장치로써 여겨집니다. 들릴 모든 렌더링된 오디오는 이 노드로 라우팅될 것입니다. 즉, AudioContext의 라우팅 그래프의 "최종" 노드입니다. AudioContext당 오직 하나의 AudioDestinationNode가 있는데, 이는 AudioContext의 destination 어트리뷰트를 통해 제공됩니다.</p>
        <br>
        <p>AudioDestinationNode의 출력은 이것의 입력을 합침으로써 생산되며, MediaStreamAudioDestinationNode 또는 MediaRecorder로 가는 AudioContext의 출력을 캡쳐할 수도 있습니다 ([mediastream-recording] 에서 기술됩니다).</p>
        <br>
        <p>AudioDestinationNode는 AudioContext나 OfflineAudioContext의 목적지일 수 있고, 채널 속성들은 컨텍스트가 무엇인지에 달려 있습니다.</p>
        <br>
        <p>AudioContext에 대해, 기본값은 아래와 같습니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"explicit"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>channelCount는 maxChannelCount보다 작거나 같은 값 무엇으로든 설정될 수 있습니다. 만약 이 값이 유효한 범위 내에 없다면 IndexSizeError는 반드시 발생되어야 합니다. 구체적인 예제로써, 만약 오디오 하드웨어가 8 채널의 출력을 지원한다면, channelCount는 8로 설정되고, 출력의 8 채널을 렌더링할 수 있을 것입니다.</p>
        <br>
        <p>OfflineAudioContext에 대해, 기본값은 아래와 같습니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>numberOfChannels</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"explicit"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterPretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>numberOfChannels는 OfflineAudioContext를 생성할 때 명시됩니다. 이 값은 변하지 않을 것입니다. 만약 channelCount가 다른 값으로 변경된다면 NotSupportedError 예외가 반드시 발생되어야 합니다.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioDestinationNode : AudioNode {
  readonly attribute unsigned long maxChannelCount;
};
        </pre>
        <br>
        <h3>1.10.1. 어트리뷰트</h3>
        <p>추후 번역</p>
        <br>
        <h2>1.11. AudioListener 인터페이스</h2>
        <p>이 인터페이스는 오디오 장면을 듣고 있는 사람의 위지와 방향을 나타냅니다. 모든 PannerNode 객체는 BaseAudioContext의 listener에 관하여 공간화합니다. § 6 공간화에 대한 더욱 자세한 사항은 공간화/패닝을 참고하세요.</p>
        <br>
        <p>positionX, positionY, positionZ 파라미터는 3D 카르테시안 좌표공간에서의 listener의 위치를 나타냅니다. PannerNode 객체는 공간화를 위해 각 오디오 소스에 관련된 이 위치를 사용합니다.</p>
        <br>
        <p>forwardX, forwardY, forwardZ 파라미터는 3D 공간에서의 방향 벡터를 나타냅니다. forward 벡터와 up 벡터 둘 다 listener의 방향을 결정하기 위해 사용됩니다. 간단한 말로 하자면, forward 벡터는 사람의 코가 가리키고 있는 방향을 나타냅니다. up 벡터는 사람의 머리 위가 가리키고 있는 방향을 나타냅니다. 이 두 벡터는 선형적으로 독립적일 것이 기대됩니다. 어떻게 이 값들이 해석되어야 하는지에 대한 규범적인 요구 조건에 대해서는, § 6 공간화/패닝 섹션을 참고해 보세요.</p>
        <br>
        <pre>
[Exposed=Window]
interface AudioListener {
  readonly attribute AudioParam positionX;
  readonly attribute AudioParam positionY;
  readonly attribute AudioParam positionZ;
  readonly attribute AudioParam forwardX;
  readonly attribute AudioParam forwardY;
  readonly attribute AudioParam forwardZ;
  readonly attribute AudioParam upX;
  readonly attribute AudioParam upY;
  readonly attribute AudioParam upZ;
  undefined setPosition (float x, float y, float z);
  undefined setOrientation (float x, float y, float z, float xUp, float yUp, float zUp);
};
        </pre>
        <br>
        <h3>1.11.1. 어트리뷰트</h3>
        <h3>1.11.2. 메서드</h3>
        <h3>1.11.3. 프로세싱</h3>
        <br>
        <h2>1.12. AudioProcessingEvent 인터페이스 -- 폐기됨</h2>
        <h3>1.12.1. 어트리뷰트</h3>
        <h3>1.12.2. AudioProcessingEventInit</h3>
        <h4>1.12.2.1. 딕셔너리 AudioProcessingEventInit 멤버</h4>
        <br>
        <h2>1.13. BiquadFilterNode 인터페이스</h2>
        <p>BiquadFilterNode는 아주 일반적인 low-order 필터를 구현하는 AudioNode 프로세서입니다.</p>
        <br>
        <p>low-order 필터들은 기본적인 음색 제어 (bass, mid, treble), 그래픽 이퀼라이저, 더욱 발전된 필터의 구성 요소입니다. 다수의 BiquadFilterNode 필터는 결합되어 더욱 복잡한 필터를 형성할 수 있습니다. 주파수와 같은 필터의 파라미터는 filter sweep등을 위해 시간에 따라 변할 수 있습니다. 각 BiquadFilterNode는 아래의 IDL에 보여지는 몇 개의 일반적인 필터 유형 중 하나로써 설정될 수 있습니다. 기본 필터 유형은 "lowpass"입니다.</p>
        <br>
        <p>추후 번역</p>
        <h2>1.14. ChannelMergerNode 인터페이스</h2>
        <p>ChannelMergerNode는 더욱 고급의 어플리케이션에서의 사용을 위한 것이고 종종 ChannelSplitterNode와 함께 사용될 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 인터페이스는 다수의 오디오 스트림으로부터 하나의 오디오 스트림으로 채널을 합치기 위한 AudioNode를 나타냅니다. 이것은 가변적인 입력의 수 (기본값 6)을 가지지만, 이것의 모두가 연결될 필요가 있는 것은 아닙니다. 하나의 출력이 있는데, 이 출력의 오디오 스트림은 입력들 중 어느 것이 활발히 처리 중일 때 입력의 수와 동일한 채널의 수를 가집니다. 만약 입력 중 아무 것도 활발히 처리 중이 아니라면, 출력은 무음인 채널 하나입니다.</p>
        <br>
        <h2>1.15. ChannelSplitterNode 인터페이스</h2>
        <p>ChannelSplitterNode는 더욱 고급의 어플리케이션에서의 사용을 위한 것이고 종종 ChannelMergerNode와 함께 사용될 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 인터페이스는 라우팅 그래프에서 오디오 스트림의 각 채널에 접근하기 위한 AudioNode를 나타냅니다. 이것은 하나의 입력을 가지고 있고, 입력 오디오 스트림의 채널의 수와 같은 "작동하는" 출력의 수를 가지고 있습니다. 예를 들어, 스테레오 입력이 ChannelSplitterNode에 연결되었다면 작동하는 출력의 수는 2일 것입니다 (좌측 채널로부터 1개, 우측 채널로부터 1개). 항상 총 수 N의 출력이 있으며 (AudioContext 메서드 createChannelSplitter()의 numberOfOutputs 매개변수에 의해 결정됨), 기본값은 만약 이 값이 제공되지 않았다면 6입니다. "작동"하지 않는 모든 출력은 무음을 출력할 것이고 통상적으로 어떠한 것에도 연결되지 않을 것입니다.</p>
        <br>
        <h2>1.16. ConstantSourceNode 인터페이스</h2>
        <p>이 인터페이스는 출력이 명목상 상수 값인 고정된 오디오 소스를 나타냅니다. 이것은 일반적으로 고정된 소스 노드로서 유용하고 이것은 마치 이것의 오프셋을 자동화함으로써 또는 이것에 다른 노드를 연결함으로써 생성 가능한 AudioParam인 것처럼 사용될 수 있습니다.</p>
        <br>
        <p>이 노드의 하나의 출력은 한 개의 채널 (모노) 로 이루어져 있습니다.</p>
        <br>
        <h2>1.17. ConvolverNode 인터페이스</h2>
        <p>이 인터페이스는 임펄스 응답이 주어졌을 때 선형 콘볼루션 이펙트를 적용하는 프로세싱 노드를 나타냅니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 모노 (1 채널) 혹은 스테레오 (2 채널) 이고 증가될 수 없습니다. 더욱 많은 채널을 가진 노드로부터의 연결은 적절하게 다운믹스될 것입니다.</p>
        <p>이 노드에는 channelCount 제약과 channelCountMode 제약이 있습니다. 이 제약들은 이 노드에의 입력이 모노나 스테레오임을 보장합니다.</p>
        <br>
        <h2>1.18. DelayNode 인터페이스</h2>
        <p>딜레이 라인은 오디오 어플리케이션에서 기본적인 구성 요소입니다. 이 인터페이스는 하나의 입력과 하나의 출력을 가진 AudioNode입니다.</p>
        <p>표는 추후 번역</p>
        <p>출력의 채널 수는 항상 입력의 채널 수와 같습니다.</p>
        <p>이것은 들어오는 오디오 신호를 특정한 양만큼 지연시킵니다. 정확히 말하자면, 각 시간 t, 입력 신호 input(t), 지연 시간 delayTime(t), 출력 신호 output(t)에서, 출력은 output(t) = input(t - delayTime(t))일 것입니다. 기본 delayTime은 0초 (지연 없음) 입니다.</p>
        <p>DelayNode의 입력의 채널 수가 바뀔 때 (따라서 출력 채널 수도 바뀜), 노드에 의해 아직 출력되지 않았고 이것의 내부 상태의 일부인 지연된 오디오 샘플들이 있을 수 있습니다. 만약 이 샘플들이 다른 채널 카운트와 함께 일찍 수신되었다면, 이것들은 반드시 새롭게 수신된 입력과 혼합되기 전에 업믹스되거나 다운믹스되어 모든 내부 딜레이 라인 믹싱이 하나의 지배적인 채널 레이아웃을 사용하여 일어나도록 해야 합니다.</p>
        <div class="note">
          <p>노트: 정의에 의해, DelayNode는 딜레이의 양과 동일한 오디오 프로세싱 레이턴시를 삽입합니다.</p>
        </div>
        <br>
        <pre>
[Exposed=Window]
interface DelayNode : AudioNode {
  constructor (BaseAudioContext context, optional DelayOptions options = {});
  readonly attribute AudioParam delayTime;
};
        </pre>
        <br>
        <h2>1.19. DynamicsCompressorNode 인터페이스</h2>
        <p>DynamicsCompressorNode는 dynamics compression 이펙트를 구현하는 AudioNode 프로세서입니다.</p>
        <p>dynamics compression은 음악 프로덕션과 게임 오디오에서 아주 일반적으로 사용됩니다. 이것은 신호의 가장 소리가 큰 부분의 볼륨을 줄이고 가장 작은 부분의 볼륨을 높입니다. 전반적으로, 크고, 풍부하고, 꽉 찬 소리가 달성될 수 있습니다. 이것은 다수의 각 사운드가 동시에 재생되어 전반적인 신호 레벨을 제어하고 스피커로 가는 오디오 출력이 클리핑(왜곡)되는 것을 방지하는 것을 돕는 게임과 뮤지컬 어플리케이션에서 보통 중요합니다.</p>
        <br>
        <h2>1.20. GainNode 인터페이스</h2>
        <p>오디오 신호의 gain을 변경하는 것은 오디오 어플리케이션에서 기본적인 작동입니다. 이 인터페이스는 하나의 입력과 하나의 출력을 가진 AudioNode입니다.</p>
        <p>표는 추후에 번역</p>
        <p>GainNode의 입력 데이터의 각 채널의 각 샘플은 반드시 gain AudioParam의 computedValue에 의해 곱해져야 합니다.</p>
        <pre>
[Exposed=Window]
interface GainNode : AudioNode {
  constructor (BaseAudioContext context, optional GainOptions options = {});
  readonly attribute AudioParam gain;
};
        </pre>
        <br>
        <h2>1.21. IIRFilterNode 인터페이스</h2>
        <p>IIRFilterNode는 일반적인 IIR 필터를 구현하는 AudioNode 프로세서입니다. 일반적으로, 다음의 이유로 higher-order 필터를 구현함에 있어 BiquadFilterNode를 사용하는 것이 최선입니다.</p>
        <ul>
          <li>일반적으로 수치적 문제에 덜 민감함</li>
          <li>필터 파라미터들이 자동화될 수 있음</li>
          <li>모든 even-ordered IIR 필터를 생성하기 위해 사용될 수 있음</li>
        </ul>
        <br>
        <p>그러나, odd-ordered 필터는 생성될 수 없으므로, 만약 그러한 필터가 필요하거나 자동화가 필요하지 않다면, IIR 필터가 적절할 수 있습니다.</p>
        <p>한 번 생성되고 나면, IIR 필터의 계수들은 변경될 수 없습니다.</p>
        <p>표는 추후 번역</p>
        <p>출력 채널의 수는 항상 입력 채널의 수와 같습니다.</p>
        <pre>
[Exposed=Window]
interface IIRFilterNode : AudioNode {
  constructor (BaseAudioContext context, IIRFilterOptions options);
  undefined getFrequencyResponse (Float32Array frequencyHz,
                                  Float32Array magResponse,
                                  Float32Array phaseResponse);
};
        </pre>
        <br>
        <h2>1.22. MediaElementAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 &lt;audio&gt; 혹은 &lt;video&gt; 요소로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 HTMLMediaElement에 의해 참조되는 미디어 채널의 수와 일치합니다. 따라서, 미디어 요소의 src 어트리뷰트에의 변화는 이 노드에 의해 출력되는 채널의 수를 변화시킬 수 있습니다.</p>
        <p>만약 HTMLMediaElement의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, HTMLMediaElement로부터의 출력은 반드시 리샘플링되어 컨텍스트의 샘플 레이트와 일치해야만 합니다.</p>
        <p>MediaElementAudioSourceNode는 AudioContext의 createMediaElementSource() 메서드 혹은 생성자의 MediaElementAudioSourceOptions 딕셔너리의 mediaElement 멤버를 사용함으로써 HTMLMediaElement가 주어졌을 때 생성됩니다.</p>
        <p>하나의 출력의 채널의 수는 createMediaElementSource()에의 인자로써 전달된 HTMLMediaElement에 의해 참조되는 오디오 채널의 수와 동일하거나, 만약 HTMLMediaElement가 오디오를 가지고 있지 않다면 1입니다.</p>
        <p>MediaElementAudioSourceNode가 생성된 이후에 HTMLMediaElement는 반드시 동일한 양식으로 행동해야만 하나, 하나의 예외가 있습니다. 그 예외란 렌더링된 오디오가 더 이상 직접적으로 들리지 않지만, 대신 라우팅 그래프를 통해 연결되어 있는 MediaElementAudioSourceNode의 결과로써 들리는 것입니다. 따라서 정지, 탐색, 볼륨, src 어트리뷰트 변화, HTMLMediaElement의 다른 측면들은 만약 MediaElementAudioSourceNode와 함께 사용되지 않는다면 반드시 이것들이 일반적으로 행동하는 것처럼 행동해야 합니다.</p>
        <pre>
예제 11
const mediaElement = document.getElementById('mediaElementID');
const sourceNode = context.createMediaElementSource(mediaElement);
sourceNode.connect(filterNode);
        </pre>
        <pre>
[Exposed=Window]
interface MediaElementAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaElementAudioSourceOptions options);
  [SameObject] readonly attribute HTMLMediaElement mediaElement;
};
        </pre>
        <h2>1.23. MediaStreamAudioDestinationNode 인터페이스</h2>
        <p>이 인터페이스는 kind가 "audio"인 하나의 MediaStreamTrack을 가진 MediaStream을 나타내는 오디오 목적지입니다. 이 MediaStream은 노드가 생성될 때 생성되고 stream 어트리뷰트를 통해 접근 가능합니다. 이 스트림은 getUserMedia()를 통해 얻어진 MediaStream과 비슷한 방식으로 사용될 수 있고, 예를 들자면 RTCPeerConnection ([webrtc]에서 기술됨) addStream() 메서드를 사용해 원격 peer에게 전송될 수 있습니다.</p>
        <p>표는 추후 번역</p>
        <p>입력 채널의 수는 기본적으로 2 입니다 (스테레오).</p>
        <pre>
[Exposed=Window]
interface MediaStreamAudioDestinationNode : AudioNode {
  constructor (AudioContext context, optional AudioNodeOptions options = {});
  readonly attribute MediaStream stream;
};
        </pre>
        <br>
        <h2>1.24. MediaStreamAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 MediaStream으로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 MediaStreamTrack의 채널의 수와 일치합니다. MediaStreamTrack이 끝났을 때, 이 AudioNode는 무음인 하나의 채널을 출력합니다.</p>
        <p>만약 MediaStreamTrack의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, MediaStreamTrack의 출력은 컨텍스트의 샘플 레이트와 일치되기 위해 리샘플링됩니다.</p>
        <pre>
[Exposed=Window]
interface MediaStreamAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaStreamAudioSourceOptions options);
  [SameObject] readonly attribute MediaStream mediaStream;
};
        </pre>
        <br>
        <h2>1.25. MediaStreamTrackAudioSourceNode 인터페이스</h2>
        <p>이 인터페이스는 MediaStreamTrack으로부터의 오디오 소스를 나타냅니다.</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time 참조</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <p>출력 채널의 수는 mediaStreamTrack의 채널의 수와 일치합니다.</p>
        <p>만약 MediaStreamTrack의 샘플 레이트가 연관된 AudioContext의 샘플 레이트와 다르다면, mediaStreamTrack의 출력은 컨텍스트의 샘플 레이트와 일치되도록 리샘플링될 것입니다.</p>
        <pre>
[Exposed=Window]
interface MediaStreamTrackAudioSourceNode : AudioNode {
  constructor (AudioContext context, MediaStreamTrackAudioSourceOptions options);
};
        </pre>
        <br>
        <h2>1.26. OscillatorNode 인터페이스</h2>
        <p>OscillatorNode는 주기적인 파형을 생성하는 오디오 소스를 나타냅니다. 이것은 몇 가지의 일반적으로 사용되는 파형으로 설정될 수 있습니다. 추가적으로, PeriodicWave 객체의 사용을 통해 이것은 임의의 주기적인 파형으로 설정될 수 있습니다.</p>
        <p>오실레이터는 오디오 합성에서 일반적인 기본 구성 요소입니다. OscillatorNode는 start() 메서드에 의해 명시된 시간에 소리를 발산하기 시작할 것입니다.</p>
        <p>수학적으로 말해서, 연속 시간 주기 파형은 주파수 영역에서 고려되었을 때 아주 높은 (혹은 무한히 높은) 주파수 정보를 가질 수 있습니다. 이 파형이 특정한 샘플 레이트에서 이산 시간 디지털 오디오 신호로 샘플링되었을 때, 파형을 디지털 형태로 변환하기 전에 나이퀴스트 주파수보다 높은 고주파 정보를 폐기하기 (거르기) 위해 관심이 반드시 기울여져야 합니다. 만약 이러한 작업이 이루어지지 않는다면, (나이퀴스트 주파수보다) 높은 주파수의 에일리어싱이 나이퀴스트 주파수보다 낮은 주파수로 mirror image로서 fold back될 것입니다. 많은 경우에서 이것은 청각적으로 불쾌한 결과를 초래할 것입니다. 이것은 오디오 디지털 신호 처리의 기본적이고 잘 이해된 원칙입니다.</p>
        <p>구현이 이 에일리어싱을 방지하기 위해 취할 수 있는 몇 가지 실용적인 접근이 있습니다. 접근에 상관없이, 이상적인 이산 시간 디지털 오디오 신호는 수학적으로 잘 정의됩니다. 이 구현에 대한 trade-off는 이 이상을 달성하는 것에 대한 충실도(fidelity)에 대한 (CPU 사용의 측면에서의) 구현 비용의 문제입니다.</p>
        <p>구현이 이 이상을 달성함에 있어 얼마간의 주의를 취할 것이 기대되지만, lower-end 하드웨어에서 낮은 품질과 덜 비용이 드는 접근을 고려하는 것은 타당합니다.</p>
        <p>frequency와 detune은 둘 다 a-rate 파라미터이고, 복합적인 파라미터를 형성합니다. 이것들은 함께 사용되어 computedOscFrequency 값을 결정합니다.</p>
        <pre>computedOscFrequency(t) = frequency(t) * pow(2, detune(t) / 1200)</pre>
        <br>
        <p>각 시간에서의 OscillatorNode의 instantaneous 단계는 computedOscFrequency의 definite time integral이며, 노드의 정확한 시작 시간에서 0의 phase angle을 상정합니다. 이것의 명목상의 범위는 [-Nyquist 주파수, Nyquist 주파수] 입니다.</p>
        <p>이 노드의 하나의 출력은 하나의 채널로 이루어져 있습니다 (모노).</p>
        <table>
          <tr>
            <td>프로퍼티</td>
            <td>값</td>
            <td>참고</td>
          </tr>
          <tr>
            <td>numberOfInputs</td>
            <td>0</td>
            <td></td>
          </tr>
          <tr>
            <td>numberOfOutputs</td>
            <td>1</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCount</td>
            <td>2</td>
            <td></td>
          </tr>
          <tr>
            <td>channelCountMode</td>
            <td>"max"</td>
            <td></td>
          </tr>
          <tr>
            <td>channelInterpretation</td>
            <td>"speakers"</td>
            <td></td>
          </tr>
          <tr>
            <td>tail-time</td>
            <td>없음</td>
            <td></td>
          </tr>
        </table>
        <br>
        <pre>
enum OscillatorType {
  "sine",
  "square",
  "sawtooth",
  "triangle",
  "custom"
};
        </pre>
        <table>
          <thead>
            <tr>
              <th colspan="2">열거형 설명</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>"<i>sine</i>"</td>
              <td>사인파</td>
            </tr>
            <tr>
              <td>"<i>square</i>"</td>
              <td>duty period 0.5의 사각파</td>
            </tr>
            <tr>
              <td>"<i>sawtooth</i>"</td>
              <td>톱니파</td>
            </tr>
            <tr>
              <td>"<i>triangle</i>"</td>
              <td>삼각파</td>
            </tr>
            <tr>
              <td>"<i>custom</i>"</td>
              <td>사용자 정의 주기파</td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre>
[Exposed=Window]
interface OscillatorNode : AudioScheduledSourceNode {
  constructor (BaseAudioContext context, optional OscillatorOptions options = {});
  attribute OscillatorType type;
  readonly attribute AudioParam frequency;
  readonly attribute AudioParam detune;
  undefined setPeriodicWave (PeriodicWave periodicWave);
};
        </pre>
        <br>
        <h2>1.27. PannerNode 인터페이스</h2>
        <p>이 인터페이스는 들어오는 오디오 스트림을 3차원 공간에 위치시키는 / 공간화시키는 프로세싱 노드를 나타냅니다. 공간화는 BaseAudioContext의 AudioListener (listener 어트리뷰트) 에 관련이 있습니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 모노 (1 채널) 이거나 스테레오 (2 채널) 이고 증가될 수 없습니다. 더 적거나 많은 채널을 가진 노드로부터의 연결은 적절하게 다운믹스되거나 업믹스될 것입니다.</p>
        <p>만약 노드가 활발히 처리 중이라면, 이 노드의 출력은 스테레오 (2 채널) 로 hard-coded되고 설정될 수 없습니다. 만약 이 노드가 활발히 처리 중이 아니라면, 출력은 한 채널의 무음입니다.</p>
        <p>PanningModelType 열거형은 어떤 공간화 알고리즘이 사용되어 3D 공간에서의 오디오를 위치시킬지 결정합니다. 기본값은 "equalpower" 입니다.</p>
        <br>
        <h2>1.28. PeriodicWave 인터페이스</h2>
        <p>PeriodicWave는 OscillatorNode와 함께 쓰일 임의의 주기 파형을 나타냅니다.</p>
        <p>(명세를?) 따르는 구현은 반드시 적어도 8192 요소까지의 PeriodicWave를 지원해야만 합니다.</p>
        <pre>
[Exposed=Window]
interface PeriodicWave {
  constructor (BaseAudioContext context, optional PeriodicWaveOptions options = {});
};
        </pre>
        <br>
        <h2>1.29. ScriptProcessorNode 인터페이스 - 폐기됨</h2>
        <p>이 인터페이스는 스크립트를 사용해 오디오를 직접적으로 생성하거나, 처리하거나, 분석할 수 있는 AudioNode입니다. 이 노드 유형은 폐기되었고, AudioWorkletNode에 의해 대체됩니다. 이 글은 오직 정보를 주는 목적으로 남아있으며 구현이 이 노드 유형을 제거할 때까지 여기 있을 것입니다.</p>
        <p>표는 추후 번역</p>
        <p>ScriptProcessorNode는 반드시 다음의 값 중 하나인 bufferSize와 함께 생성됩니다: 256, 512, 1024, 2048, 4096, 8192, 16384. 이 값은 얼마나 자주 onaudioprocess 이벤트가 전파되고 얼마나 많은 샘플 프레임이 각 호출당 처리될 필요가 있는지를 제어합니다. onaudiorprocess 이벤트는 오직 ScriptProcessorNode가 적어도 한 개의 입력 혹은 한 개의 연결된 출력을 가지고 있을 때만 전파됩니다. bufferSize가 적은 숫자를 가지는 것은 낮은 (더 좋은) 레이턴시의 결과를 낳습니다. 높은 숫자는 오디오 breakup 혹은 glitch를 방지하기 위해 필수적일 것입니다. 이 값은 만약 createScriptProcessor()로의 bufferSize 인자가 전달되지 않았거나, 0으로 설정되었다면 구현에 의해 선택될 것입니다.</p>
        <p>numberOfInputChannels와 numberOfOutputChannels는 입력과 출력 채널의 수를 결정합니다. numberOfInputChannels와 numberOfOutputChannels 모두 0의 값을 가지는 것은 유효하지 않습니다.</p>
        <pre>
[Exposed=Window]
interface ScriptProcessorNode : AudioNode {
  attribute EventHandler onaudioprocess;
  readonly attribute long bufferSize;
};
        </pre>
        <br>
        <h2>1.30. StereoPannerNode 인터페이스</h2>
        <p>이 인터페이스는 low-cost 패닝 알고리즘을 사용하여 스테레오 이미지에서, 들어오는 오디오 스트림을 위치시키는 프로세싱 노드를 나타냅니다. 이 패닝 효과는 스테레오 스트림에서 오디오 컴포넌트를 위치시킴에 있어 일반적입니다.</p>
        <p>표는 추후 번역</p>
        <p>이 노드의 입력은 스테레오 (2 채널) 이고 증가될 수 없습니다. 더 적거나 더 많은 채널을 가진 노드로부터의 연결은 적절하게 업믹스되거나 다운믹스될 것입니다.</p>
        <p>이 노드의 출력은 스테레오 (2 채널) 로 hard-coded되고 설정될 수 없습니다.</p>
        <pre>
[Exposed=Window]
interface StereoPannerNode : AudioNode {
  constructor (BaseAudioContext context, optional StereoPannerOptions options = {});
  readonly attribute AudioParam pan;
};
        </pre>
        <br>
        <h2>1.31. WaveShaperNode 인터페이스</h2>
        <p>WaveShaperNode는 비선형 왜곡 효과를 구현하는 AudioNode 프로세서입니다.</p>
        <p>비선형 waveshaping 왜곡은 미묘한 비선형 warming 혹은 더욱 분명한 왜곡 효과 둘 다에 보통 사용됩니다. 임의 비선형 shaping curve는 명시될 수 있습니다.</p>
        <br>
        <h2>1.32. AudioWorklet 인터페이스</h2>
        <br>
        <pre>
[Exposed=Window, SecureContext]
interface AudioWorklet : Worklet {
};
        </pre>
        <h3>1.32.1. 개념</h3>
        <p>AudioWorklet 객체는 개발자로 하여금 (JavaScript나 WebAssembly 코드와 같은) 스크립트를 제공하여 렌더링 스레드에서 오디오를 처리할 수 있게 해 주며, 사용자 정의 AudioNode를 지원합니다. 이 프로세싱 메커니즘은 오디오 그래프의 다른 내장 AudioNode와 스크립트 코드의 동기적인 실행을 보장합니다.</p>
        <p>연관된 쌍의 객체는 반드시 정의되어 이 메커니즘을 실현시켜야만 합니다. 연관된 쌍의 객체란 AudioWorkletNode와 AudioWorkletProcessor입니다. AudioWorkletNode (전자) 는 다른 AudioNode 객체와 유사한 메인 글로벌 스코프에 대한 인터페이스를 나타내고 AudioWorkletProcessor (후자) 는 AudioWorkletGlobalScope라는 특별한 스코프 내에서의 내부적인 오디오 처리를 구현합니다.</p>
        <figure>
          <img src="img/audioworklet-concept.png" width="756" height="144">
          <figcaption><strong>도식 16</strong> AudioWorkletNode와 AudioWorkletProcessor</figcaption>
        </figure>
        <p>각 BaseAudioContext는 정확히 하나의 AudioWorklet을 소유합니다.</p>
        <p>AudioWorklet의 worklet 글로벌 스코프 자료형은 AudioWorkletGlobalScope입니다.</p>
        <p>AudioWorklet의 worklet 목적지 유형은 "audioworklet" 입니다.</p>
        <p>스크립트를 addModule(moduleUrl) 메서드를 통해 스크립트를 import하는 것은 AudioWorkletGlobalScope 하에서 AudioWorkletProcessor의 클래스 정의를 등록합니다. imported된 클래스 생성자와 이 생성자로부터 실제 생성된 인스턴스에 대한 두 개의 내부 저장 영역이 있습니다.</p>
        <br>
        <h1>2. 프로세싱 모델</h1>
        <h2>2.1. 배경</h2>
        <p><i>이 섹션은 비규범적입니다.</i></p>
        <p>낮은 레이턴시를 요구하는 실시간 오디오 시스템은 종종 콜백 함수를 사용하여 구현되는데, 여기서 운영 체제는 많은 오디오가 방해받지 않은 채로 재생되기 위해 계산되어야만 할 때 프로그램을 콜백합니다. 그런 콜백은 이상적으로 높은 우선도의 스레드에서 호출됩니다 (종종 시스템의 가장 높은 우선도). 이는 오직 오디오만을 다루는 프로그램은 코드를 이 콜백으로부터 실행해야 함을 의미합니다. 스레드 경계선은 가로지르거나 렌더링 스레드와 콜백 사이에 얼마간의 버퍼링을 추가하는 것은 자연히 레이턴시를 추가할 것이거나 시스템을 글리치에 덜 탄력성 있게 만들 것입니다.</p>
        <p>이러한 이유에서, Web Platform에서의 전통적인 비동기 작업 실행법인 이벤트 루프는 여기서는 작동하지 않습니다. 왜냐하면 이 스레드는 계속해서 실행 중이지 않기 때문입니다. 또한, 많은 불필요하고 잠재적으로 가로막는(blocking) 작업이 전통적인 수행 컨텍스트 (Windows 그리고 Workers)로부터 가능한데, 이는 수용할 만한 퍼포먼스 수준에 도달하기에 바람직한 것이 아닙니다.</p>
        <p>추가적으로, Worker 모델은 스크립트 실행 컨텍스트에 필수적인 작업용 스레드를 생성하는 것을 만드는 반면, 모든 AudioNode는 보통 같은 실행 컨텍스트를 공유합니다.</p>
        <div class="note">
          <p>알림: 이 섹션은 end result가 어떻게 보여야 하는지를 명시하지, 어떻게 이것이 구현되어야 하는지를 명시하는 것이 아닙니다. 특히, 메시지 큐를 사용하는 대신, 구현자는 메모리 연산이 reordered되지 않는 이상 스레드 간에서 공유되는 메모리를 사용할 수 있습니다.</p>
        </div>
        <br>
        <h2>2.2. 컨트롤 스레드와 렌더링 스레드</h2>
        <p>Web Audio API는 반드시 컨트롤 스레드와 렌더링 스레드를 사용하여 구현되어야만 합니다.</p>
        <p><strong>컨트롤 스레드</strong>란 AudioContext가 인스턴스화되고, author가 오디오 그래프를 조작하는 스레드입니다. 즉, BaseAudioContext에서의 작동이 호출되는 곳입니다. <strong>렌더링 스레드</strong>란 실제 오디오 출력이 컨트롤 스레드로부터의 호출에 응답하여 계산되는 스레드입니다. 만약 AudioContext에 대한 오디오 계산이면 이것은 실시간, 콜백 기반의 오디오 스레드일수도 있고, 만약 OfflineAudioContext에 대한 오디오 계산이면 일반 스레드입니다.</p>
        <p>컨트롤 스레드는 [HTML] 에서 기술되었듯이 전통적인 이벤트 루프를 사용합니다.</p>
        <p>렌더링 스레드는 전문화된 렌더링 루프를 사용하는데, 이는 2.4. 오디오 그래프 렌더링하기 섹션에서 기술됩니다.</p>
        <p>컨트롤 스레드로부터 렌더링 스레드로의 소통은 컨트롤 메시지 전달을 사용하여 이루어집니다. 다른 방향으로의 소통은 일반 이벤트 루프 태스크를 사용하여 이루어집니다.</p>
        <p>각 AudioContext는 컨트롤 메시지의 리스트인 하나의 <strong>컨트롤 메시지 큐</strong>를 가지고 있습니다. <strong>컨트롤 메시지</strong>는 렌더링 스레드에서 실행되는 동작입니다.</p>
        <p><strong>컨트롤 메시지를 queuing한다는 것</strong>은 BaseAudioContext의 컨트롤 메시지 큐의 끝에 메시지를 추가한다는 것을 의미합니다.</p>
        <div class="note">
          <p>알림: 예를 들어, AudioBufferSourceNode에서 성공적으로 start()를 호출하는 것은 연관된 BaseAudioContext의 컨트롤 메시지 큐에 컨트롤 메시지를 추가합니다.</p>
        </div>
        <br>
        <p>컨트롤 메시지 큐의 컨트롤 메시지는 삽입 시간에 의해 정렬됩니다. <strong>가장 오래된 메시지</strong>는 따라서 컨트롤 메시지의 큐의 맨 앞에 있는 메시지입니다.</p>
        <p><strong>컨트롤 메시지 큐 Qa를 또 다른 컨트롤 메시지 큐 Qb와 swapping한다는 것</strong>은 다음의 단계를 실행한다는 것을 의미합니다:</p>
        <ol>
          <li>Qc를 새롭고 빈 컨트롤 메시지 큐라고 하자.</li>
          <li>모든 컨트롤 메시지 Qa를 Qc로 옮긴다.</li>
          <li>모든 컨트롤 메시지 Qb를 Qa로 옮긴다.</li>
          <li>모든 컨트롤 메시지 Qc를 Qb로 옮긴다.</li>
        </ol>
        <br>
        <h2>2.3. 비동기적 작동</h2>
        <p>AudioNode에서 메서드를 호출하는 것은 효과적으로 비동기적이며, 반드시 두 과정을 거쳐 이루어져야 합니다. 그것은 동기적인 부분과 비동기적인 부분입니다. 각 메서드에 대해, 실행의 몇 부분은 컨트롤 스레드에서 발생하고 (예를 들어, 유효하지 않는 매개변수의 경우 예외를 던진다든지), 몇 부분은 렌더링 스레드에서 발생합니다 (예를 들자면, AudioParam의 값을 변경하는 것).</p>
        <p>AudioNode와 BaseAudioContext의 각 동작의 설명에서, 동기적 부분은 ⌛로 마킹되어 있습니다. 모든 다른 동작은 병렬적으로 수행되는데, 이는 [HTML]에 기술되어 있습니다.</p>
        <p>동기적 부분은 컨트롤 스레드에서 수행되고, 즉시 발생합니다. 만약 이것이 실패하면, 메서드 수행은 중단되고, 아마 예외를 발생시킬 것입니다. 만약 이것이 성공하면, 렌더링 스레드에서 실행될 작동을 부호화하는 컨트롤 메시지가 이 렌더링 스레드의 컨트롤 메시지 큐에 enque될 것입니다.</p>
        <p>다른 이벤트들과 관련된 동기적 그리고 비동기적 부분 순서는 반드시 같아야만 합니다. 각각 동기적이고 비동기적인 섹션 A<sub>Sync</sub>와 A<sub>Async</sub> 그리고 B<sub>Sync</sub>와 B<sub>Async</sub>인 두 작동 A와 B가 주어졌을 때, 만약 A가 B보다 먼저 발생한다면, A<sub>Sync</sub>는 B<sub>Sync</sub>보다 먼저 발생하고, A<sub>Async</sub>가 B<sub>Async</sub>보다 먼저 발생합니다. 다른 말로 하자면, 동기적이고 비동기적인 부분은 reorder될 수 없습니다.</p>
        <br>
        <h2>2.4. 오디오 그래프 렌더링하기</h2>
        <p>오디오 그래프를 렌더링한다는 것은 128 샘플 프레임의 블럭 내에서 이루어집니다. 128 샘플 프레임의 블럭은 <strong>렌더 퀀텀</strong>(render quantum)이라고 불리며, <strong>렌더 퀀텀 크기</strong>(~ size) 는 128입니다.</p>
        <p>주어진 스레드에서 <strong>atomically</strong>하게 발생하는 작동은 다른 atomic한 작동이 또 다른 스레드에서 작동하고 있지 않을때에만 오직 수행 가능합니다.</p>
        <p>BaseAudioContext에서 오디오 블럭을 렌더링하는 알고리즘 G 그리고 컨트롤 메시지 큐 Q는 다수의 단계로 구성되며 추가적인 디테일은 그래프 렌더링의 알고리즘에서 설명됩니다.</p>
        <div class="note">
          <p>실제로는, AudioContext 렌더링 스레드는 종종 시스템 수준의 오디오 콜백을 run off하는데, 이는 등시적인(isochronous) 방식으로 수행됩니다.</p>
          <p>OfflineAudioContext는 시스템 수준의 오디오 콜백을 가질 것이 요구되지 않으나, 이전 콜백이 종료되자마자 발생하는 콜백과 함께 이것이 그러한 콜백을 가지고 있는 것 처럼 행동합니다.</p>
        </div>
        <br>
        <p>오디오 콜백은 또한 컨트롤 메시지 큐에서 태스크로서 queue됩니다. UA는 반드시 다음의 알고리즘을 수행하여 렌더 퀀텀들을 처리해 요구된 버퍼 사이즈를 채움으로써 그러한 태스크를 이행해야만 합니다. 컨트롤 메시지 큐와 함게, 각 AudioContext는 일반적인 태스크 큐를 가지고 있으며, 컨트롤 스레드로부터 렌더링 스레드에 post된 태스크들에 대해 이것의 <strong>연관된 태스크 큐</strong>를 호출합니다. 추가적인 마이크로 태스크 체크 포인트는 렌더 퀀텀을 처리한 후에 수행되어 AudioWorkletProcessor의 process 메서드의 실행 중에 queue되었을 지도 모르는 어떠한 마이크로태스크들을 실행합니다.</p>
        <p>AudioWorkletNode로부터 post된 모든 태스크들은 이것의 연관된 BaseAudioContext의 연관된 태스크 큐에 post됩니다.</p>
        <p>다음의 과정은 렌더링 루프가 시작되기 전에 반드시 한 번 수행되어야만 합니다.</p>
        <p>과정은 추후에 번역</p>
        <p>AudioNode를 <strong>mute</strong>한다는 것은 이것의 출력이 반드시 이 오디오 블럭의 렌더링에 대해 무음을 출력해야만 한다는 것을 의미합니다.</p>
        <p><strong>AudioNode로부터 읽는 것에 대해 버퍼를 사용 가능하게 만든다는 것</strong>은 이 AudioNode에 연결된 다른 AudioNode들이 이것으로부터 안전하게 읽을 수 있는 상태에 이것을 놓는다는 것을 의미합니다.</p>
        <div class="note">
          <p>참고: 예를 들어, 구현은 새로운 버퍼를 할당하거나, 더욱 정교한 메커니즘을 가지는 것을 선택할 수 있습니다. 그 메커니즘이란 지금은 사용되지 않는 존재하는 버퍼를 재사용하는 것입니다.</p>
        </div>
        <br>
        <p><strong>AudioNode의 입력을 record한다는 것</strong>은 이 AudioNode의 입력 데이터를 미래의 사용을 위해 복사한다는 것을 의미합니다.</p>
        <p><strong>오디오 블럭을 계산한다는 것</strong>은 이 AudioNode에 대해 알고리즘을 실행하여 128 샘플 프레임을 생성하는 것을 의미합니다.</p>
        <p><strong>입력 버퍼를 처리한다는 것</strong>은 AudioNode에 대해 알고리즘을 실행하는 것을 의미하는데, 입력 버퍼와 이 AudioNode의 AudioParam(들)의 값(들)을 이 알고리즘에 대한 입력으로서 사용합니다.</p>
        <br>
        <h2>2.5. 문서 unload하기</h2>
        <p>추가적인 document 언로딩 cleanup 단계가 BaseAudioContext를 사용하는 문서에 대해 정의됩니다:</p>
        <ul>
          <li>관련 있는 global object가 document의 연관된 Window와 같은 각 AudioContext와 OfflineAudioContext에 대해 InvalidStateError와 함께 [[pending promises]]의 모든 프로미스를 reject시킵니다.</li>
          <li>모든 decoding 스레드를 정지시킵니다.</li>
          <li>AudioContext 또는 OfflineAudioContext를 close()하기 위한 컨트롤 메시지를 queue시킵니다.</li>
        </ul>
        <br>
        <h1>3. 동적 생명 주기</h1>
        <h2>3.1. 배경</h2>
        <div class="note">
          <p>알림: AudioContext와 AudioNode 생명 주기 특성에 대한 규범적인 설명은 AudioContext 생명주기와 AudioNode 생명주기에서 설명되고 있습니다.</p>
        </div>
        <br>
        <p><i>이 섹션은 비규범적입니다.</i></p>
        <p>정적 라우팅 설정의 생성을 허용함에 추가적으로, 제한된 생명 주기를 가지는 동적으로 할당된 소리에 사용자 정의 효과 라우팅을 적용하는 것 또한 가능합니다. 이 논의의 목적을 위해, 이 짧게 사는 소리를 "노트" (note) 라고 부릅시다. 많은 오디오 어플리케이션들은 노트의 개념을 포함하며, 그 예로는 드럼 머신, 시퀸서, 게임 플레이에 따라 발동되는 많은 한 번의 (one-shot) 소리를 가진 3D 게임을 들 수 있습니다.</p>
        <p>전통적인 소프트웨어 신디사이저에서는, 노트는 동적으로 할당되고 사용 가능한 자원의 풀(pool)에서 해제되었습니다. 노트는 MIDI note-on 메시지가 받아졌을 때 할당됩니다. 노트가 샘플 데이터의 끝에 도달했거나 (만약 반복되지 않는다면), 엔벨로프의 서스테인 단계에 도달했는데 서스테인이 0이거나, 혹은 엔벨로프의 릴리즈 단계로 가게 하는 MIDI note-off 메시지에 기인해 노트가 재생을 마쳤을 때 해제됩니다. MIDI note-off 경우에서, 노트는 바로 해제되지 않고, 오직 릴리즈 엔벨로프 단계가 마쳤을 때에만 해제됩니다. 어떠한 주어진 시간에서라도, 다수의 재생되는 노트가 있을 수 있으나 노트의 집합은 계속 바뀝니다. 왜냐하면 새로운 노트들이 라우팅 그래프에 추가되고, 오래된 것들은 해제되기 때문입니다.</p>
        <p>오디오 시스템은 자동적으로 개개의 "노트" 이벤트에 대한 라우팅 그래프의 부분을 해체(tear down)하는 것을 다룹니다. "노트" 는 AudioBufferSourceNode에 의해 표현되는데, 이는 직접적으로 다른 프로세싱 노드에 연결될 수 있습니다. 노트가 재생을 마쳤을 때, 컨텍스트는 자동적으로 이 AudioBufferSourceNode에 대한 참조를 해제하는데, 이는 차례로 이것이 연결된 모든 노드들에 대한 참조를 해제합니다. 노드들은 자동적으로 그래프에서 연결 해제되고 더 이상 참조를 가지고 있지 않을 때 삭제될 것입니다. 동적인 소리들 사이에서 공유되고 오래 사는 그래프 상의 노드들은 명시적으로 관리될 수 있습니다. 비록 이것이 복잡하게 들리긴 하지만, 추가적인 요구되는 처리 없이 이 모든 것은 자동적으로 발생합니다.</p>
        <br>
        <h2>3.2. 예제</h2>
        <figure>
          <img src="img/dynamic-allocation.png" width="671" height="221">
          <figcaption><strong>도식 18</strong> 이르게 해제(release)될 서브그래프를 나타내는 그래프</figcaption>
        </figure>
        <p>위의 로우패스필터, 패너, 두번째 게인 노드는 직접적으로 한 번의(one-shot) 소리로부터 연결되어 있습니다. 따라서 이 소리가 재생을 마쳤을 때 컨텍스트는 자동적으로 이것들을 해제할 것입니다 (점선 내에 있는 모든 것). 만약 이 소리와 연결된 노드들에 대한 어떠한 참조도 더 이상 없다면, 이것들은 즉시 그래프로부터 해제되고 삭제됩니다. 스트리밍 소스는 전역 참조를 가지고 있고 이것이 명시적으로 연결 해제되기 전까지는 연결이 유지될 것입니다. 여기 이것이 JavaScript에서 어떻게 보일지에 대한 코드입니다.</p>
        <pre>예제 20
let context = 0;
let compressor = 0;
let gainNode1 = 0;
let streamingAudioSource = 0;
// 라우팅 그래프의 "길게 사는" 부분에 대한 초기 설정
function setupAudioContext() {
    context = new AudioContext();
    compressor = context.createDynamicsCompressor();
    gainNode1 = context.createGain();
    // 스트리밍 오디오 소스를 생성합니다.
    const audioElement = document.getElementById('audioTagID');
    streamingAudioSource = context.createMediaElementSource(audioElement);
    streamingAudioSource.connect(gainNode1);
    gainNode1.connect(compressor);
    compressor.connect(context.destination);
}
// 나중에 유저 액션에 응답하여 (보통 마우스나 키 이벤트)
// 한 번의 소리가 재생될 수 있습니다.
function playSound() {
    const oneShotSound = context.createBufferSource();
    oneShotSound.buffer = dogBarkingBuffer;
    // 필터, 패너, 게인 노드를 생성합니다.
    const lowpass = context.createBiquadFilter();
    const panner = context.createPanner();
    const gainNode2 = context.createGain();
    // 연결을 만듭니다
    oneShotSound.connect(lowpass);
    lowpass.connect(panner);
    panner.connect(gainNode2);
    gainNode2.connect(compressor);
    // 지금으로부터 0.75초 후에 재생합니다 (즉시 재생하려면 0을 전달하십시오)
    oneShotSound.start(context.currentTime + 0.75);
}
        </pre>
        <br>
        <h1>4. 채널 업믹싱과 다운믹싱</h1>
        <p><i>이 섹션은 규범적입니다.</i></p>
        <p>AudioNode 입력은 자신에게 연결된 모든 입력들로부터 채널들을 결합하는 믹싱 규칙을 가지고 있습니다. 단순한 예제로, 만약 입력이 모노 출력과 스테레오 출력으로부터 연결되어 있다면 보통 모노 입력이 스테레오로 업믹스되고 스테레오 연결로 합쳐질 것입니다. 하지만, 물론, 모든 입력에 대해 모든 AudioNode에 정확한 믹싱 규칙을 정의하는 것은 중요합니다. 모든 입력에 대한 기본 믹싱 규칙은 선택되어 있어 일들은 "그냥 작동" 합니다. 세부 사항에 대해 너무 걱정할 필요 없이 말이죠. 특히 모노와 스테레오 스트림의 아주 일반적인 경우에서 말입니다. 물론, 특히 멀티 채널처럼, 규칙은 고급의 사용 경우에 대해 변경될 수 있습니다.</p>
        <br>
        <p>몇가지 용어를 정의하자면, 업믹싱이란 더 적은 채널의 수를 가진 스트림을 취하고 그것을 더 큰 수의 채널을 가진 스트림으로 변환하는 과정을 의미합니다. 다운믹싱이란 더 큰 채널의 수를 가진 스트림을 취하고 그것을 더 적은 수의 채널을 가진 스트림으로 변환하는 과정을 의미합니다.</p>
        <br>
        <p>AudioNode 입력은 이 입력에 연결된 모든 출력을 믹스할 필요가 있습니다. 이 과정의 일부로 이것은 내부 값인 computedNumberOfChannels를 계산하는데 이는 주어진 시간에서의 입력 채널의 실제 수를 나타냅니다.</p>
        <p>AudioNode의 각 입력에 대해, 구현은 반드시 아래를 수행해야 합니다:</p>
        <ol>
          <li>computedNumberOfChannels를 계산합니다.</li>
          <li>입력의 각 연결에 대해서:
            <ol>
              <li>노드의 channelInterpretation 어트리뷰트에 의해 주어진 ChannelInterpretation 값에 따른 computedNumberOfChannels에의 연결을 업믹스 또는 다운믹스합니다.</li>
              <li>(다른 연결로부터의) 모든 다른 믹스된 스트림과 함께 모두 믹스합니다. 이것은 각 연결에 대해 윗 단계에서 업믹스나 다운믹스된 해당하는 각 채널을 함께 간단하게 합치는 것입니다.</li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>4.1. 스피커 채널 레이아웃</h2>
        <p>channelInterpretation이 "speakers" 라면, 특정한 채널 레이아웃에 대해 업믹싱과 다운믹싱이 정의됩니다.</p>
        <p>모노 (1채널), 스테레오 (2채널), 쿼드 (4채널), 5.1 (6채널) 이 반드시 지원되어야만 합니다. 다른 채널 레이아웃은 이 명세의 미래 버전에서 지원될 수도 있습니다.</p>
        <br>
        <h2>4.2. 채널 순서</h2>
        <p>채널 순서는 다음 표에 의해 정의된다. 개개의 다채널 포맷은 모든 중간 채널들을 지원하지 않을 수 있다. 구현은 반드시 아래에 정의된 순서로 제공된 채널들을 나타내야만 하며, 나타나지 않은 채널은 생략한다.</p>
        <table>
          <thead>
            <tr>
              <td>순서</td>
              <td>라벨</td>
              <td>모노</td>
              <td>스테레오</td>
              <td>쿼드</td>
              <td>5.1</td>
            </tr>
            <tr>
              <td>0</td>
              <td>SPEAKER_FRONT_LEFT</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>SPEAKER_FRONT_RIGHT</td>
              <td></td>
              <td>1</td>
              <td>1</td>
              <td>1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>SPEAKER_FRONT_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td>2</td>
            </tr>
            <tr>
              <td>3</td>
              <td>SPEAKER_LOW_FREQUENCY</td>
              <td></td>
              <td></td>
              <td></td>
              <td>3</td>
            </tr>
            <tr>
              <td>4</td>
              <td>SPEAKER_BACK_LEFT</td>
              <td></td>
              <td></td>
              <td>2</td>
              <td>4</td>
            </tr>
            <tr>
              <td>5</td>
              <td>SPEAKER_BACK_RIGHT</td>
              <td></td>
              <td></td>
              <td>3</td>
              <td>5</td>
            </tr>
            <tr>
              <td>6</td>
              <td>SPEAKER_FRONT_LEFT_OF_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>7</td>
              <td>SPEAKER_FRONT_RIGHT_OF_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>8</td>
              <td>SPEAKER_BACK_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>9</td>
              <td>SPEAKER_SIDE_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>10</td>
              <td>SPEAKER_SIDE_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>11</td>
              <td>SPEAKER_TOP_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>12</td>
              <td>SPEAKER_TOP_FRONT_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>13</td>
              <td>SPEAKER_TOP_FRONT_CENTER	</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>14</td>
              <td>SPEAKER_TOP_FRONT_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>15</td>
              <td>SPEAKER_TOP_BACK_LEFT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>16</td>
              <td>SPEAKER_TOP_BACK_CENTER</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
            <tr>
              <td>17</td>
              <td>SPEAKER_TOP_BACK_RIGHT</td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
        </table>
        <br>
        <h2>4.3. 입력과 출력 채널 카운트의 꼬리 시간의 구현</h2>
        <p>AudioNode가 0이 아닌 꼬리 시간을 가지고, 입력 채널 카운트에 좌우되는 출력 채널 카운트를 가졌을 때, 입력 채널 카운트가 변화할 때 AudioNode의 꼬리 시간은 반드시 고려되어져야만 합니다.</p>
        <p>입력 채널 카운트에 감소가 있을 때, 더 큰 채널 카운트와 함께 받아진 입력이 더 이상 출력에 영향을 주지 않을 때 출력 채널 카운트에서의 변화는 반드시 발생해야만 합니다.</p>
        <p>입력 채널 카운트에서의 증가가 있을 때, 동작은 AudioNode 유형에 따라 다릅니다:</p>
        <ul>
          <li>DelayNode나 DynamicsCompressorNode에 대해서, 출력 채널의 수는 더 큰 채널 카운트와 함께 받아진 입력이 출력에 영향을 주기 시작했을 때 반드시 증가해야만 합니다.</li>
          <li>꼬리 시간을 가진 다른 AudioNode에 대해서, 출력 채널의 수는 반드시 즉시 증가해야만 합니다.
            <div class="note">
              <p>참고: ConvolverNode에 대해서, 이것은 오직 임펄스 응답이 모노인 경우에만 적용됩니다. 그렇지 않으면, ConvolverNode는 자신의 입력 채널 카운트에 관계없이 항상 스테레오 신호를 출력합니다.</p>
            </div>
          </li>
        </ul>
        <br>
        <div class="note">
          <p>직감적으로, 이것은 프로세싱의 부분으로서 스테레오 정보를 잃지 않게 해 줍니다: 다른 채널 카운트의 다수의 입력 render quantum들이 출력 render quantum에 공헌할 때 출력 render quantum의 채널 카운트는 입력 render quantum들의 입력 채널 카운트의 상위 집합입니다.</p>
        </div>
        <br>
        <h2>4.4. 업믹싱 스피커 레이아웃</h2>
        <pre>

모노 업믹스:

  1 -> 2 : 모노에서 스테레오로 업믹스
    output.L = input;
    output.R = input;

  1 -> 4 : 모노에서 쿼드로 업믹스
  output.L = input;
  output.R = input;
  output.SL = 0;
  output.SR = 0;

  1 -> 5.1 : 모노에서 5.1로 업믹스
    output.L = 0;
    output.R = 0;
    output.C = input; // put in center channel
    output.LFE = 0;
    output.SL = 0;
    output.SR = 0;

스테레오 업믹스:

  2 -> 4 : 스테레오에서 쿼드로 업믹스
    output.L = input.L;
    output.R = input.R;
    output.SL = 0;
    output.SR = 0;

  2 -> 5.1 : 스테레오에서 5.1로 업믹스
    output.L = input.L;
    output.R = input.R;
    output.C = 0;
    output.LFE = 0;
    output.SL = 0;
    output.SR = 0;

쿼드 업믹스:

4 -> 5.1 : 쿼드에서 5.1로 업믹스
  output.L = input.L;
  output.R = input.R;
  output.C = 0;
  output.LFE = 0;
  output.SL = input.SL;
  output.SR = input.SR;
        </pre>
        <br>
        <h2>4.5. 다운믹싱 스피커 레이아웃</h2>
        <p>예를 들어, 5.1 소스 material을 프로세싱했지만, 스테레오로 재생할 때 다운믹스가 필요할 것입니다.</p>
        <pre>

모노 다운믹스:

  2 -> 1 : 스테레오에서 모노로
    output = 0.5 * (input.L + input.R);

  4 -> 1 : 쿼드에서 모노로
    output = 0.25 * (input.L + input.R + input.SL + input.SR);

  5.1 -> 1 : 5.1에서 모노로
    output = sqrt(0.5) * (input.L + input.R) + input.C + 0.5 * (input.SL + input.SR)

스테레오 다운믹스:

  4 -> 2 : 쿼드에서 스테레오로
    output.L = 0.5 * (input.L + input.SL);
    output.R = 0.5 * (input.R + input.SR);

  5.1 -> 2 : 5.1에서 스테레오로
    output.L = L + sqrt(0.5) * (input.C + input.SL)
    output.R = R + sqrt(0.5) * (input.C + input.SR)

쿼드 다운믹스:

  5.1 -> 4 : 5.1에서 쿼드로
    output.L = L + sqrt(0.5) * input.C
    output.R = R + sqrt(0.5) * input.C
    output.SL = input.SL
    output.SR = input.SR
        </pre>
        <br>
        <h2>4.6. 채널 규칙 예제</h2>
        <pre>

// gain 노드를 explicit 2채널 (스테레오) 로 설정합니다.
gain.channelCount = 2;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";
// 2개의 스테레오 출력 버스를 가진 DJ 앱을 위해 "하드웨어 출력"을 4채널로 설정합니다.
context.destination.channelCount = 4;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";
// 사용자 정의 다채널 스피커 배열을 위해 "하드웨어 출력"을 8채널로 설정합니다,
// 사용자 정의 매트릭스 믹싱과 함께.
context.destination.channelCount = 8;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "discrete";
// HTMLAudioElement를 재생하기 위해 "하드웨어 출력"을 5.1로 설정합니다.
context.destination.channelCount = 6;
context.destination.channelCountMode = "explicit";
context.destination.channelInterpretation = "speakers";
// 명시적으로 모노로 다운믹스합니다.
gain.channelCount = 1;
gain.channelCountMode = "explicit";
gain.channelInterpretation = "speakers";
        </pre>
        <br>
        <h1>5. 오디오 신호 값</h1>
        <h2>5.1. 오디오 샘플 포맷</h2>
        <p><strong>선형 펄스 부호 변조</strong>(linear pulse code modulation; linear PCM) 은 하나의 포맷인데, 이 포맷에서는 오디오 값들이 정기적인 구간에서 샘플링되며, 두 개의 연이은 값 사이의 양자화 레벨이 선형적으로 균일합니다.</p>
        <p>이 명세에서 신호 값들이 스크립트에 노출될 때는 언제든지, 그 값들은 선형 32비트 부동 소수점 펄스 부호 변조 포맷 (선형 32비트 부동 소수점 PCM) 이며, 종종 Float32Array 객체의 형태로 존재합니다.</p>
        <br>
        <h2>5.2. 렌더링</h2>
        <p>무슨 오디오 그래프에서든지 목적지 노드에서의 모든 오디오 신호의 범위는 명목상(공칭상) [1, 1]입니다. 이 범위 바깥, NaN값, 양의 무한, 음의 무한의 신호 값의 오디오 렌더링은 이 명세에 의해 정의되지 않습니다.</p>
        <br>
        <h1>6. 공간화/패닝</h1>
        <h2>6.1. 배경</h2>
        <p>현대 3D 게임에서의 일반적인 기능 요구는 3D 공간에서의 다수의 오디오 소스의 동적 공간화와 이동입니다. 예를 들자면 OpenAL이 이러한 기능을 가지고 있습니다.</p>
        <p>PannerNode를 사용하여, 오디오 스트림은 AudioListener에 관해 공간화되거나 공간에서 위치가 지정될 수 있습니다. BaseAudioContext는 하나의 AudioListener를 가지고 있습니다. 패너와 리스너 둘 다 오른손 카르테시안 좌표계를 사용하여 3D 공간에서의 위치를 가집니다. 좌표계에서 사용되는 단위는 정의되지 않았고, 그럴 필요도 없는 것이 이 좌표들과 함께 계산되는 효과는 미터나 피트와 같은 특정한 단위에 독립적이거나 변함이 없기 때문입니다. PannerNode 객체 (소스 스트림을 나타냄) 는 소리가 어떤 방향을 비추고 있는지를 나타내는 방향 벡터를 가지고 있습니다. 추가적으로, PannerNode는 소리가 얼마나 방향성인지를 나타내는 sound cone을 가지고 있습니다. 예를 들어, 소리가 전방향이어서 방향에 관계없이 어디서든 들릴 수 있고, 또는 소리가 지향성이라 소리가 리스너를 바라볼 때에만 들릴 수 있습니다. AudioListener 객체 (사람의 귀를 나타냄) 은 앞과 위 벡터를 가지고 있고 이 벡터들은 사람이 바라보고 있는 방향을 나타냅니다.</p>
        <p>아래의 도식은 공간화에서의 좌표계와 기본값을 보여줍니다. AudioListener와 PannerNode의 위치는 우리가 상황을 더 잘 파악할 수 있도록 기본 위치에서 이동되어 있습니다.</p>
        <figure>
          <img src="img/panner-coord.svg">
          <figcaption><strong>도식 19</strong> AudioListener와 PannerNode 어트리뷰트가 보이는 좌표계를 나타내는 도식</figcaption>
        </figure>
        <p>렌더링 중에, PannerNode는 방위각(azimuth)과 고도(elevation)를 계산합니다. 이 값들은 구현에 의해 나부적으로 사용되어 공간화 효과를 렌더링합니다. 이 값들이 어떻게 쓰이는지에 대한 자세한 사항에 대해서는 Panning Algorithm 섹션을 참고하세요.</p>
        <br>
        <h2>6.2. 방위각과 고도</h2>
        <p>다음의 알고리즘은 PannerNode에서 방위각과 고도를 계산하기 위해 반드시 사용되어야만 합니다. 구현은 반드시 적절하게 아래의 다양한 AudioParam들이 "a-rate" 인지 "k-rate" 인지 고려해야만 합니다.</p>
        <pre>

// |context|를 BaseAudioContext, |panner|를
// |context|내에서 생성된 PannerNode라고 합시다
// 소스-리스너 벡터를 계산합니다.
const listener = context.listener;
const sourcePosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                panner.positionZ.value);
const listenerPosition =
    new Vec3(listener.positionX.value, listener.positionY.value,
              listener.positionZ.value);
const sourceListener = sourcePosition.diff(listenerPosition).normalize();
if (sourceListener.magnitude == 0) {
  // 소스와 리스너가 같은 지점에 있는 degenerate 경우를 다룹니다.
  azimuth = 0;
  elevation = 0;
  return;
}
// 축들을 정렬합니다.
const listenerForward = new Vec3(listener.forwardX.value, listener.forwardY.value,
                                  listener.forwardZ.value);
const listenerUp =
    new Vec3(listener.upX.value, listener.upY.value, listener.upZ.value);
const listenerRight = listenerForward.cross(listenerUp);
if (listenerRight.magnitude == 0) {
  // 리스너의 '위' 와 '앞' 벡터가 선형적으로 독립적이어서,
  // '오른쪽' 이 결정될 수 없는 경우를 다룹니다
  azimuth = 0;
  elevation = 0;
  return;
}
// 리스너의 오른쪽, 앞에 직교하는 단위벡터를 결정합니다
const listenerRightNorm = listenerRight.normalize();
const listenerForwardNorm = listenerForward.normalize();
const up = listenerRightNorm.cross(listenerForwardNorm);
const upProjection = sourceListener.dot(up);
const projectedSource = sourceListener.diff(up.scale(upProjection)).normalize();
azimuth = 180 * Math.acos(projectedSource.dot(listenerRightNorm)) / Math.PI;
// 리스너의 앞 또는 뒤의 소스.
const frontBack = projectedSource.dot(listenerForwardNorm);
if (frontBack &lt; 0)
  azimuth = 360 - azimuth;
// 방위각을 "오른쪽" 리스너 벡터가 아니라 "앞" 벡터에 관계되게 만듭니다.
if ((azimuth >= 0) && (azimuth &lt;= 270))
  azimuth = 90 - azimuth;
else
  azimuth = 450 - azimuth;
elevation = 90 - 180 * Math.acos(sourceListener.dot(up)) / Math.PI;
if (elevation > 90)
  elevation = 180 - elevation;
else if (elevation &lt; -90)
  elevation = -180 - elevation;
        </pre>
        <br>
        <h2>6.3. 패닝 알고리즘</h2>
        <p>모노→스테레오 그리고 스테레오→스테레오 패닝이 반드시 지원되어야만 합니다. 모노→스테레오 프로세싱은 입력에의 모든 연결이 모노일 때 사용됩니다. 그렇지 않으면 스테레오→스테레오 프로세싱이 사용됩니다.</p>
        <h3>6.3.1. PannerNode "equalpower" 패닝</h3>
        <p>이것은 기본적이나, 합리적인 결과를 제공하는 단순하고 비교적 저렴한 알고리즘입니다. 이것은 panningModel 어트리뷰트가 "equalpower" 로 설정되어 있어, 고도 값이 무시되는 상황에 PannerNode가 사용합니다. 이 알고리즘은 automationRate에 의해 명시된 것과 같은 적절한 비율을 사용해 구현되어야만 합니다. 만약 PannerNode의 AudioParam이나 AudioListener의 AudioParam 중 하나라도 "a-rate"라면, a-rate 프로세싱이 반드시 사용되어야만 합니다.</p>
        <ol>
          <li>이 AudioNode에 의해 계산될 각 샘플에 대해:
            <ol>
              <li>azimuth가 방위각과 고도 섹션에서 계산된 값이라고 합시다.</li>
              <li>방위각 값은 우선 다음에 따라 범위 [-90, 90] 내에 포함됩니다:
                <pre>
// 우선, azimuth를 [-180, 180]의 허용된 범위 내로 조정합니다.
azimuth = max(-180, azimuth);
azimuth = min(180, azimuth);

// 그리고 나서 범위 [-90, 90] 으로 조정합니다.
if (azimuth < -90)
  azimuth = -180 - azimuth;
else if (azimuth > 90)
  azimuth = 180 - azimuth;
                </pre>
              </li>
              <li>정규화된 값 x는 모노 입력에 대해 azimuth로부터 다음과 같이 계산됩니다:
                <pre>
x = (azimuth + 90) / 180;
                </pre>
                또는 스테레오 입력에 대해서 다음과 같이 계산됩니다:
                <pre>
if (azimuth &lt;= 0) { // -90 -> 0
  // [-90, 0] 각도 내의 azimuth 값을 [-90, 90] 범위 내로 변환합니다.
  x = (azimuth + 90) / 90;
} else { // 0 -> 90
  // [0, 90] 각도 내의 azimuth 값을 [-90, 90] 범위 내로 변환합니다.
  x = azimuth / 90;
}
                </pre>
              </li>
              <li>좌측과 우측 gain 값은 다음과 같이 계산됩니다:
                <pre>
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
                </pre>
              </li>
              <li>모노 입력에 대해, 스테레오 출력은 다음과 같이 계산됩니다:
                <pre>
outputL = input * gainL;
outputR = input * gainR;
                </pre>
                또는 스테레오 입력에 대해, 출력은 다음과 같이 계산됩니다:
                <pre>
if (azimuth <= 0) {
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
                </pre>
              </li>
              <li>거리의 계산이 6.4. 거리 효과에서 기술되었고 cone gain이 6.5. 소리 cone에서 기술된 거리 gain과 cone gain을 적용합니다.
                <pre>
let distance = distance();
let distanceGain = distanceModel(distance);
let totalGain = coneGain() * distanceGain();
outputL = totalGain * outputL;
outputR = totalGain * outputR;
                </pre>
              </li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>6.3.2. PannerNode "HRTF" 패닝 (스테레오만 해당)</h2>
        <p>이것은 다양한 방위각과 고도에서 녹음된 HRTF (Head-related Transfer Function) 임펄스 응답의 집합을 요구합니다. 구현은 대단히 최적화된 컨볼루션 함수를 요구합니다. 이것은 "equalpower" 보다 다소 비싸지만, 인지적으로 더욱 공간화된 소리를 제공합니다.</p>
        <figure>
          <image src="img/HRTF_panner.png" width="644" height="419"></image>
          <figcaption><strong>도식 20</strong> HRTF를 사용하여 소스를 패닝하는 과정을 보여주는 도식</figcaption>
        </figure>
        <br>
        <h2>6.3.3. StereoPannerNode 패닝</h2>
        <p>StereoPannerNode에 대해서, 다음의 알고리즘이 반드시 구현되어야만 합니다.</p>
        <ol>
          <li>이 AudioNode에 의해 계산될 각 샘플에 대해서
            <ol>
              <li>이 StereoPannerNode의 pan AudioParam의 computedValue를 pan이라고 합시다.</li>
              <li>pan을 [-1, 1] 로 조정합니다.
                <pre>
pan = max(-1, pan);
pan = min(1, pan);
                </pre>
              </li>
              <li>pan 값을 [0, 1] 로 정규화함으로써 x를 계산합니다. 모노 입력에 대해:
                <pre>
x = (pan + 1) / 2;
                </pre>
                스테레오 입력에 대해:
                <pre>
if (pan &lt;= 0)
  x = pan + 1;
else
  x = pan;
                </pre>
              </li>
              <li>좌측과 우측 gain 값은 다음과 같이 계산됩니다:
                <pre>
gainL = cos(x * Math.PI / 2);
gainR = sin(x * Math.PI / 2);
                </pre>
              </li>
              <li>모노 입력에 대해, 스테레오 출력은 다음과 같이 계산됩니다:
                <pre>
outputL = input * gainL;
outputR = input * gainR;
                </pre>
                혹은 스테레오 입력에 대해, 출력은 다음과 같이 계산됩니다.
                <pre>
if (pan &lt;= 0) {
  outputL = inputL + inputR * gainL;
  outputR = inputR * gainR;
} else {
  outputL = inputL * gainL;
  outputR = inputR + inputL * gainR;
}
                </pre>
              </li>
            </ol>
          </li>
        </ol>
        <br>
        <h2>6.4. 거리 효과</h2>
        <p>가까운 소리는 더 큰 반면, 멀리 떨어진 소리는 조용합니다. 소리의 음량이 리스너로부터의 거리에 따라 바뀌는 방법은 정확히 distanceModel 어트리뷰트에 달려 있습니다.</p>
        <p>오디오 렌더링 동안, 거리 값은 다음에 따른 패너와 리스너 위치에 기반해 계산될 것입니다.</p>
        <pre>
function distance(panner) {
  const pannerPosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                  panner.positionZ.value);
  const listener = context.listener;
  const listenerPosition =
      new Vec3(listener.positionX.value, listener.positionY.value,
               listener.positionZ.value);
  return pannerPosition.diff(listenerPosition).magnitude;
}
        </pre>
        <p>거리는 그리고 나서 distanceGain을 계산하기 위해 사용되는데 distanceGain은 distanceModel 어트리뷰트에 좌우됩니다. 각 거리 모델이 계산되는 방법에 대한 자세한 자상은 DistanceModelType 섹션을 참고하세요.</p>
        <p>이것의 프로세싱의 일부로서, PannerNode는 입력 오디오 신호를 distanceGain만큼 크기를 조정/곱해서 멀리 있는 소리를 조용하게 하고 가까운 소리를 크게 만듭니다.</p>
        <br>
        <h2>6.5. 소리 cone</h2>
        <p>리스너와 각 음원은 어떤 방향으로 자신이 향하고 있는지를 기술하는 방향 벡터를 가지고 있습니다. 각 음원의 소리 사영 특징은 음원의 방향 벡터로부터의 음원/리스너 각도의 함수로서 소리 강도를 기술하는 안 그리고 바깥 "cone"에 의해 기술됩니다. 따라서, 리스너를 직접적으로 가리키는 음원은 off-axis를 가리킬 때보다 더 큽니다. 음원은 또한 전방향일 수 있습니다.</p>
        <p>다음의 도식은 리스너에 대한 음원의 cone 사이의 관계를 나타냅니다. 이 도식에서, coneInnerAngle = 50이고 coneOuterAngle = 120입니다. 즉, 내부 cone은 방향 벡터의 각 측면에서 25도 확장됩니다. 유사하게, 외부 cone은 각 측면에서 60도 확장됩니다.</p>
        <figure>
          <image src="img/cone-diagram.svg" width="50%"></image>
          <figcaption><strong>도식 21</strong> 음원 방향과 리스너의 위치와 방향에 관한 음원의 cone 각도</figcaption>
        </figure>
        <p>음원 (PannerNode) 과 리스너가 주어졌을 때 cone 효과에 기인한 gain 기여를 계산하기 위해 다음의 알고리즘이 반드시 사용되어야만 합니다.</p>
        <pre>
function coneGain() {
  const sourceOrientation =
      new Vec3(source.orientationX, source.orientationY, source.orientationZ);
  if (sourceOrientation.magnitude == 0 ||
      ((source.coneInnerAngle == 360) && (source.coneOuterAngle == 360)))
    return 1; // cone이 명시되지 않음 - unity gain
  // 정규화된 음원-리스너 벡터
  const sourcePosition = new Vec3(panner.positionX.value, panner.positionY.value,
                                  panner.positionZ.value);
  const listenerPosition =
      new Vec3(listener.positionX.value, listener.positionY.value,
                listener.positionZ.value);
  const sourceToListener = sourcePosition.diff(listenerPosition).normalize();
  const normalizedSourceOrientation = sourceOrientation.normalize();
  // 음원 방향 벡터와 음원-리스너 벡터 사이의 각도
  const angle = 180 *
                Math.acos(sourceToListener.dot(normalizedSourceOrientation)) /
                Math.PI;
  const absAngle = Math.abs(angle);
  // API는 전체 각도이므로 (반각이 아님) 여기서 2로 나눔
  const absInnerAngle = Math.abs(source.coneInnerAngle) / 2;
  const absOuterAngle = Math.abs(source.coneOuterAngle) / 2;
  let gain = 1;
  if (absAngle &lt;= absInnerAngle) {
    // 감쇠 없음
    gain = 1;
  } else if (absAngle &gt;= absOuterAngle) {
    // 최대 감쇠
    gain = source.coneOuterGain;
  } else {
    // 내부와 외부 cone 사이
    // 내부 -&gt; 외부, x는 0에서 1로 감
    const x = (absAngle - absInnerAngle) / (absOuterAngle - absInnerAngle);
    gain = (1 - x) + source.coneOuterGain * x;
  }
  return gain;
}
        </pre>
        <br>
        <h1>7. 성능 고려</h1>
        <h2>7.1. 레이턴시</h2>
        <figure>
          <img src="img/latency.png" width="700" height="201">
          <figcaption><strong>도식 22</strong> 레이턴시가 중요할 수 있는 사용 경우</figcaption>
        </figure>
        <p>웹 어플리케이션에서, 마우스와 키보드 이벤트 (키다운, 마우스다운 등) 과 들리는 소리 사이의 시간 지연은 중요합니다.</p>
        <p>이 시간 지연을 레이턴시라고 부르며 몇 가지 요소들에 의해 유발되고 (입력 장치 레이턴시, 내부 버퍼링 레이턴시, DSP 프로세싱 레이턴시, 출력 장치 레이턴시, 스피커와 사용자의 귀 사이의 거리 등), 이것은 누적됩니다. 레이턴시가 클수록 유저의 경험은 덜 만족스러워질 것입니다. 극단적으로는, 레이턴시는 음악 생성이나 게임 플레이를 불가능하게 만들 수도 있습니다. 적당한 수준에서 레이턴시는 타이밍에 영향을 끼칠 수 있고 소리가 처지거나 게임이 반응적이지 않은 느낌을 줄 수 있습니다. 음악 어플리케이션에서 타이밍 문제는 리듬에 영향을 줍니다. 게임에서 타이밍 문제는 게임 플레이의 정확성에 영향을 줍니다. 상호작용을 하는 어플리케이션에서, 레이턴시는 아주 느린 애니메이션 프레임 레이트가 하는 것과 같은 방식으로 유저의 경험을 대단히 낮춥니다. 어플리케이션에 따라, 합리적인 레이턴시는 3-6 밀리초의 낮은 것부터 25-50 밀리초까지 될 수 있습니다.</p>
        <p>구현은 보통 전반적인 레이턴시를 최소화하는 것을 추구할 것입니다.</p>
        <p>전반적인 레이턴시를 최소화하는 것과 함께, 구현은 보통 AudioContext의 currentTime과 AudioProcessingEvent의 playbackTime 사이의 차이를 최소화하기를 추구할 것입니다. ScriptProcessorNode의 폐지는 시간이 흐름에 따라 이 고려를 덜 중요하게 만들 것입니다.</p>
        <p>추가적으로, 몇몇 AudioNode들은 오디오 그래프의 몇몇 경로에 레이턴시를 추가할 수 있습니다. 주목할만한 것으로는,</p>
        <ul>
          <li>AudioWorkletNode는 내부적으로 버퍼링을 해, 신호 경로에 지연을 추가하는 스크립트를 실행할 수 있습니다.</li>
          <li>DelayNode의 역할은, 제어된 레이턴시를 추가하는 것입니다.</li>
          <li>BiquadFilterNode와 IIRFilterNode 필터 디자인은 인과적 필터링 프로세스의 자연스러운 결과로서 들어오는 샘플을 지연시킬 수 있습니다.</li>
          <li>ConvolverNode는, 임펄스에 따라, 컨볼루션 연산의 자연적인 결과로서, 들어오는 샘플을 지연시킬 수 있습니다.</li>
          <li>DynamicsCompressorNode는 신호 경로에서 딜레이를 유발하는 내다보기(look ahead) 알고리즘을 가지고 있습니다.</li>
          <li>MediaStreamAudioSourceNode, MediaStreamTrackAudioSourceNode, MediaStreamAudioDestinationNode는, 구현에 따라, 지연을 추가하는 버퍼를 내부적으로 추가할 수 있습니다.</li>
          <li>ScriptProcessorNode는 컨트롤 스레드와 렌더링 스레드 사이에서 버퍼를 가질 수 있습니다.</li>
          <li>WaveShaperNode는, 오버샘플링일 때, 그리고 오버샘플링 기법에 따라, 신호 경로에 지연을 추가합니다.</li>
        </ul>
        <br>
        <h2>7.2. 오디오 버퍼 복사</h2>
        <p>컨텐츠 획득 동작이 AudioBuffer에서 수행되었을 때, 전체 동작은 보통 채널 데이터를 복사하는 일 없이 구현됩니다. 특히, 마지막 단계는 다음 getChannelData() 호출에서 느리게 수행되어야 합니다. 이것은 개입하는 getChannelData()가 없는 일련의 연이은 컨텐츠 획득 동작 (예: 같은 AudioBuffer를 재생하는 다수의 AudioBufferSourceNode들) 이 할당이나 복사 없이 구현될 수 있음을 의미합니다.</p>
        <p>구현은 추가적인 최적화를 수행할 수 있습니다: 만약 getChannelData()가 AudioBuffer에서 호출된다면, 새로운 ArrayBuffer들은 아직 할당되지 않았지만, AudioBuffer에서의 이전 컨텐츠 획득 동작의 모든 호출자들은 AudioBuffer의 데이터를 사용하는 것을 멈추고, 원시 데이터 버퍼가 새로운 AudioBuffer와의 사용을 위해 재사용되며, 채널 데이터의 어떠한 재할당이나 복사를 방지합니다.</p>
        <br>
      </div>
    </main>
  </body>
</html>